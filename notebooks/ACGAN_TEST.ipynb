{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code:\n",
    "# https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/acgan/acgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "cuda = False\n",
    "\n",
    "n_epochs=200\n",
    "batch_size=64\n",
    "lr=0.0002\n",
    "b1=0.5\n",
    "b2=0.999\n",
    "latent_dim=100\n",
    "n_classes=10\n",
    "img_size=32\n",
    "channels=1\n",
    "sample_interval=20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions and Model\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(n_classes, latent_dim)\n",
    "\n",
    "        self.init_size = img_size // 4  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2 ** 4\n",
    "\n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, n_classes), nn.Softmax())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "\n",
    "        return validity, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs(\"../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, \"../images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/938] [D loss: 1.498148, acc: 9%] [G loss: 1.508554]\n",
      "[Epoch 0/200] [Batch 1/938] [D loss: 1.497787, acc: 11%] [G loss: 1.507713]\n",
      "[Epoch 0/200] [Batch 2/938] [D loss: 1.497904, acc: 8%] [G loss: 1.507665]\n",
      "[Epoch 0/200] [Batch 3/938] [D loss: 1.497959, acc: 9%] [G loss: 1.507322]\n",
      "[Epoch 0/200] [Batch 4/938] [D loss: 1.497561, acc: 8%] [G loss: 1.506833]\n",
      "[Epoch 0/200] [Batch 5/938] [D loss: 1.497791, acc: 9%] [G loss: 1.506875]\n",
      "[Epoch 0/200] [Batch 6/938] [D loss: 1.497904, acc: 8%] [G loss: 1.506737]\n",
      "[Epoch 0/200] [Batch 7/938] [D loss: 1.497782, acc: 12%] [G loss: 1.506469]\n",
      "[Epoch 0/200] [Batch 8/938] [D loss: 1.497883, acc: 7%] [G loss: 1.506177]\n",
      "[Epoch 0/200] [Batch 9/938] [D loss: 1.497540, acc: 16%] [G loss: 1.505599]\n",
      "[Epoch 0/200] [Batch 10/938] [D loss: 1.497109, acc: 15%] [G loss: 1.504723]\n",
      "[Epoch 0/200] [Batch 11/938] [D loss: 1.497531, acc: 13%] [G loss: 1.504702]\n",
      "[Epoch 0/200] [Batch 12/938] [D loss: 1.497193, acc: 7%] [G loss: 1.504767]\n",
      "[Epoch 0/200] [Batch 13/938] [D loss: 1.497137, acc: 13%] [G loss: 1.504422]\n",
      "[Epoch 0/200] [Batch 14/938] [D loss: 1.497298, acc: 7%] [G loss: 1.504161]\n",
      "[Epoch 0/200] [Batch 15/938] [D loss: 1.496995, acc: 11%] [G loss: 1.503471]\n",
      "[Epoch 0/200] [Batch 16/938] [D loss: 1.496849, acc: 14%] [G loss: 1.502788]\n",
      "[Epoch 0/200] [Batch 17/938] [D loss: 1.496677, acc: 7%] [G loss: 1.502370]\n",
      "[Epoch 0/200] [Batch 18/938] [D loss: 1.496583, acc: 13%] [G loss: 1.501503]\n",
      "[Epoch 0/200] [Batch 19/938] [D loss: 1.497019, acc: 8%] [G loss: 1.500316]\n",
      "[Epoch 0/200] [Batch 20/938] [D loss: 1.496598, acc: 5%] [G loss: 1.500056]\n",
      "[Epoch 0/200] [Batch 21/938] [D loss: 1.496607, acc: 9%] [G loss: 1.498597]\n",
      "[Epoch 0/200] [Batch 22/938] [D loss: 1.496438, acc: 12%] [G loss: 1.498328]\n",
      "[Epoch 0/200] [Batch 23/938] [D loss: 1.495881, acc: 14%] [G loss: 1.497539]\n",
      "[Epoch 0/200] [Batch 24/938] [D loss: 1.496116, acc: 11%] [G loss: 1.496102]\n",
      "[Epoch 0/200] [Batch 25/938] [D loss: 1.496266, acc: 11%] [G loss: 1.494609]\n",
      "[Epoch 0/200] [Batch 26/938] [D loss: 1.495016, acc: 12%] [G loss: 1.494872]\n",
      "[Epoch 0/200] [Batch 27/938] [D loss: 1.496196, acc: 10%] [G loss: 1.491531]\n",
      "[Epoch 0/200] [Batch 28/938] [D loss: 1.496801, acc: 6%] [G loss: 1.490921]\n",
      "[Epoch 0/200] [Batch 29/938] [D loss: 1.496683, acc: 10%] [G loss: 1.491905]\n",
      "[Epoch 0/200] [Batch 30/938] [D loss: 1.496378, acc: 7%] [G loss: 1.492793]\n",
      "[Epoch 0/200] [Batch 31/938] [D loss: 1.494711, acc: 13%] [G loss: 1.494595]\n",
      "[Epoch 0/200] [Batch 32/938] [D loss: 1.495022, acc: 16%] [G loss: 1.495222]\n",
      "[Epoch 0/200] [Batch 33/938] [D loss: 1.495059, acc: 7%] [G loss: 1.494517]\n",
      "[Epoch 0/200] [Batch 34/938] [D loss: 1.493962, acc: 10%] [G loss: 1.495328]\n",
      "[Epoch 0/200] [Batch 35/938] [D loss: 1.494117, acc: 7%] [G loss: 1.495519]\n",
      "[Epoch 0/200] [Batch 36/938] [D loss: 1.495048, acc: 10%] [G loss: 1.496596]\n",
      "[Epoch 0/200] [Batch 37/938] [D loss: 1.492481, acc: 11%] [G loss: 1.500433]\n",
      "[Epoch 0/200] [Batch 38/938] [D loss: 1.493139, acc: 14%] [G loss: 1.502504]\n",
      "[Epoch 0/200] [Batch 39/938] [D loss: 1.492034, acc: 15%] [G loss: 1.503262]\n",
      "[Epoch 0/200] [Batch 40/938] [D loss: 1.492035, acc: 10%] [G loss: 1.503924]\n",
      "[Epoch 0/200] [Batch 41/938] [D loss: 1.493983, acc: 10%] [G loss: 1.504571]\n",
      "[Epoch 0/200] [Batch 42/938] [D loss: 1.492661, acc: 7%] [G loss: 1.505254]\n",
      "[Epoch 0/200] [Batch 43/938] [D loss: 1.494576, acc: 7%] [G loss: 1.499805]\n",
      "[Epoch 0/200] [Batch 44/938] [D loss: 1.493630, acc: 8%] [G loss: 1.496494]\n",
      "[Epoch 0/200] [Batch 45/938] [D loss: 1.494416, acc: 10%] [G loss: 1.491064]\n",
      "[Epoch 0/200] [Batch 46/938] [D loss: 1.494596, acc: 4%] [G loss: 1.488861]\n",
      "[Epoch 0/200] [Batch 47/938] [D loss: 1.492535, acc: 10%] [G loss: 1.480507]\n",
      "[Epoch 0/200] [Batch 48/938] [D loss: 1.490009, acc: 10%] [G loss: 1.482080]\n",
      "[Epoch 0/200] [Batch 49/938] [D loss: 1.491723, acc: 10%] [G loss: 1.475900]\n",
      "[Epoch 0/200] [Batch 50/938] [D loss: 1.491758, acc: 6%] [G loss: 1.470017]\n",
      "[Epoch 0/200] [Batch 51/938] [D loss: 1.489971, acc: 10%] [G loss: 1.468277]\n",
      "[Epoch 0/200] [Batch 52/938] [D loss: 1.491752, acc: 10%] [G loss: 1.460727]\n",
      "[Epoch 0/200] [Batch 53/938] [D loss: 1.495695, acc: 9%] [G loss: 1.464768]\n",
      "[Epoch 0/200] [Batch 54/938] [D loss: 1.496916, acc: 14%] [G loss: 1.460774]\n",
      "[Epoch 0/200] [Batch 55/938] [D loss: 1.494475, acc: 10%] [G loss: 1.470837]\n",
      "[Epoch 0/200] [Batch 56/938] [D loss: 1.497792, acc: 9%] [G loss: 1.479065]\n",
      "[Epoch 0/200] [Batch 57/938] [D loss: 1.491071, acc: 13%] [G loss: 1.482942]\n",
      "[Epoch 0/200] [Batch 58/938] [D loss: 1.494537, acc: 10%] [G loss: 1.486054]\n",
      "[Epoch 0/200] [Batch 59/938] [D loss: 1.494916, acc: 12%] [G loss: 1.489776]\n",
      "[Epoch 0/200] [Batch 60/938] [D loss: 1.495759, acc: 10%] [G loss: 1.489522]\n",
      "[Epoch 0/200] [Batch 61/938] [D loss: 1.494233, acc: 10%] [G loss: 1.491533]\n",
      "[Epoch 0/200] [Batch 62/938] [D loss: 1.496116, acc: 10%] [G loss: 1.496404]\n",
      "[Epoch 0/200] [Batch 63/938] [D loss: 1.498247, acc: 8%] [G loss: 1.496171]\n",
      "[Epoch 0/200] [Batch 64/938] [D loss: 1.496471, acc: 12%] [G loss: 1.493485]\n",
      "[Epoch 0/200] [Batch 65/938] [D loss: 1.495842, acc: 11%] [G loss: 1.499319]\n",
      "[Epoch 0/200] [Batch 66/938] [D loss: 1.495060, acc: 13%] [G loss: 1.493203]\n",
      "[Epoch 0/200] [Batch 67/938] [D loss: 1.493943, acc: 11%] [G loss: 1.485738]\n",
      "[Epoch 0/200] [Batch 68/938] [D loss: 1.496425, acc: 12%] [G loss: 1.491978]\n",
      "[Epoch 0/200] [Batch 69/938] [D loss: 1.498157, acc: 7%] [G loss: 1.487273]\n",
      "[Epoch 0/200] [Batch 70/938] [D loss: 1.495578, acc: 12%] [G loss: 1.489159]\n",
      "[Epoch 0/200] [Batch 71/938] [D loss: 1.493838, acc: 8%] [G loss: 1.487589]\n",
      "[Epoch 0/200] [Batch 72/938] [D loss: 1.496527, acc: 10%] [G loss: 1.486188]\n",
      "[Epoch 0/200] [Batch 73/938] [D loss: 1.497191, acc: 10%] [G loss: 1.485370]\n",
      "[Epoch 0/200] [Batch 74/938] [D loss: 1.495753, acc: 11%] [G loss: 1.490779]\n",
      "[Epoch 0/200] [Batch 75/938] [D loss: 1.497303, acc: 6%] [G loss: 1.487355]\n",
      "[Epoch 0/200] [Batch 76/938] [D loss: 1.499703, acc: 10%] [G loss: 1.492059]\n",
      "[Epoch 0/200] [Batch 77/938] [D loss: 1.498022, acc: 11%] [G loss: 1.495677]\n",
      "[Epoch 0/200] [Batch 78/938] [D loss: 1.497306, acc: 10%] [G loss: 1.502402]\n",
      "[Epoch 0/200] [Batch 79/938] [D loss: 1.499353, acc: 7%] [G loss: 1.507135]\n",
      "[Epoch 0/200] [Batch 80/938] [D loss: 1.494623, acc: 13%] [G loss: 1.512237]\n",
      "[Epoch 0/200] [Batch 81/938] [D loss: 1.497854, acc: 10%] [G loss: 1.508768]\n",
      "[Epoch 0/200] [Batch 82/938] [D loss: 1.496689, acc: 18%] [G loss: 1.509933]\n",
      "[Epoch 0/200] [Batch 83/938] [D loss: 1.498426, acc: 10%] [G loss: 1.503072]\n",
      "[Epoch 0/200] [Batch 84/938] [D loss: 1.497097, acc: 16%] [G loss: 1.504514]\n",
      "[Epoch 0/200] [Batch 85/938] [D loss: 1.501719, acc: 15%] [G loss: 1.498166]\n",
      "[Epoch 0/200] [Batch 86/938] [D loss: 1.497439, acc: 11%] [G loss: 1.496966]\n",
      "[Epoch 0/200] [Batch 87/938] [D loss: 1.498099, acc: 16%] [G loss: 1.495819]\n",
      "[Epoch 0/200] [Batch 88/938] [D loss: 1.496947, acc: 12%] [G loss: 1.487883]\n",
      "[Epoch 0/200] [Batch 89/938] [D loss: 1.495796, acc: 10%] [G loss: 1.487751]\n",
      "[Epoch 0/200] [Batch 90/938] [D loss: 1.493968, acc: 14%] [G loss: 1.481012]\n",
      "[Epoch 0/200] [Batch 91/938] [D loss: 1.493968, acc: 15%] [G loss: 1.474382]\n",
      "[Epoch 0/200] [Batch 92/938] [D loss: 1.491426, acc: 14%] [G loss: 1.473909]\n",
      "[Epoch 0/200] [Batch 93/938] [D loss: 1.495898, acc: 14%] [G loss: 1.469381]\n",
      "[Epoch 0/200] [Batch 94/938] [D loss: 1.501809, acc: 24%] [G loss: 1.456730]\n",
      "[Epoch 0/200] [Batch 95/938] [D loss: 1.501716, acc: 14%] [G loss: 1.460570]\n",
      "[Epoch 0/200] [Batch 96/938] [D loss: 1.502006, acc: 10%] [G loss: 1.462419]\n",
      "[Epoch 0/200] [Batch 97/938] [D loss: 1.506344, acc: 14%] [G loss: 1.477958]\n",
      "[Epoch 0/200] [Batch 98/938] [D loss: 1.499758, acc: 10%] [G loss: 1.491866]\n",
      "[Epoch 0/200] [Batch 99/938] [D loss: 1.502052, acc: 17%] [G loss: 1.499210]\n",
      "[Epoch 0/200] [Batch 100/938] [D loss: 1.495600, acc: 16%] [G loss: 1.511439]\n",
      "[Epoch 0/200] [Batch 101/938] [D loss: 1.498981, acc: 17%] [G loss: 1.513372]\n",
      "[Epoch 0/200] [Batch 102/938] [D loss: 1.498710, acc: 12%] [G loss: 1.515650]\n",
      "[Epoch 0/200] [Batch 103/938] [D loss: 1.496569, acc: 15%] [G loss: 1.524186]\n",
      "[Epoch 0/200] [Batch 104/938] [D loss: 1.498874, acc: 13%] [G loss: 1.526354]\n",
      "[Epoch 0/200] [Batch 105/938] [D loss: 1.497674, acc: 11%] [G loss: 1.521741]\n",
      "[Epoch 0/200] [Batch 106/938] [D loss: 1.494703, acc: 14%] [G loss: 1.526861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 107/938] [D loss: 1.495400, acc: 9%] [G loss: 1.526870]\n",
      "[Epoch 0/200] [Batch 108/938] [D loss: 1.497007, acc: 15%] [G loss: 1.523069]\n",
      "[Epoch 0/200] [Batch 109/938] [D loss: 1.497737, acc: 14%] [G loss: 1.524390]\n",
      "[Epoch 0/200] [Batch 110/938] [D loss: 1.497457, acc: 13%] [G loss: 1.518049]\n",
      "[Epoch 0/200] [Batch 111/938] [D loss: 1.497514, acc: 14%] [G loss: 1.518646]\n",
      "[Epoch 0/200] [Batch 112/938] [D loss: 1.499322, acc: 16%] [G loss: 1.514588]\n",
      "[Epoch 0/200] [Batch 113/938] [D loss: 1.498451, acc: 17%] [G loss: 1.510117]\n",
      "[Epoch 0/200] [Batch 114/938] [D loss: 1.498335, acc: 12%] [G loss: 1.504641]\n",
      "[Epoch 0/200] [Batch 115/938] [D loss: 1.496128, acc: 10%] [G loss: 1.506671]\n",
      "[Epoch 0/200] [Batch 116/938] [D loss: 1.495417, acc: 17%] [G loss: 1.501188]\n",
      "[Epoch 0/200] [Batch 117/938] [D loss: 1.495697, acc: 16%] [G loss: 1.499801]\n",
      "[Epoch 0/200] [Batch 118/938] [D loss: 1.492818, acc: 17%] [G loss: 1.496751]\n",
      "[Epoch 0/200] [Batch 119/938] [D loss: 1.491364, acc: 13%] [G loss: 1.493869]\n",
      "[Epoch 0/200] [Batch 120/938] [D loss: 1.494203, acc: 10%] [G loss: 1.486721]\n",
      "[Epoch 0/200] [Batch 121/938] [D loss: 1.495976, acc: 16%] [G loss: 1.472223]\n",
      "[Epoch 0/200] [Batch 122/938] [D loss: 1.495588, acc: 18%] [G loss: 1.468853]\n",
      "[Epoch 0/200] [Batch 123/938] [D loss: 1.501279, acc: 14%] [G loss: 1.464695]\n",
      "[Epoch 0/200] [Batch 124/938] [D loss: 1.502608, acc: 15%] [G loss: 1.465572]\n",
      "[Epoch 0/200] [Batch 125/938] [D loss: 1.503446, acc: 15%] [G loss: 1.471334]\n",
      "[Epoch 0/200] [Batch 126/938] [D loss: 1.506761, acc: 14%] [G loss: 1.474235]\n",
      "[Epoch 0/200] [Batch 127/938] [D loss: 1.501881, acc: 14%] [G loss: 1.487696]\n",
      "[Epoch 0/200] [Batch 128/938] [D loss: 1.501711, acc: 16%] [G loss: 1.494056]\n",
      "[Epoch 0/200] [Batch 129/938] [D loss: 1.500530, acc: 9%] [G loss: 1.502429]\n",
      "[Epoch 0/200] [Batch 130/938] [D loss: 1.498980, acc: 11%] [G loss: 1.504999]\n",
      "[Epoch 0/200] [Batch 131/938] [D loss: 1.499401, acc: 10%] [G loss: 1.507948]\n",
      "[Epoch 0/200] [Batch 132/938] [D loss: 1.498043, acc: 13%] [G loss: 1.514976]\n",
      "[Epoch 0/200] [Batch 133/938] [D loss: 1.497618, acc: 12%] [G loss: 1.517291]\n",
      "[Epoch 0/200] [Batch 134/938] [D loss: 1.496701, acc: 16%] [G loss: 1.517204]\n",
      "[Epoch 0/200] [Batch 135/938] [D loss: 1.493939, acc: 16%] [G loss: 1.516932]\n",
      "[Epoch 0/200] [Batch 136/938] [D loss: 1.497011, acc: 12%] [G loss: 1.519651]\n",
      "[Epoch 0/200] [Batch 137/938] [D loss: 1.494306, acc: 10%] [G loss: 1.517738]\n",
      "[Epoch 0/200] [Batch 138/938] [D loss: 1.493562, acc: 12%] [G loss: 1.525911]\n",
      "[Epoch 0/200] [Batch 139/938] [D loss: 1.493873, acc: 12%] [G loss: 1.521503]\n",
      "[Epoch 0/200] [Batch 140/938] [D loss: 1.493858, acc: 12%] [G loss: 1.517763]\n",
      "[Epoch 0/200] [Batch 141/938] [D loss: 1.493999, acc: 17%] [G loss: 1.514771]\n",
      "[Epoch 0/200] [Batch 142/938] [D loss: 1.492805, acc: 14%] [G loss: 1.512742]\n",
      "[Epoch 0/200] [Batch 143/938] [D loss: 1.495347, acc: 11%] [G loss: 1.504383]\n",
      "[Epoch 0/200] [Batch 144/938] [D loss: 1.495320, acc: 9%] [G loss: 1.506622]\n",
      "[Epoch 0/200] [Batch 145/938] [D loss: 1.496138, acc: 10%] [G loss: 1.498867]\n",
      "[Epoch 0/200] [Batch 146/938] [D loss: 1.496749, acc: 11%] [G loss: 1.495500]\n",
      "[Epoch 0/200] [Batch 147/938] [D loss: 1.499930, acc: 15%] [G loss: 1.492237]\n",
      "[Epoch 0/200] [Batch 148/938] [D loss: 1.495614, acc: 21%] [G loss: 1.487162]\n",
      "[Epoch 0/200] [Batch 149/938] [D loss: 1.496140, acc: 16%] [G loss: 1.485981]\n",
      "[Epoch 0/200] [Batch 150/938] [D loss: 1.495542, acc: 20%] [G loss: 1.480244]\n",
      "[Epoch 0/200] [Batch 151/938] [D loss: 1.495309, acc: 16%] [G loss: 1.477026]\n",
      "[Epoch 0/200] [Batch 152/938] [D loss: 1.499544, acc: 17%] [G loss: 1.471043]\n",
      "[Epoch 0/200] [Batch 153/938] [D loss: 1.497399, acc: 12%] [G loss: 1.469945]\n",
      "[Epoch 0/200] [Batch 154/938] [D loss: 1.497249, acc: 11%] [G loss: 1.466634]\n",
      "[Epoch 0/200] [Batch 155/938] [D loss: 1.497839, acc: 14%] [G loss: 1.470279]\n",
      "[Epoch 0/200] [Batch 156/938] [D loss: 1.505383, acc: 15%] [G loss: 1.465142]\n",
      "[Epoch 0/200] [Batch 157/938] [D loss: 1.501390, acc: 11%] [G loss: 1.478242]\n",
      "[Epoch 0/200] [Batch 158/938] [D loss: 1.499847, acc: 14%] [G loss: 1.481216]\n",
      "[Epoch 0/200] [Batch 159/938] [D loss: 1.499317, acc: 10%] [G loss: 1.486714]\n",
      "[Epoch 0/200] [Batch 160/938] [D loss: 1.498148, acc: 14%] [G loss: 1.495094]\n",
      "[Epoch 0/200] [Batch 161/938] [D loss: 1.499273, acc: 16%] [G loss: 1.504445]\n",
      "[Epoch 0/200] [Batch 162/938] [D loss: 1.497789, acc: 16%] [G loss: 1.506030]\n",
      "[Epoch 0/200] [Batch 163/938] [D loss: 1.495127, acc: 12%] [G loss: 1.508954]\n",
      "[Epoch 0/200] [Batch 164/938] [D loss: 1.496467, acc: 15%] [G loss: 1.518270]\n",
      "[Epoch 0/200] [Batch 165/938] [D loss: 1.495915, acc: 15%] [G loss: 1.518391]\n",
      "[Epoch 0/200] [Batch 166/938] [D loss: 1.493308, acc: 15%] [G loss: 1.518209]\n",
      "[Epoch 0/200] [Batch 167/938] [D loss: 1.491607, acc: 19%] [G loss: 1.527926]\n",
      "[Epoch 0/200] [Batch 168/938] [D loss: 1.496534, acc: 15%] [G loss: 1.523902]\n",
      "[Epoch 0/200] [Batch 169/938] [D loss: 1.495274, acc: 18%] [G loss: 1.524741]\n",
      "[Epoch 0/200] [Batch 170/938] [D loss: 1.496498, acc: 14%] [G loss: 1.523513]\n",
      "[Epoch 0/200] [Batch 171/938] [D loss: 1.493043, acc: 19%] [G loss: 1.525597]\n",
      "[Epoch 0/200] [Batch 172/938] [D loss: 1.494066, acc: 18%] [G loss: 1.518671]\n",
      "[Epoch 0/200] [Batch 173/938] [D loss: 1.500769, acc: 14%] [G loss: 1.512601]\n",
      "[Epoch 0/200] [Batch 174/938] [D loss: 1.500266, acc: 18%] [G loss: 1.510395]\n",
      "[Epoch 0/200] [Batch 175/938] [D loss: 1.500481, acc: 9%] [G loss: 1.508410]\n",
      "[Epoch 0/200] [Batch 176/938] [D loss: 1.498350, acc: 26%] [G loss: 1.504486]\n",
      "[Epoch 0/200] [Batch 177/938] [D loss: 1.500343, acc: 13%] [G loss: 1.500113]\n",
      "[Epoch 0/200] [Batch 178/938] [D loss: 1.497457, acc: 16%] [G loss: 1.499445]\n",
      "[Epoch 0/200] [Batch 179/938] [D loss: 1.498510, acc: 25%] [G loss: 1.493454]\n",
      "[Epoch 0/200] [Batch 180/938] [D loss: 1.497003, acc: 19%] [G loss: 1.494407]\n",
      "[Epoch 0/200] [Batch 181/938] [D loss: 1.496925, acc: 19%] [G loss: 1.487041]\n",
      "[Epoch 0/200] [Batch 182/938] [D loss: 1.497586, acc: 17%] [G loss: 1.486293]\n",
      "[Epoch 0/200] [Batch 183/938] [D loss: 1.496176, acc: 17%] [G loss: 1.482595]\n",
      "[Epoch 0/200] [Batch 184/938] [D loss: 1.499246, acc: 17%] [G loss: 1.485713]\n",
      "[Epoch 0/200] [Batch 185/938] [D loss: 1.494613, acc: 22%] [G loss: 1.483763]\n",
      "[Epoch 0/200] [Batch 186/938] [D loss: 1.496811, acc: 24%] [G loss: 1.481705]\n",
      "[Epoch 0/200] [Batch 187/938] [D loss: 1.497294, acc: 15%] [G loss: 1.479722]\n",
      "[Epoch 0/200] [Batch 188/938] [D loss: 1.494709, acc: 24%] [G loss: 1.481602]\n",
      "[Epoch 0/200] [Batch 189/938] [D loss: 1.495386, acc: 17%] [G loss: 1.487374]\n",
      "[Epoch 0/200] [Batch 190/938] [D loss: 1.495489, acc: 21%] [G loss: 1.486662]\n",
      "[Epoch 0/200] [Batch 191/938] [D loss: 1.494764, acc: 17%] [G loss: 1.487886]\n",
      "[Epoch 0/200] [Batch 192/938] [D loss: 1.497642, acc: 16%] [G loss: 1.487683]\n",
      "[Epoch 0/200] [Batch 193/938] [D loss: 1.499388, acc: 13%] [G loss: 1.493558]\n",
      "[Epoch 0/200] [Batch 194/938] [D loss: 1.498476, acc: 18%] [G loss: 1.496780]\n",
      "[Epoch 0/200] [Batch 195/938] [D loss: 1.494651, acc: 21%] [G loss: 1.498690]\n",
      "[Epoch 0/200] [Batch 196/938] [D loss: 1.499304, acc: 25%] [G loss: 1.499599]\n",
      "[Epoch 0/200] [Batch 197/938] [D loss: 1.497699, acc: 21%] [G loss: 1.502801]\n",
      "[Epoch 0/200] [Batch 198/938] [D loss: 1.496798, acc: 19%] [G loss: 1.506179]\n",
      "[Epoch 0/200] [Batch 199/938] [D loss: 1.496930, acc: 25%] [G loss: 1.508343]\n",
      "[Epoch 0/200] [Batch 200/938] [D loss: 1.499035, acc: 26%] [G loss: 1.502302]\n",
      "[Epoch 0/200] [Batch 201/938] [D loss: 1.499817, acc: 15%] [G loss: 1.500027]\n",
      "[Epoch 0/200] [Batch 202/938] [D loss: 1.500501, acc: 20%] [G loss: 1.498637]\n",
      "[Epoch 0/200] [Batch 203/938] [D loss: 1.499480, acc: 17%] [G loss: 1.500151]\n",
      "[Epoch 0/200] [Batch 204/938] [D loss: 1.499352, acc: 21%] [G loss: 1.495489]\n",
      "[Epoch 0/200] [Batch 205/938] [D loss: 1.498003, acc: 21%] [G loss: 1.499219]\n",
      "[Epoch 0/200] [Batch 206/938] [D loss: 1.497405, acc: 17%] [G loss: 1.502578]\n",
      "[Epoch 0/200] [Batch 207/938] [D loss: 1.497664, acc: 19%] [G loss: 1.498094]\n",
      "[Epoch 0/200] [Batch 208/938] [D loss: 1.496116, acc: 19%] [G loss: 1.496496]\n",
      "[Epoch 0/200] [Batch 209/938] [D loss: 1.494634, acc: 20%] [G loss: 1.495551]\n",
      "[Epoch 0/200] [Batch 210/938] [D loss: 1.494399, acc: 18%] [G loss: 1.494654]\n",
      "[Epoch 0/200] [Batch 211/938] [D loss: 1.491177, acc: 21%] [G loss: 1.494604]\n",
      "[Epoch 0/200] [Batch 212/938] [D loss: 1.493518, acc: 27%] [G loss: 1.491502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 213/938] [D loss: 1.495741, acc: 20%] [G loss: 1.490902]\n",
      "[Epoch 0/200] [Batch 214/938] [D loss: 1.490810, acc: 16%] [G loss: 1.489759]\n",
      "[Epoch 0/200] [Batch 215/938] [D loss: 1.494229, acc: 26%] [G loss: 1.493207]\n",
      "[Epoch 0/200] [Batch 216/938] [D loss: 1.487662, acc: 25%] [G loss: 1.490037]\n",
      "[Epoch 0/200] [Batch 217/938] [D loss: 1.492506, acc: 23%] [G loss: 1.492589]\n",
      "[Epoch 0/200] [Batch 218/938] [D loss: 1.494677, acc: 23%] [G loss: 1.490757]\n",
      "[Epoch 0/200] [Batch 219/938] [D loss: 1.489209, acc: 24%] [G loss: 1.496707]\n",
      "[Epoch 0/200] [Batch 220/938] [D loss: 1.494195, acc: 23%] [G loss: 1.494116]\n",
      "[Epoch 0/200] [Batch 221/938] [D loss: 1.492618, acc: 19%] [G loss: 1.499205]\n",
      "[Epoch 0/200] [Batch 222/938] [D loss: 1.494690, acc: 27%] [G loss: 1.494944]\n",
      "[Epoch 0/200] [Batch 223/938] [D loss: 1.496628, acc: 21%] [G loss: 1.497934]\n",
      "[Epoch 0/200] [Batch 224/938] [D loss: 1.499570, acc: 21%] [G loss: 1.498233]\n",
      "[Epoch 0/200] [Batch 225/938] [D loss: 1.502715, acc: 24%] [G loss: 1.494535]\n",
      "[Epoch 0/200] [Batch 226/938] [D loss: 1.499671, acc: 18%] [G loss: 1.505073]\n",
      "[Epoch 0/200] [Batch 227/938] [D loss: 1.498007, acc: 20%] [G loss: 1.506411]\n",
      "[Epoch 0/200] [Batch 228/938] [D loss: 1.495928, acc: 27%] [G loss: 1.495791]\n",
      "[Epoch 0/200] [Batch 229/938] [D loss: 1.500399, acc: 18%] [G loss: 1.497021]\n",
      "[Epoch 0/200] [Batch 230/938] [D loss: 1.495799, acc: 23%] [G loss: 1.499360]\n",
      "[Epoch 0/200] [Batch 231/938] [D loss: 1.495239, acc: 28%] [G loss: 1.499992]\n",
      "[Epoch 0/200] [Batch 232/938] [D loss: 1.496333, acc: 27%] [G loss: 1.495481]\n",
      "[Epoch 0/200] [Batch 233/938] [D loss: 1.494748, acc: 19%] [G loss: 1.498384]\n",
      "[Epoch 0/200] [Batch 234/938] [D loss: 1.491176, acc: 25%] [G loss: 1.492210]\n",
      "[Epoch 0/200] [Batch 235/938] [D loss: 1.491183, acc: 22%] [G loss: 1.495735]\n",
      "[Epoch 0/200] [Batch 236/938] [D loss: 1.485943, acc: 30%] [G loss: 1.485563]\n",
      "[Epoch 0/200] [Batch 237/938] [D loss: 1.488267, acc: 25%] [G loss: 1.483814]\n",
      "[Epoch 0/200] [Batch 238/938] [D loss: 1.484616, acc: 23%] [G loss: 1.482364]\n",
      "[Epoch 0/200] [Batch 239/938] [D loss: 1.488248, acc: 24%] [G loss: 1.474824]\n",
      "[Epoch 0/200] [Batch 240/938] [D loss: 1.486976, acc: 25%] [G loss: 1.463867]\n",
      "[Epoch 0/200] [Batch 241/938] [D loss: 1.484093, acc: 23%] [G loss: 1.460735]\n",
      "[Epoch 0/200] [Batch 242/938] [D loss: 1.491243, acc: 28%] [G loss: 1.441620]\n",
      "[Epoch 0/200] [Batch 243/938] [D loss: 1.501772, acc: 29%] [G loss: 1.437650]\n",
      "[Epoch 0/200] [Batch 244/938] [D loss: 1.501019, acc: 28%] [G loss: 1.440136]\n",
      "[Epoch 0/200] [Batch 245/938] [D loss: 1.498632, acc: 25%] [G loss: 1.446609]\n",
      "[Epoch 0/200] [Batch 246/938] [D loss: 1.497685, acc: 35%] [G loss: 1.456614]\n",
      "[Epoch 0/200] [Batch 247/938] [D loss: 1.505842, acc: 19%] [G loss: 1.462292]\n",
      "[Epoch 0/200] [Batch 248/938] [D loss: 1.499426, acc: 21%] [G loss: 1.479252]\n",
      "[Epoch 0/200] [Batch 249/938] [D loss: 1.497753, acc: 18%] [G loss: 1.493719]\n",
      "[Epoch 0/200] [Batch 250/938] [D loss: 1.499611, acc: 24%] [G loss: 1.494553]\n",
      "[Epoch 0/200] [Batch 251/938] [D loss: 1.500192, acc: 22%] [G loss: 1.494828]\n",
      "[Epoch 0/200] [Batch 252/938] [D loss: 1.495155, acc: 20%] [G loss: 1.508888]\n",
      "[Epoch 0/200] [Batch 253/938] [D loss: 1.490811, acc: 27%] [G loss: 1.511549]\n",
      "[Epoch 0/200] [Batch 254/938] [D loss: 1.493372, acc: 17%] [G loss: 1.519490]\n",
      "[Epoch 0/200] [Batch 255/938] [D loss: 1.489879, acc: 23%] [G loss: 1.519882]\n",
      "[Epoch 0/200] [Batch 256/938] [D loss: 1.487739, acc: 28%] [G loss: 1.528977]\n",
      "[Epoch 0/200] [Batch 257/938] [D loss: 1.482109, acc: 25%] [G loss: 1.529719]\n",
      "[Epoch 0/200] [Batch 258/938] [D loss: 1.481555, acc: 26%] [G loss: 1.541359]\n",
      "[Epoch 0/200] [Batch 259/938] [D loss: 1.486043, acc: 14%] [G loss: 1.540515]\n",
      "[Epoch 0/200] [Batch 260/938] [D loss: 1.484917, acc: 19%] [G loss: 1.536583]\n",
      "[Epoch 0/200] [Batch 261/938] [D loss: 1.489329, acc: 19%] [G loss: 1.523094]\n",
      "[Epoch 0/200] [Batch 262/938] [D loss: 1.479449, acc: 25%] [G loss: 1.523481]\n",
      "[Epoch 0/200] [Batch 263/938] [D loss: 1.487981, acc: 25%] [G loss: 1.506064]\n",
      "[Epoch 0/200] [Batch 264/938] [D loss: 1.484587, acc: 25%] [G loss: 1.500930]\n",
      "[Epoch 0/200] [Batch 265/938] [D loss: 1.480559, acc: 21%] [G loss: 1.504154]\n",
      "[Epoch 0/200] [Batch 266/938] [D loss: 1.488140, acc: 28%] [G loss: 1.466310]\n",
      "[Epoch 0/200] [Batch 267/938] [D loss: 1.490891, acc: 33%] [G loss: 1.467923]\n",
      "[Epoch 0/200] [Batch 268/938] [D loss: 1.495291, acc: 19%] [G loss: 1.453591]\n",
      "[Epoch 0/200] [Batch 269/938] [D loss: 1.493919, acc: 21%] [G loss: 1.458351]\n",
      "[Epoch 0/200] [Batch 270/938] [D loss: 1.485708, acc: 22%] [G loss: 1.465513]\n",
      "[Epoch 0/200] [Batch 271/938] [D loss: 1.479283, acc: 28%] [G loss: 1.467787]\n",
      "[Epoch 0/200] [Batch 272/938] [D loss: 1.474371, acc: 29%] [G loss: 1.476764]\n",
      "[Epoch 0/200] [Batch 273/938] [D loss: 1.468426, acc: 25%] [G loss: 1.501242]\n",
      "[Epoch 0/200] [Batch 274/938] [D loss: 1.467085, acc: 26%] [G loss: 1.497642]\n",
      "[Epoch 0/200] [Batch 275/938] [D loss: 1.464688, acc: 25%] [G loss: 1.518893]\n",
      "[Epoch 0/200] [Batch 276/938] [D loss: 1.457675, acc: 25%] [G loss: 1.505626]\n",
      "[Epoch 0/200] [Batch 277/938] [D loss: 1.449681, acc: 29%] [G loss: 1.521665]\n",
      "[Epoch 0/200] [Batch 278/938] [D loss: 1.471140, acc: 23%] [G loss: 1.502905]\n",
      "[Epoch 0/200] [Batch 279/938] [D loss: 1.461477, acc: 28%] [G loss: 1.484185]\n",
      "[Epoch 0/200] [Batch 280/938] [D loss: 1.475607, acc: 21%] [G loss: 1.478139]\n",
      "[Epoch 0/200] [Batch 281/938] [D loss: 1.477215, acc: 28%] [G loss: 1.432683]\n",
      "[Epoch 0/200] [Batch 282/938] [D loss: 1.503965, acc: 15%] [G loss: 1.446036]\n",
      "[Epoch 0/200] [Batch 283/938] [D loss: 1.487544, acc: 28%] [G loss: 1.436270]\n",
      "[Epoch 0/200] [Batch 284/938] [D loss: 1.475540, acc: 26%] [G loss: 1.428348]\n",
      "[Epoch 0/200] [Batch 285/938] [D loss: 1.466705, acc: 28%] [G loss: 1.437761]\n",
      "[Epoch 0/200] [Batch 286/938] [D loss: 1.461119, acc: 25%] [G loss: 1.462036]\n",
      "[Epoch 0/200] [Batch 287/938] [D loss: 1.456332, acc: 23%] [G loss: 1.479607]\n",
      "[Epoch 0/200] [Batch 288/938] [D loss: 1.452649, acc: 28%] [G loss: 1.479855]\n",
      "[Epoch 0/200] [Batch 289/938] [D loss: 1.455167, acc: 26%] [G loss: 1.487135]\n",
      "[Epoch 0/200] [Batch 290/938] [D loss: 1.438805, acc: 28%] [G loss: 1.521976]\n",
      "[Epoch 0/200] [Batch 291/938] [D loss: 1.457252, acc: 21%] [G loss: 1.522557]\n",
      "[Epoch 0/200] [Batch 292/938] [D loss: 1.429976, acc: 28%] [G loss: 1.539363]\n",
      "[Epoch 0/200] [Batch 293/938] [D loss: 1.436406, acc: 26%] [G loss: 1.553088]\n",
      "[Epoch 0/200] [Batch 294/938] [D loss: 1.461633, acc: 26%] [G loss: 1.532957]\n",
      "[Epoch 0/200] [Batch 295/938] [D loss: 1.471938, acc: 24%] [G loss: 1.507485]\n",
      "[Epoch 0/200] [Batch 296/938] [D loss: 1.460073, acc: 23%] [G loss: 1.526020]\n",
      "[Epoch 0/200] [Batch 297/938] [D loss: 1.463405, acc: 25%] [G loss: 1.461851]\n",
      "[Epoch 0/200] [Batch 298/938] [D loss: 1.461321, acc: 25%] [G loss: 1.479720]\n",
      "[Epoch 0/200] [Batch 299/938] [D loss: 1.450655, acc: 27%] [G loss: 1.479140]\n",
      "[Epoch 0/200] [Batch 300/938] [D loss: 1.422316, acc: 34%] [G loss: 1.492413]\n",
      "[Epoch 0/200] [Batch 301/938] [D loss: 1.411131, acc: 39%] [G loss: 1.481668]\n",
      "[Epoch 0/200] [Batch 302/938] [D loss: 1.428938, acc: 33%] [G loss: 1.485447]\n",
      "[Epoch 0/200] [Batch 303/938] [D loss: 1.424737, acc: 28%] [G loss: 1.476614]\n",
      "[Epoch 0/200] [Batch 304/938] [D loss: 1.401767, acc: 32%] [G loss: 1.457642]\n",
      "[Epoch 0/200] [Batch 305/938] [D loss: 1.397371, acc: 37%] [G loss: 1.448352]\n",
      "[Epoch 0/200] [Batch 306/938] [D loss: 1.406274, acc: 31%] [G loss: 1.448231]\n",
      "[Epoch 0/200] [Batch 307/938] [D loss: 1.421955, acc: 32%] [G loss: 1.418321]\n",
      "[Epoch 0/200] [Batch 308/938] [D loss: 1.384581, acc: 35%] [G loss: 1.395425]\n",
      "[Epoch 0/200] [Batch 309/938] [D loss: 1.416990, acc: 26%] [G loss: 1.435304]\n",
      "[Epoch 0/200] [Batch 310/938] [D loss: 1.415475, acc: 35%] [G loss: 1.420681]\n",
      "[Epoch 0/200] [Batch 311/938] [D loss: 1.413041, acc: 34%] [G loss: 1.426817]\n",
      "[Epoch 0/200] [Batch 312/938] [D loss: 1.414261, acc: 33%] [G loss: 1.447072]\n",
      "[Epoch 0/200] [Batch 313/938] [D loss: 1.428561, acc: 35%] [G loss: 1.499044]\n",
      "[Epoch 0/200] [Batch 314/938] [D loss: 1.417311, acc: 36%] [G loss: 1.511120]\n",
      "[Epoch 0/200] [Batch 315/938] [D loss: 1.413224, acc: 35%] [G loss: 1.471077]\n",
      "[Epoch 0/200] [Batch 316/938] [D loss: 1.406255, acc: 42%] [G loss: 1.453319]\n",
      "[Epoch 0/200] [Batch 317/938] [D loss: 1.405869, acc: 37%] [G loss: 1.493846]\n",
      "[Epoch 0/200] [Batch 318/938] [D loss: 1.392500, acc: 41%] [G loss: 1.503382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 319/938] [D loss: 1.391784, acc: 42%] [G loss: 1.468189]\n",
      "[Epoch 0/200] [Batch 320/938] [D loss: 1.416155, acc: 34%] [G loss: 1.483149]\n",
      "[Epoch 0/200] [Batch 321/938] [D loss: 1.433609, acc: 32%] [G loss: 1.524684]\n",
      "[Epoch 0/200] [Batch 322/938] [D loss: 1.399724, acc: 35%] [G loss: 1.506818]\n",
      "[Epoch 0/200] [Batch 323/938] [D loss: 1.399046, acc: 35%] [G loss: 1.492302]\n",
      "[Epoch 0/200] [Batch 324/938] [D loss: 1.410566, acc: 31%] [G loss: 1.509084]\n",
      "[Epoch 0/200] [Batch 325/938] [D loss: 1.364809, acc: 39%] [G loss: 1.462684]\n",
      "[Epoch 0/200] [Batch 326/938] [D loss: 1.394494, acc: 36%] [G loss: 1.453243]\n",
      "[Epoch 0/200] [Batch 327/938] [D loss: 1.402651, acc: 36%] [G loss: 1.485531]\n",
      "[Epoch 0/200] [Batch 328/938] [D loss: 1.374701, acc: 40%] [G loss: 1.493008]\n",
      "[Epoch 0/200] [Batch 329/938] [D loss: 1.426009, acc: 31%] [G loss: 1.517607]\n",
      "[Epoch 0/200] [Batch 330/938] [D loss: 1.394455, acc: 39%] [G loss: 1.458793]\n",
      "[Epoch 0/200] [Batch 331/938] [D loss: 1.376678, acc: 40%] [G loss: 1.503688]\n",
      "[Epoch 0/200] [Batch 332/938] [D loss: 1.379693, acc: 39%] [G loss: 1.445118]\n",
      "[Epoch 0/200] [Batch 333/938] [D loss: 1.426952, acc: 32%] [G loss: 1.459100]\n",
      "[Epoch 0/200] [Batch 334/938] [D loss: 1.401794, acc: 35%] [G loss: 1.447750]\n",
      "[Epoch 0/200] [Batch 335/938] [D loss: 1.390735, acc: 41%] [G loss: 1.452572]\n",
      "[Epoch 0/200] [Batch 336/938] [D loss: 1.394173, acc: 42%] [G loss: 1.435558]\n",
      "[Epoch 0/200] [Batch 337/938] [D loss: 1.362970, acc: 44%] [G loss: 1.450696]\n",
      "[Epoch 0/200] [Batch 338/938] [D loss: 1.369839, acc: 42%] [G loss: 1.442674]\n",
      "[Epoch 0/200] [Batch 339/938] [D loss: 1.387539, acc: 41%] [G loss: 1.421975]\n",
      "[Epoch 0/200] [Batch 340/938] [D loss: 1.396760, acc: 40%] [G loss: 1.442860]\n",
      "[Epoch 0/200] [Batch 341/938] [D loss: 1.381743, acc: 39%] [G loss: 1.429225]\n",
      "[Epoch 0/200] [Batch 342/938] [D loss: 1.356112, acc: 48%] [G loss: 1.421736]\n",
      "[Epoch 0/200] [Batch 343/938] [D loss: 1.378698, acc: 43%] [G loss: 1.410907]\n",
      "[Epoch 0/200] [Batch 344/938] [D loss: 1.391392, acc: 43%] [G loss: 1.463370]\n",
      "[Epoch 0/200] [Batch 345/938] [D loss: 1.370102, acc: 45%] [G loss: 1.468516]\n",
      "[Epoch 0/200] [Batch 346/938] [D loss: 1.375223, acc: 39%] [G loss: 1.483047]\n",
      "[Epoch 0/200] [Batch 347/938] [D loss: 1.368360, acc: 46%] [G loss: 1.499436]\n",
      "[Epoch 0/200] [Batch 348/938] [D loss: 1.352194, acc: 44%] [G loss: 1.484949]\n",
      "[Epoch 0/200] [Batch 349/938] [D loss: 1.361842, acc: 46%] [G loss: 1.468374]\n",
      "[Epoch 0/200] [Batch 350/938] [D loss: 1.339446, acc: 46%] [G loss: 1.497423]\n",
      "[Epoch 0/200] [Batch 351/938] [D loss: 1.396576, acc: 35%] [G loss: 1.548711]\n",
      "[Epoch 0/200] [Batch 352/938] [D loss: 1.358473, acc: 40%] [G loss: 1.493442]\n",
      "[Epoch 0/200] [Batch 353/938] [D loss: 1.370023, acc: 40%] [G loss: 1.488356]\n",
      "[Epoch 0/200] [Batch 354/938] [D loss: 1.373968, acc: 37%] [G loss: 1.508612]\n",
      "[Epoch 0/200] [Batch 355/938] [D loss: 1.349112, acc: 47%] [G loss: 1.485025]\n",
      "[Epoch 0/200] [Batch 356/938] [D loss: 1.377927, acc: 42%] [G loss: 1.537661]\n",
      "[Epoch 0/200] [Batch 357/938] [D loss: 1.358894, acc: 50%] [G loss: 1.511377]\n",
      "[Epoch 0/200] [Batch 358/938] [D loss: 1.362504, acc: 46%] [G loss: 1.476953]\n",
      "[Epoch 0/200] [Batch 359/938] [D loss: 1.362114, acc: 50%] [G loss: 1.466524]\n",
      "[Epoch 0/200] [Batch 360/938] [D loss: 1.369694, acc: 45%] [G loss: 1.470025]\n",
      "[Epoch 0/200] [Batch 361/938] [D loss: 1.354552, acc: 48%] [G loss: 1.442023]\n",
      "[Epoch 0/200] [Batch 362/938] [D loss: 1.348683, acc: 53%] [G loss: 1.437832]\n",
      "[Epoch 0/200] [Batch 363/938] [D loss: 1.341697, acc: 46%] [G loss: 1.512435]\n",
      "[Epoch 0/200] [Batch 364/938] [D loss: 1.366648, acc: 41%] [G loss: 1.492413]\n",
      "[Epoch 0/200] [Batch 365/938] [D loss: 1.375616, acc: 42%] [G loss: 1.478574]\n",
      "[Epoch 0/200] [Batch 366/938] [D loss: 1.367949, acc: 47%] [G loss: 1.511032]\n",
      "[Epoch 0/200] [Batch 367/938] [D loss: 1.350272, acc: 46%] [G loss: 1.456950]\n",
      "[Epoch 0/200] [Batch 368/938] [D loss: 1.340658, acc: 45%] [G loss: 1.447829]\n",
      "[Epoch 0/200] [Batch 369/938] [D loss: 1.365119, acc: 46%] [G loss: 1.455730]\n",
      "[Epoch 0/200] [Batch 370/938] [D loss: 1.350215, acc: 52%] [G loss: 1.420377]\n",
      "[Epoch 0/200] [Batch 371/938] [D loss: 1.370878, acc: 43%] [G loss: 1.449023]\n",
      "[Epoch 0/200] [Batch 372/938] [D loss: 1.355742, acc: 46%] [G loss: 1.459911]\n",
      "[Epoch 0/200] [Batch 373/938] [D loss: 1.347680, acc: 46%] [G loss: 1.436222]\n",
      "[Epoch 0/200] [Batch 374/938] [D loss: 1.356693, acc: 45%] [G loss: 1.471887]\n",
      "[Epoch 0/200] [Batch 375/938] [D loss: 1.352651, acc: 44%] [G loss: 1.490690]\n",
      "[Epoch 0/200] [Batch 376/938] [D loss: 1.317745, acc: 52%] [G loss: 1.407328]\n",
      "[Epoch 0/200] [Batch 377/938] [D loss: 1.344515, acc: 39%] [G loss: 1.444659]\n",
      "[Epoch 0/200] [Batch 378/938] [D loss: 1.349159, acc: 46%] [G loss: 1.437790]\n",
      "[Epoch 0/200] [Batch 379/938] [D loss: 1.344064, acc: 48%] [G loss: 1.438656]\n",
      "[Epoch 0/200] [Batch 380/938] [D loss: 1.325488, acc: 50%] [G loss: 1.468072]\n",
      "[Epoch 0/200] [Batch 381/938] [D loss: 1.301316, acc: 51%] [G loss: 1.433267]\n",
      "[Epoch 0/200] [Batch 382/938] [D loss: 1.354658, acc: 42%] [G loss: 1.482060]\n",
      "[Epoch 0/200] [Batch 383/938] [D loss: 1.356292, acc: 48%] [G loss: 1.431705]\n",
      "[Epoch 0/200] [Batch 384/938] [D loss: 1.350974, acc: 48%] [G loss: 1.437986]\n",
      "[Epoch 0/200] [Batch 385/938] [D loss: 1.345421, acc: 48%] [G loss: 1.453020]\n",
      "[Epoch 0/200] [Batch 386/938] [D loss: 1.306522, acc: 52%] [G loss: 1.418344]\n",
      "[Epoch 0/200] [Batch 387/938] [D loss: 1.360422, acc: 44%] [G loss: 1.447708]\n",
      "[Epoch 0/200] [Batch 388/938] [D loss: 1.335892, acc: 47%] [G loss: 1.468269]\n",
      "[Epoch 0/200] [Batch 389/938] [D loss: 1.364999, acc: 39%] [G loss: 1.491190]\n",
      "[Epoch 0/200] [Batch 390/938] [D loss: 1.327367, acc: 50%] [G loss: 1.461699]\n",
      "[Epoch 0/200] [Batch 391/938] [D loss: 1.307756, acc: 59%] [G loss: 1.370481]\n",
      "[Epoch 0/200] [Batch 392/938] [D loss: 1.313049, acc: 56%] [G loss: 1.414928]\n",
      "[Epoch 0/200] [Batch 393/938] [D loss: 1.350249, acc: 47%] [G loss: 1.461728]\n",
      "[Epoch 0/200] [Batch 394/938] [D loss: 1.354679, acc: 46%] [G loss: 1.437532]\n",
      "[Epoch 0/200] [Batch 395/938] [D loss: 1.358878, acc: 47%] [G loss: 1.414854]\n",
      "[Epoch 0/200] [Batch 396/938] [D loss: 1.352190, acc: 46%] [G loss: 1.436295]\n",
      "[Epoch 0/200] [Batch 397/938] [D loss: 1.348173, acc: 42%] [G loss: 1.436349]\n",
      "[Epoch 0/200] [Batch 398/938] [D loss: 1.350610, acc: 44%] [G loss: 1.426103]\n",
      "[Epoch 0/200] [Batch 399/938] [D loss: 1.355467, acc: 42%] [G loss: 1.465224]\n",
      "[Epoch 0/200] [Batch 400/938] [D loss: 1.324457, acc: 46%] [G loss: 1.462192]\n",
      "[Epoch 0/200] [Batch 401/938] [D loss: 1.342066, acc: 48%] [G loss: 1.455247]\n",
      "[Epoch 0/200] [Batch 402/938] [D loss: 1.309798, acc: 52%] [G loss: 1.466563]\n",
      "[Epoch 0/200] [Batch 403/938] [D loss: 1.318893, acc: 50%] [G loss: 1.478079]\n",
      "[Epoch 0/200] [Batch 404/938] [D loss: 1.334283, acc: 47%] [G loss: 1.504088]\n",
      "[Epoch 0/200] [Batch 405/938] [D loss: 1.345672, acc: 42%] [G loss: 1.498241]\n",
      "[Epoch 0/200] [Batch 406/938] [D loss: 1.324161, acc: 48%] [G loss: 1.473739]\n",
      "[Epoch 0/200] [Batch 407/938] [D loss: 1.338823, acc: 47%] [G loss: 1.478654]\n",
      "[Epoch 0/200] [Batch 408/938] [D loss: 1.305235, acc: 57%] [G loss: 1.398868]\n",
      "[Epoch 0/200] [Batch 409/938] [D loss: 1.354663, acc: 48%] [G loss: 1.403529]\n",
      "[Epoch 0/200] [Batch 410/938] [D loss: 1.328861, acc: 47%] [G loss: 1.457562]\n",
      "[Epoch 0/200] [Batch 411/938] [D loss: 1.309895, acc: 57%] [G loss: 1.418056]\n",
      "[Epoch 0/200] [Batch 412/938] [D loss: 1.317031, acc: 53%] [G loss: 1.476199]\n",
      "[Epoch 0/200] [Batch 413/938] [D loss: 1.341409, acc: 47%] [G loss: 1.471244]\n",
      "[Epoch 0/200] [Batch 414/938] [D loss: 1.293290, acc: 57%] [G loss: 1.416446]\n",
      "[Epoch 0/200] [Batch 415/938] [D loss: 1.334545, acc: 50%] [G loss: 1.483290]\n",
      "[Epoch 0/200] [Batch 416/938] [D loss: 1.292654, acc: 56%] [G loss: 1.464975]\n",
      "[Epoch 0/200] [Batch 417/938] [D loss: 1.335395, acc: 49%] [G loss: 1.461942]\n",
      "[Epoch 0/200] [Batch 418/938] [D loss: 1.345248, acc: 45%] [G loss: 1.457134]\n",
      "[Epoch 0/200] [Batch 419/938] [D loss: 1.309369, acc: 56%] [G loss: 1.470757]\n",
      "[Epoch 0/200] [Batch 420/938] [D loss: 1.356698, acc: 41%] [G loss: 1.469817]\n",
      "[Epoch 0/200] [Batch 421/938] [D loss: 1.316709, acc: 53%] [G loss: 1.494986]\n",
      "[Epoch 0/200] [Batch 422/938] [D loss: 1.293836, acc: 58%] [G loss: 1.468601]\n",
      "[Epoch 0/200] [Batch 423/938] [D loss: 1.311937, acc: 54%] [G loss: 1.447151]\n",
      "[Epoch 0/200] [Batch 424/938] [D loss: 1.341412, acc: 45%] [G loss: 1.461619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 425/938] [D loss: 1.296468, acc: 56%] [G loss: 1.461508]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity, pred_label = discriminator(gen_imgs)\n",
    "        g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels))\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_pred, real_aux = discriminator(real_imgs)\n",
    "        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "        # Loss for fake images\n",
    "        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        # Calculate discriminator accuracy\n",
    "        pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "        gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "        d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %d%%] [G loss: %f]\"\n",
    "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), 100 * d_acc, g_loss.item())\n",
    "        )\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_image(n_row=10, batches_done=batches_done)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
