{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code:\n",
    "# https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/acgan/acgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# Models\n",
    "from torchvision.models import vgg19\n",
    "from sys import path\n",
    "path.append(\"../utils\")\n",
    "from models import LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "cuda = True\n",
    "\n",
    "n_epochs=200\n",
    "batch_size=64\n",
    "lr=0.0002\n",
    "b1=0.5\n",
    "b2=0.999\n",
    "latent_dim=100\n",
    "n_classes=10\n",
    "img_size=28\n",
    "channels=1\n",
    "sample_interval=50\n",
    "\n",
    "d_real_loss_coeff = 0.65\n",
    "d_fake_loss_coeff = 0.35\n",
    "\n",
    "adv_loss_coeff = 1\n",
    "aux_loss_coeff = 1\n",
    "tar_loss_coeff = .06\n",
    "\n",
    "tar_loss_default = 22.2  # This is equal to the max possible tar_loss value\n",
    "\n",
    "# target classifier conditional constants\n",
    "adv_loss_threshold = 0.9\n",
    "aux_loss_threshold = 1.48\n",
    "\n",
    "lenet5_state_path = \"../utils/models/trained_lenet5.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions for fixed target classifier\n",
    "\n",
    "def load_pytorch_model():\n",
    "    # Load model from torchvision.models (NOTE: VVG19 IS NOT FOR MNIST, DON'T USE)\n",
    "    model = vgg19(pretrained=True)\n",
    "    model.eval()\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    \n",
    "    for p in model.features.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.classifier.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_LeNet5():\n",
    "    net = LeNet5()\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    # remove map location = cpu if using cuda\n",
    "    net.load_state_dict(torch.load(lenet5_state_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    # set model to eval mode so nothing is changed\n",
    "    net.eval()\n",
    "    \n",
    "    return net\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(n_classes, latent_dim)\n",
    "\n",
    "        self.init_size = img_size // 4  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = 2#img_size // 2 ** 4\n",
    "\n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, n_classes), nn.Softmax())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "\n",
    "        return validity, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "adversarial_loss = nn.BCELoss()\n",
    "auxiliary_loss = nn.CrossEntropyLoss()\n",
    "target_classifier_loss = nn.CrossEntropyLoss() # negate target classifier output when passing to this loss function\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Load target classifier\n",
    "\n",
    "target_classifier = load_LeNet5()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()\n",
    "    target_classifier = target_classifier.cuda()\n",
    "    target_classifier_loss = target_classifier_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs(\"../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, \"../images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def shouldCalculateTargetLoss(validity, pred_label, label):\n",
    "    # Only works if batch size is 1, so just uses the first value\n",
    "    return (validity > threshold_valid and  # validity is above threshold\n",
    "        pred_label[label] == np.amax(pred_label) and  # classification is correct\n",
    "        pred_label[label] > threshold_classification)  # classification confidence is above threshold\n",
    "\n",
    "def get_target_loss(validity, pred_labels, label, target_classification):\n",
    "    # Apply conditional loss to each output individually\n",
    "    for i in range(target_classification.size(0)):\n",
    "        if shouldCalculateTargetLoss(validity[i], pred_labels[i], label[i]):\n",
    "            target_classification[i] *= -1\n",
    "            \n",
    "            \n",
    "    loss = target_classifier_loss(target_classification, gen_labels)\n",
    "    \n",
    "    print(\"Count of loss including target loss:\", torch.sum(loss_mult))\n",
    "    \n",
    "    return loss * loss_mult\n",
    "'''\n",
    "\n",
    "\n",
    "def get_target_loss(adv_loss, aux_loss, target_classification, true_classification):\n",
    "    #print(target_classification[0])\n",
    "    if (adv_loss < adv_loss_threshold and aux_loss < aux_loss_threshold):\n",
    "        return target_classifier_loss(target_classification * -1, true_classification)\n",
    "    return Variable(FloatTensor([tar_loss_default]))\n",
    "\n",
    "def test_attack(size):\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (size, latent_dim))))\n",
    "    gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, size)))\n",
    "    imgs = generator(z, gen_labels)\n",
    "    validity, pred_label = discriminator(imgs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMPORARY SECTION: testing target classifier output\n",
    "\n",
    "z = Variable(FloatTensor(np.random.normal(0, 1, (1, latent_dim))))\n",
    "gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, 1)))\n",
    "imgs = generator(z, gen_labels)\n",
    "validity, pred_label = discriminator(imgs)\n",
    "\n",
    "#print(validity)\n",
    "\n",
    "\n",
    "#bceloss_test = nn.BCELoss()\n",
    "celoss_test = nn.CrossEntropyLoss()\n",
    "\n",
    "inp = Variable(FloatTensor([[10,10,10,10,10,10,10,10,10,-10]]))\n",
    "true = Variable(LongTensor([9]))\n",
    "\n",
    "print(celoss_test(inp, true))\n",
    "\n",
    "#print(bceloss_test(Variable(FloatTensor([[0.65]])), Variable(FloatTensor([[1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 0/200, Batch 0/938\n",
      "D loss: 1.495095, acc: 7% // tar acc: 10% // adv loss: 0.675007, aux loss: 2.302276, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 50/938\n",
      "D loss: 1.470443, acc: 9% // tar acc: 14% // adv loss: 0.417340, aux loss: 2.302612, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 100/938\n",
      "D loss: 1.472975, acc: 13% // tar acc: 6% // adv loss: 0.405612, aux loss: 2.301651, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 150/938\n",
      "D loss: 1.480109, acc: 7% // tar acc: 12% // adv loss: 0.435369, aux loss: 2.303051, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 200/938\n",
      "D loss: 1.477440, acc: 10% // tar acc: 10% // adv loss: 0.431102, aux loss: 2.302141, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 250/938\n",
      "D loss: 1.479097, acc: 14% // tar acc: 6% // adv loss: 0.408893, aux loss: 2.302971, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 300/938\n",
      "D loss: 1.481359, acc: 15% // tar acc: 7% // adv loss: 0.421006, aux loss: 2.302632, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 350/938\n",
      "D loss: 1.454748, acc: 15% // tar acc: 10% // adv loss: 0.452846, aux loss: 2.302359, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 400/938\n",
      "D loss: 1.364183, acc: 35% // tar acc: 10% // adv loss: 0.407279, aux loss: 2.282935, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 450/938\n",
      "D loss: 1.280976, acc: 52% // tar acc: 15% // adv loss: 0.435299, aux loss: 2.178869, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 500/938\n",
      "D loss: 1.242472, acc: 59% // tar acc: 18% // adv loss: 0.482270, aux loss: 2.112136, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 550/938\n",
      "D loss: 1.201534, acc: 63% // tar acc: 23% // adv loss: 0.510345, aux loss: 1.945105, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 600/938\n",
      "D loss: 1.163317, acc: 71% // tar acc: 42% // adv loss: 0.506575, aux loss: 1.866548, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 650/938\n",
      "D loss: 1.137489, acc: 75% // tar acc: 34% // adv loss: 0.569137, aux loss: 1.788278, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 700/938\n",
      "D loss: 1.126705, acc: 85% // tar acc: 48% // adv loss: 0.525197, aux loss: 1.661111, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 750/938\n",
      "D loss: 1.099972, acc: 83% // tar acc: 50% // adv loss: 0.568084, aux loss: 1.703910, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 800/938\n",
      "D loss: 1.113281, acc: 85% // tar acc: 53% // adv loss: 0.590006, aux loss: 1.650600, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 850/938\n",
      "D loss: 1.079745, acc: 88% // tar acc: 57% // adv loss: 0.564074, aux loss: 1.594934, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 0/200, Batch 900/938\n",
      "D loss: 1.095650, acc: 89% // tar acc: 62% // adv loss: 0.545848, aux loss: 1.590528, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 12/938\n",
      "D loss: 1.056687, acc: 92% // tar acc: 64% // adv loss: 0.619276, aux loss: 1.546833, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 62/938\n",
      "D loss: 1.088908, acc: 82% // tar acc: 60% // adv loss: 0.532586, aux loss: 1.628825, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 112/938\n",
      "D loss: 1.097672, acc: 88% // tar acc: 70% // adv loss: 0.549512, aux loss: 1.593309, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 162/938\n",
      "D loss: 1.097602, acc: 89% // tar acc: 67% // adv loss: 0.629625, aux loss: 1.575830, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 212/938\n",
      "D loss: 1.101715, acc: 84% // tar acc: 65% // adv loss: 0.591237, aux loss: 1.600638, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 262/938\n",
      "D loss: 1.033626, acc: 90% // tar acc: 59% // adv loss: 0.637154, aux loss: 1.555215, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 312/938\n",
      "D loss: 1.029523, acc: 93% // tar acc: 71% // adv loss: 0.756161, aux loss: 1.521994, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 362/938\n",
      "D loss: 1.033197, acc: 92% // tar acc: 78% // adv loss: 0.630147, aux loss: 1.525387, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 412/938\n",
      "D loss: 1.058633, acc: 90% // tar acc: 60% // adv loss: 0.649622, aux loss: 1.582391, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 462/938\n",
      "D loss: 1.082344, acc: 89% // tar acc: 62% // adv loss: 0.814313, aux loss: 1.527967, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 512/938\n",
      "D loss: 1.086828, acc: 88% // tar acc: 70% // adv loss: 0.606387, aux loss: 1.573085, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 562/938\n",
      "D loss: 1.081439, acc: 89% // tar acc: 71% // adv loss: 0.679257, aux loss: 1.552490, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 612/938\n",
      "D loss: 1.067110, acc: 91% // tar acc: 84% // adv loss: 0.548100, aux loss: 1.510350, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 662/938\n",
      "D loss: 1.049465, acc: 89% // tar acc: 79% // adv loss: 0.547791, aux loss: 1.566336, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 712/938\n",
      "D loss: 1.036744, acc: 92% // tar acc: 68% // adv loss: 0.677379, aux loss: 1.527602, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 762/938\n",
      "D loss: 1.084669, acc: 89% // tar acc: 81% // adv loss: 0.613396, aux loss: 1.534131, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 812/938\n",
      "D loss: 1.015679, acc: 91% // tar acc: 82% // adv loss: 0.770383, aux loss: 1.507033, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 862/938\n",
      "D loss: 1.046890, acc: 96% // tar acc: 70% // adv loss: 0.609851, aux loss: 1.503115, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 1/200, Batch 912/938\n",
      "D loss: 1.070414, acc: 92% // tar acc: 84% // adv loss: 0.731705, aux loss: 1.484844, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 24/938\n",
      "D loss: 1.056011, acc: 91% // tar acc: 90% // adv loss: 0.615665, aux loss: 1.520670, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 74/938\n",
      "D loss: 1.031304, acc: 96% // tar acc: 85% // adv loss: 0.566452, aux loss: 1.492785, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 124/938\n",
      "D loss: 1.041888, acc: 89% // tar acc: 70% // adv loss: 0.773670, aux loss: 1.524781, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 174/938\n",
      "D loss: 1.042360, acc: 92% // tar acc: 78% // adv loss: 0.633433, aux loss: 1.538578, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 224/938\n",
      "D loss: 1.049907, acc: 93% // tar acc: 84% // adv loss: 0.560826, aux loss: 1.516016, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 274/938\n",
      "D loss: 1.085177, acc: 94% // tar acc: 79% // adv loss: 0.658472, aux loss: 1.504453, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 324/938\n",
      "D loss: 1.045528, acc: 92% // tar acc: 79% // adv loss: 0.586355, aux loss: 1.528888, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 374/938\n",
      "D loss: 1.086138, acc: 96% // tar acc: 78% // adv loss: 0.689765, aux loss: 1.522168, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 424/938\n",
      "D loss: 1.052840, acc: 92% // tar acc: 84% // adv loss: 0.644513, aux loss: 1.530397, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 474/938\n",
      "D loss: 1.100667, acc: 94% // tar acc: 93% // adv loss: 0.430919, aux loss: 1.499533, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 524/938\n",
      "D loss: 1.034303, acc: 94% // tar acc: 76% // adv loss: 0.584853, aux loss: 1.515613, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 574/938\n",
      "D loss: 1.108873, acc: 90% // tar acc: 90% // adv loss: 0.649973, aux loss: 1.498928, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 624/938\n",
      "D loss: 1.048705, acc: 95% // tar acc: 79% // adv loss: 0.602080, aux loss: 1.525852, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 674/938\n",
      "D loss: 1.096182, acc: 91% // tar acc: 93% // adv loss: 0.654361, aux loss: 1.520360, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 724/938\n",
      "D loss: 1.038978, acc: 94% // tar acc: 87% // adv loss: 0.779439, aux loss: 1.481184, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 774/938\n",
      "D loss: 1.015731, acc: 96% // tar acc: 81% // adv loss: 0.683671, aux loss: 1.517709, tar loss: 1.332000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 2/200, Batch 824/938\n",
      "D loss: 1.074260, acc: 92% // tar acc: 82% // adv loss: 0.587761, aux loss: 1.528969, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 874/938\n",
      "D loss: 1.025654, acc: 93% // tar acc: 85% // adv loss: 0.510870, aux loss: 1.494153, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 2/200, Batch 924/938\n",
      "D loss: 1.064894, acc: 92% // tar acc: 89% // adv loss: 0.601245, aux loss: 1.517765, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 36/938\n",
      "D loss: 1.093604, acc: 92% // tar acc: 96% // adv loss: 0.594575, aux loss: 1.494347, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 86/938\n",
      "D loss: 1.025279, acc: 96% // tar acc: 95% // adv loss: 0.624714, aux loss: 1.489703, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 136/938\n",
      "D loss: 1.111304, acc: 89% // tar acc: 90% // adv loss: 0.427516, aux loss: 1.532510, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 186/938\n",
      "D loss: 1.059257, acc: 92% // tar acc: 96% // adv loss: 0.616790, aux loss: 1.476929, tar loss: 0.806757\n",
      "=====================\n",
      "Epoch 3/200, Batch 236/938\n",
      "D loss: 1.061966, acc: 92% // tar acc: 89% // adv loss: 0.555328, aux loss: 1.526095, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 286/938\n",
      "D loss: 1.040166, acc: 95% // tar acc: 90% // adv loss: 0.585652, aux loss: 1.500899, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 336/938\n",
      "D loss: 1.090050, acc: 94% // tar acc: 85% // adv loss: 0.563116, aux loss: 1.528473, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 386/938\n",
      "D loss: 1.092052, acc: 93% // tar acc: 90% // adv loss: 0.743477, aux loss: 1.509462, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 436/938\n",
      "D loss: 1.067645, acc: 95% // tar acc: 82% // adv loss: 0.598448, aux loss: 1.496614, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 486/938\n",
      "D loss: 1.044602, acc: 95% // tar acc: 89% // adv loss: 0.529027, aux loss: 1.498637, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 536/938\n",
      "D loss: 1.026045, acc: 96% // tar acc: 89% // adv loss: 0.559623, aux loss: 1.507781, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 586/938\n",
      "D loss: 1.061231, acc: 97% // tar acc: 84% // adv loss: 0.564802, aux loss: 1.513721, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 636/938\n",
      "D loss: 1.152560, acc: 94% // tar acc: 90% // adv loss: 0.747050, aux loss: 1.504577, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 686/938\n",
      "D loss: 1.028654, acc: 93% // tar acc: 84% // adv loss: 0.446747, aux loss: 1.502592, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 736/938\n",
      "D loss: 1.061178, acc: 98% // tar acc: 90% // adv loss: 0.430873, aux loss: 1.482440, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 786/938\n",
      "D loss: 1.053810, acc: 96% // tar acc: 82% // adv loss: 0.610133, aux loss: 1.517169, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 836/938\n",
      "D loss: 1.055835, acc: 97% // tar acc: 90% // adv loss: 0.552288, aux loss: 1.478633, tar loss: 0.837159\n",
      "=====================\n",
      "Epoch 3/200, Batch 886/938\n",
      "D loss: 1.068433, acc: 92% // tar acc: 92% // adv loss: 0.558399, aux loss: 1.510846, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 3/200, Batch 936/938\n",
      "D loss: 1.048041, acc: 96% // tar acc: 90% // adv loss: 0.489091, aux loss: 1.534564, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 48/938\n",
      "D loss: 1.024789, acc: 96% // tar acc: 93% // adv loss: 0.600629, aux loss: 1.498562, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 98/938\n",
      "D loss: 1.083496, acc: 96% // tar acc: 87% // adv loss: 0.650837, aux loss: 1.484731, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 148/938\n",
      "D loss: 1.071996, acc: 94% // tar acc: 89% // adv loss: 0.603795, aux loss: 1.529105, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 198/938\n",
      "D loss: 1.087731, acc: 92% // tar acc: 95% // adv loss: 0.483251, aux loss: 1.478988, tar loss: 0.836617\n",
      "=====================\n",
      "Epoch 4/200, Batch 248/938\n",
      "D loss: 1.035727, acc: 96% // tar acc: 90% // adv loss: 0.574857, aux loss: 1.470537, tar loss: 0.730673\n",
      "=====================\n",
      "Epoch 4/200, Batch 298/938\n",
      "D loss: 1.086209, acc: 92% // tar acc: 93% // adv loss: 0.549399, aux loss: 1.481554, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 348/938\n",
      "D loss: 1.061097, acc: 98% // tar acc: 87% // adv loss: 0.404833, aux loss: 1.473328, tar loss: 0.748081\n",
      "=====================\n",
      "Epoch 4/200, Batch 398/938\n",
      "D loss: 1.051650, acc: 93% // tar acc: 89% // adv loss: 0.556923, aux loss: 1.505082, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 448/938\n",
      "D loss: 1.077733, acc: 93% // tar acc: 96% // adv loss: 0.640094, aux loss: 1.481416, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 498/938\n",
      "D loss: 1.060366, acc: 96% // tar acc: 84% // adv loss: 0.587698, aux loss: 1.470538, tar loss: 0.792278\n",
      "=====================\n",
      "Epoch 4/200, Batch 548/938\n",
      "D loss: 1.038568, acc: 98% // tar acc: 93% // adv loss: 0.627373, aux loss: 1.479951, tar loss: 0.824551\n",
      "=====================\n",
      "Epoch 4/200, Batch 598/938\n",
      "D loss: 1.099778, acc: 88% // tar acc: 84% // adv loss: 0.537936, aux loss: 1.525695, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 648/938\n",
      "D loss: 1.066124, acc: 94% // tar acc: 89% // adv loss: 0.531939, aux loss: 1.502472, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 698/938\n",
      "D loss: 1.052554, acc: 93% // tar acc: 95% // adv loss: 0.534529, aux loss: 1.485143, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 748/938\n",
      "D loss: 1.036463, acc: 94% // tar acc: 93% // adv loss: 0.418406, aux loss: 1.503689, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 798/938\n",
      "D loss: 1.042143, acc: 97% // tar acc: 95% // adv loss: 0.552082, aux loss: 1.489240, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 4/200, Batch 848/938\n",
      "D loss: 1.019216, acc: 96% // tar acc: 92% // adv loss: 0.531490, aux loss: 1.469494, tar loss: 0.827221\n",
      "=====================\n",
      "Epoch 4/200, Batch 898/938\n",
      "D loss: 1.075006, acc: 91% // tar acc: 89% // adv loss: 0.508462, aux loss: 1.488411, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 10/938\n",
      "D loss: 1.025683, acc: 97% // tar acc: 85% // adv loss: 0.448566, aux loss: 1.487202, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 60/938\n",
      "D loss: 1.059242, acc: 95% // tar acc: 95% // adv loss: 0.573805, aux loss: 1.495106, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 110/938\n",
      "D loss: 1.066750, acc: 95% // tar acc: 85% // adv loss: 0.547669, aux loss: 1.504137, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 160/938\n",
      "D loss: 1.052101, acc: 93% // tar acc: 95% // adv loss: 0.516460, aux loss: 1.479361, tar loss: 0.814699\n",
      "=====================\n",
      "Epoch 5/200, Batch 210/938\n",
      "D loss: 1.048126, acc: 95% // tar acc: 82% // adv loss: 0.616801, aux loss: 1.515307, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 260/938\n",
      "D loss: 1.062714, acc: 95% // tar acc: 90% // adv loss: 0.598050, aux loss: 1.482507, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 310/938\n",
      "D loss: 1.038525, acc: 99% // tar acc: 96% // adv loss: 0.523141, aux loss: 1.467189, tar loss: 0.796024\n",
      "=====================\n",
      "Epoch 5/200, Batch 360/938\n",
      "D loss: 1.046398, acc: 97% // tar acc: 96% // adv loss: 0.564808, aux loss: 1.487354, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 410/938\n",
      "D loss: 1.055669, acc: 96% // tar acc: 93% // adv loss: 0.541054, aux loss: 1.497939, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 460/938\n",
      "D loss: 1.063014, acc: 95% // tar acc: 87% // adv loss: 0.502313, aux loss: 1.499599, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 510/938\n",
      "D loss: 1.077290, acc: 96% // tar acc: 87% // adv loss: 0.567496, aux loss: 1.505954, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 560/938\n",
      "D loss: 1.154986, acc: 89% // tar acc: 85% // adv loss: 0.471862, aux loss: 1.517033, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 610/938\n",
      "D loss: 1.074197, acc: 96% // tar acc: 89% // adv loss: 0.574711, aux loss: 1.493280, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 660/938\n",
      "D loss: 1.029055, acc: 97% // tar acc: 89% // adv loss: 0.644706, aux loss: 1.510131, tar loss: 1.332000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 5/200, Batch 710/938\n",
      "D loss: 1.044700, acc: 94% // tar acc: 85% // adv loss: 0.554129, aux loss: 1.502105, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 760/938\n",
      "D loss: 1.061638, acc: 96% // tar acc: 92% // adv loss: 0.424462, aux loss: 1.480063, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 810/938\n",
      "D loss: 1.009913, acc: 98% // tar acc: 92% // adv loss: 0.459440, aux loss: 1.468423, tar loss: 0.812253\n",
      "=====================\n",
      "Epoch 5/200, Batch 860/938\n",
      "D loss: 1.064041, acc: 94% // tar acc: 92% // adv loss: 0.740444, aux loss: 1.533289, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 5/200, Batch 910/938\n",
      "D loss: 1.069877, acc: 92% // tar acc: 89% // adv loss: 0.510470, aux loss: 1.486792, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 22/938\n",
      "D loss: 1.087418, acc: 92% // tar acc: 95% // adv loss: 0.436984, aux loss: 1.495356, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 72/938\n",
      "D loss: 1.060470, acc: 98% // tar acc: 90% // adv loss: 0.463374, aux loss: 1.506882, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 122/938\n",
      "D loss: 1.015648, acc: 98% // tar acc: 93% // adv loss: 0.606939, aux loss: 1.463518, tar loss: 0.778447\n",
      "=====================\n",
      "Epoch 6/200, Batch 172/938\n",
      "D loss: 1.046976, acc: 96% // tar acc: 92% // adv loss: 0.549021, aux loss: 1.509028, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 222/938\n",
      "D loss: 1.076107, acc: 94% // tar acc: 90% // adv loss: 0.482628, aux loss: 1.495597, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 272/938\n",
      "D loss: 1.098764, acc: 95% // tar acc: 95% // adv loss: 0.525510, aux loss: 1.479084, tar loss: 0.832041\n",
      "=====================\n",
      "Epoch 6/200, Batch 322/938\n",
      "D loss: 1.059187, acc: 96% // tar acc: 95% // adv loss: 0.384025, aux loss: 1.495930, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 372/938\n",
      "D loss: 1.056302, acc: 95% // tar acc: 96% // adv loss: 0.469755, aux loss: 1.477324, tar loss: 0.829388\n",
      "=====================\n",
      "Epoch 6/200, Batch 422/938\n",
      "D loss: 1.055032, acc: 97% // tar acc: 90% // adv loss: 0.445056, aux loss: 1.473185, tar loss: 0.816923\n",
      "=====================\n",
      "Epoch 6/200, Batch 472/938\n",
      "D loss: 1.093035, acc: 93% // tar acc: 96% // adv loss: 0.530467, aux loss: 1.473020, tar loss: 0.837703\n",
      "=====================\n",
      "Epoch 6/200, Batch 522/938\n",
      "D loss: 1.050426, acc: 97% // tar acc: 89% // adv loss: 0.586874, aux loss: 1.500988, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 572/938\n",
      "D loss: 1.033762, acc: 95% // tar acc: 90% // adv loss: 0.382388, aux loss: 1.490061, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 622/938\n",
      "D loss: 1.035126, acc: 96% // tar acc: 92% // adv loss: 0.489248, aux loss: 1.497745, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 672/938\n",
      "D loss: 1.066054, acc: 98% // tar acc: 89% // adv loss: 0.511804, aux loss: 1.479262, tar loss: 0.747305\n",
      "=====================\n",
      "Epoch 6/200, Batch 722/938\n",
      "D loss: 1.068154, acc: 94% // tar acc: 90% // adv loss: 0.494769, aux loss: 1.513451, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 772/938\n",
      "D loss: 1.111319, acc: 96% // tar acc: 93% // adv loss: 0.549271, aux loss: 1.486094, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 822/938\n",
      "D loss: 1.085981, acc: 95% // tar acc: 95% // adv loss: 0.495808, aux loss: 1.477372, tar loss: 0.828977\n",
      "=====================\n",
      "Epoch 6/200, Batch 872/938\n",
      "D loss: 1.063758, acc: 94% // tar acc: 92% // adv loss: 0.493945, aux loss: 1.509473, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 6/200, Batch 922/938\n",
      "D loss: 1.044084, acc: 98% // tar acc: 93% // adv loss: 0.453443, aux loss: 1.475731, tar loss: 0.843743\n",
      "=====================\n",
      "Epoch 7/200, Batch 34/938\n",
      "D loss: 1.066838, acc: 96% // tar acc: 98% // adv loss: 0.500544, aux loss: 1.487943, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 84/938\n",
      "D loss: 1.064711, acc: 96% // tar acc: 92% // adv loss: 0.509658, aux loss: 1.485706, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 134/938\n",
      "D loss: 1.040975, acc: 93% // tar acc: 89% // adv loss: 0.476457, aux loss: 1.511009, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 184/938\n",
      "D loss: 1.058439, acc: 97% // tar acc: 87% // adv loss: 0.400669, aux loss: 1.511372, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 234/938\n",
      "D loss: 1.066650, acc: 95% // tar acc: 90% // adv loss: 0.606767, aux loss: 1.491251, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 284/938\n",
      "D loss: 1.120664, acc: 95% // tar acc: 98% // adv loss: 0.442080, aux loss: 1.472075, tar loss: 0.901996\n",
      "=====================\n",
      "Epoch 7/200, Batch 334/938\n",
      "D loss: 1.081681, acc: 96% // tar acc: 93% // adv loss: 0.578413, aux loss: 1.479373, tar loss: 0.771552\n",
      "=====================\n",
      "Epoch 7/200, Batch 384/938\n",
      "D loss: 1.046089, acc: 98% // tar acc: 93% // adv loss: 0.595393, aux loss: 1.485445, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 434/938\n",
      "D loss: 1.027269, acc: 99% // tar acc: 95% // adv loss: 0.588506, aux loss: 1.470332, tar loss: 0.812089\n",
      "=====================\n",
      "Epoch 7/200, Batch 484/938\n",
      "D loss: 1.027443, acc: 96% // tar acc: 90% // adv loss: 0.591525, aux loss: 1.525076, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 534/938\n",
      "D loss: 1.089152, acc: 97% // tar acc: 96% // adv loss: 0.629718, aux loss: 1.470873, tar loss: 0.855632\n",
      "=====================\n",
      "Epoch 7/200, Batch 584/938\n",
      "D loss: 1.058285, acc: 96% // tar acc: 95% // adv loss: 0.524486, aux loss: 1.500838, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 634/938\n",
      "D loss: 1.079029, acc: 94% // tar acc: 90% // adv loss: 0.515054, aux loss: 1.478806, tar loss: 0.859633\n",
      "=====================\n",
      "Epoch 7/200, Batch 684/938\n",
      "D loss: 1.084776, acc: 96% // tar acc: 95% // adv loss: 0.551731, aux loss: 1.508937, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 734/938\n",
      "D loss: 1.045256, acc: 94% // tar acc: 89% // adv loss: 0.432767, aux loss: 1.515184, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 784/938\n",
      "D loss: 1.077772, acc: 97% // tar acc: 96% // adv loss: 0.596192, aux loss: 1.476419, tar loss: 0.853281\n",
      "=====================\n",
      "Epoch 7/200, Batch 834/938\n",
      "D loss: 1.087387, acc: 96% // tar acc: 92% // adv loss: 0.550683, aux loss: 1.487797, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 884/938\n",
      "D loss: 1.042152, acc: 96% // tar acc: 92% // adv loss: 0.397136, aux loss: 1.486485, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 7/200, Batch 934/938\n",
      "D loss: 1.067292, acc: 98% // tar acc: 90% // adv loss: 0.564000, aux loss: 1.477781, tar loss: 0.863419\n",
      "=====================\n",
      "Epoch 8/200, Batch 46/938\n",
      "D loss: 1.056931, acc: 96% // tar acc: 93% // adv loss: 0.520735, aux loss: 1.488945, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 96/938\n",
      "D loss: 1.095368, acc: 93% // tar acc: 92% // adv loss: 0.453480, aux loss: 1.470092, tar loss: 0.829153\n",
      "=====================\n",
      "Epoch 8/200, Batch 146/938\n",
      "D loss: 1.056145, acc: 96% // tar acc: 85% // adv loss: 0.562878, aux loss: 1.484892, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 196/938\n",
      "D loss: 1.058382, acc: 97% // tar acc: 95% // adv loss: 0.526668, aux loss: 1.478151, tar loss: 0.823382\n",
      "=====================\n",
      "Epoch 8/200, Batch 246/938\n",
      "D loss: 0.980516, acc: 95% // tar acc: 85% // adv loss: 0.563659, aux loss: 1.482840, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 296/938\n",
      "D loss: 1.081902, acc: 96% // tar acc: 96% // adv loss: 0.553282, aux loss: 1.482134, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 346/938\n",
      "D loss: 1.091825, acc: 92% // tar acc: 89% // adv loss: 0.585043, aux loss: 1.518613, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 396/938\n",
      "D loss: 1.092905, acc: 93% // tar acc: 87% // adv loss: 0.554139, aux loss: 1.477794, tar loss: 0.770368\n",
      "=====================\n",
      "Epoch 8/200, Batch 446/938\n",
      "D loss: 1.055453, acc: 95% // tar acc: 93% // adv loss: 0.618218, aux loss: 1.475380, tar loss: 0.840809\n",
      "=====================\n",
      "Epoch 8/200, Batch 496/938\n",
      "D loss: 1.050894, acc: 96% // tar acc: 90% // adv loss: 0.558473, aux loss: 1.484568, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 546/938\n",
      "D loss: 1.085672, acc: 96% // tar acc: 90% // adv loss: 0.449894, aux loss: 1.489804, tar loss: 1.332000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 8/200, Batch 596/938\n",
      "D loss: 1.081383, acc: 92% // tar acc: 95% // adv loss: 0.433001, aux loss: 1.480368, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 646/938\n",
      "D loss: 1.070291, acc: 95% // tar acc: 89% // adv loss: 0.537743, aux loss: 1.488573, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 696/938\n",
      "D loss: 1.059254, acc: 96% // tar acc: 93% // adv loss: 0.419749, aux loss: 1.484153, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 746/938\n",
      "D loss: 1.085974, acc: 96% // tar acc: 93% // adv loss: 0.504601, aux loss: 1.494791, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 796/938\n",
      "D loss: 1.074272, acc: 92% // tar acc: 90% // adv loss: 0.422791, aux loss: 1.499204, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 846/938\n",
      "D loss: 1.114376, acc: 94% // tar acc: 93% // adv loss: 0.388401, aux loss: 1.483462, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 8/200, Batch 896/938\n",
      "D loss: 1.075125, acc: 96% // tar acc: 95% // adv loss: 0.391472, aux loss: 1.500353, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 9/200, Batch 8/938\n",
      "D loss: 1.083269, acc: 96% // tar acc: 93% // adv loss: 0.432509, aux loss: 1.487388, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 9/200, Batch 58/938\n",
      "D loss: 1.048053, acc: 96% // tar acc: 96% // adv loss: 0.474373, aux loss: 1.479916, tar loss: 0.846972\n",
      "=====================\n",
      "Epoch 9/200, Batch 108/938\n",
      "D loss: 1.075599, acc: 96% // tar acc: 89% // adv loss: 0.381568, aux loss: 1.487901, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 9/200, Batch 158/938\n",
      "D loss: 1.070362, acc: 95% // tar acc: 95% // adv loss: 0.438309, aux loss: 1.470207, tar loss: 0.862422\n",
      "=====================\n",
      "Epoch 9/200, Batch 208/938\n",
      "D loss: 1.064196, acc: 98% // tar acc: 92% // adv loss: 0.543364, aux loss: 1.469085, tar loss: 0.811422\n",
      "=====================\n",
      "Epoch 9/200, Batch 258/938\n",
      "D loss: 1.120701, acc: 92% // tar acc: 96% // adv loss: 0.560955, aux loss: 1.503899, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 9/200, Batch 308/938\n",
      "D loss: 1.064327, acc: 96% // tar acc: 93% // adv loss: 0.483142, aux loss: 1.469076, tar loss: 0.828361\n",
      "=====================\n",
      "Epoch 9/200, Batch 358/938\n",
      "D loss: 1.064151, acc: 96% // tar acc: 96% // adv loss: 0.416462, aux loss: 1.477807, tar loss: 0.793774\n",
      "=====================\n",
      "Epoch 9/200, Batch 408/938\n",
      "D loss: 1.091471, acc: 96% // tar acc: 92% // adv loss: 0.452035, aux loss: 1.492570, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 9/200, Batch 458/938\n",
      "D loss: 1.103553, acc: 97% // tar acc: 96% // adv loss: 0.399959, aux loss: 1.479969, tar loss: 0.772600\n",
      "=====================\n",
      "Epoch 9/200, Batch 508/938\n",
      "D loss: 1.068554, acc: 97% // tar acc: 96% // adv loss: 0.670090, aux loss: 1.481066, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 9/200, Batch 558/938\n",
      "D loss: 1.111141, acc: 96% // tar acc: 93% // adv loss: 0.361704, aux loss: 1.469661, tar loss: 0.738830\n",
      "=====================\n",
      "Epoch 9/200, Batch 608/938\n",
      "D loss: 1.033156, acc: 98% // tar acc: 89% // adv loss: 0.503755, aux loss: 1.470529, tar loss: 0.819050\n",
      "=====================\n",
      "Epoch 9/200, Batch 658/938\n",
      "D loss: 1.049058, acc: 97% // tar acc: 90% // adv loss: 0.451050, aux loss: 1.502306, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 9/200, Batch 708/938\n",
      "D loss: 1.089399, acc: 96% // tar acc: 93% // adv loss: 0.485919, aux loss: 1.490360, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 9/200, Batch 758/938\n",
      "D loss: 1.086305, acc: 94% // tar acc: 95% // adv loss: 0.528375, aux loss: 1.468805, tar loss: 0.766010\n",
      "=====================\n",
      "Epoch 9/200, Batch 808/938\n",
      "D loss: 1.062466, acc: 96% // tar acc: 95% // adv loss: 0.425436, aux loss: 1.490391, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 9/200, Batch 858/938\n",
      "D loss: 1.042971, acc: 96% // tar acc: 87% // adv loss: 0.659085, aux loss: 1.462953, tar loss: 0.815467\n",
      "=====================\n",
      "Epoch 9/200, Batch 908/938\n",
      "D loss: 1.136162, acc: 96% // tar acc: 96% // adv loss: 0.325364, aux loss: 1.474297, tar loss: 0.889386\n",
      "=====================\n",
      "Epoch 10/200, Batch 20/938\n",
      "D loss: 1.033122, acc: 97% // tar acc: 90% // adv loss: 0.697629, aux loss: 1.493861, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 10/200, Batch 70/938\n",
      "D loss: 1.041209, acc: 96% // tar acc: 96% // adv loss: 0.491030, aux loss: 1.462931, tar loss: 0.810410\n",
      "=====================\n",
      "Epoch 10/200, Batch 120/938\n",
      "D loss: 0.989022, acc: 98% // tar acc: 95% // adv loss: 0.471411, aux loss: 1.487239, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 10/200, Batch 170/938\n",
      "D loss: 1.106955, acc: 96% // tar acc: 95% // adv loss: 0.497512, aux loss: 1.489376, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 10/200, Batch 220/938\n",
      "D loss: 1.068485, acc: 95% // tar acc: 93% // adv loss: 0.509897, aux loss: 1.473384, tar loss: 0.764059\n",
      "=====================\n",
      "Epoch 10/200, Batch 270/938\n",
      "D loss: 1.038984, acc: 98% // tar acc: 85% // adv loss: 0.424045, aux loss: 1.470361, tar loss: 0.782000\n",
      "=====================\n",
      "Epoch 10/200, Batch 320/938\n",
      "D loss: 1.091317, acc: 93% // tar acc: 95% // adv loss: 0.464866, aux loss: 1.487104, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 10/200, Batch 370/938\n",
      "D loss: 1.083817, acc: 95% // tar acc: 93% // adv loss: 0.494241, aux loss: 1.487678, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 10/200, Batch 420/938\n",
      "D loss: 1.071335, acc: 96% // tar acc: 89% // adv loss: 0.613288, aux loss: 1.477985, tar loss: 0.774854\n",
      "=====================\n",
      "Epoch 10/200, Batch 470/938\n",
      "D loss: 1.066130, acc: 97% // tar acc: 95% // adv loss: 0.517696, aux loss: 1.483889, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 10/200, Batch 520/938\n",
      "D loss: 1.062715, acc: 95% // tar acc: 93% // adv loss: 0.539219, aux loss: 1.470766, tar loss: 0.861338\n",
      "=====================\n",
      "Epoch 10/200, Batch 570/938\n",
      "D loss: 1.072816, acc: 96% // tar acc: 85% // adv loss: 0.431775, aux loss: 1.485906, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 10/200, Batch 620/938\n",
      "D loss: 1.063528, acc: 100% // tar acc: 92% // adv loss: 0.446845, aux loss: 1.466632, tar loss: 0.821683\n",
      "=====================\n",
      "Epoch 10/200, Batch 670/938\n",
      "D loss: 1.057755, acc: 96% // tar acc: 95% // adv loss: 0.515426, aux loss: 1.514418, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 10/200, Batch 720/938\n",
      "D loss: 1.075813, acc: 96% // tar acc: 95% // adv loss: 0.463425, aux loss: 1.488500, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 10/200, Batch 770/938\n",
      "D loss: 1.038472, acc: 98% // tar acc: 90% // adv loss: 0.489975, aux loss: 1.478558, tar loss: 0.869015\n",
      "=====================\n",
      "Epoch 10/200, Batch 820/938\n",
      "D loss: 1.088002, acc: 96% // tar acc: 92% // adv loss: 0.525087, aux loss: 1.479363, tar loss: 0.838405\n",
      "=====================\n",
      "Epoch 10/200, Batch 870/938\n",
      "D loss: 1.044773, acc: 96% // tar acc: 93% // adv loss: 0.597664, aux loss: 1.507000, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 10/200, Batch 920/938\n",
      "D loss: 1.062141, acc: 98% // tar acc: 93% // adv loss: 0.459640, aux loss: 1.463599, tar loss: 0.842763\n",
      "=====================\n",
      "Epoch 11/200, Batch 32/938\n",
      "D loss: 1.023543, acc: 98% // tar acc: 95% // adv loss: 0.555134, aux loss: 1.468421, tar loss: 0.843400\n",
      "=====================\n",
      "Epoch 11/200, Batch 82/938\n",
      "D loss: 1.055901, acc: 92% // tar acc: 95% // adv loss: 0.462927, aux loss: 1.475019, tar loss: 0.749290\n",
      "=====================\n",
      "Epoch 11/200, Batch 132/938\n",
      "D loss: 1.066963, acc: 96% // tar acc: 90% // adv loss: 0.401090, aux loss: 1.482017, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 11/200, Batch 182/938\n",
      "D loss: 1.015560, acc: 96% // tar acc: 85% // adv loss: 0.620136, aux loss: 1.487732, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 11/200, Batch 232/938\n",
      "D loss: 1.086547, acc: 98% // tar acc: 96% // adv loss: 0.443512, aux loss: 1.464408, tar loss: 0.803240\n",
      "=====================\n",
      "Epoch 11/200, Batch 282/938\n",
      "D loss: 1.037488, acc: 97% // tar acc: 96% // adv loss: 0.623363, aux loss: 1.467136, tar loss: 0.890173\n",
      "=====================\n",
      "Epoch 11/200, Batch 332/938\n",
      "D loss: 1.076743, acc: 95% // tar acc: 98% // adv loss: 0.471479, aux loss: 1.465489, tar loss: 0.900299\n",
      "=====================\n",
      "Epoch 11/200, Batch 382/938\n",
      "D loss: 1.087274, acc: 95% // tar acc: 92% // adv loss: 0.335688, aux loss: 1.493829, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 11/200, Batch 432/938\n",
      "D loss: 1.084446, acc: 95% // tar acc: 87% // adv loss: 0.479216, aux loss: 1.480356, tar loss: 1.332000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 11/200, Batch 482/938\n",
      "D loss: 1.117317, acc: 93% // tar acc: 93% // adv loss: 0.467595, aux loss: 1.484904, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 11/200, Batch 532/938\n",
      "D loss: 1.040502, acc: 96% // tar acc: 90% // adv loss: 0.547382, aux loss: 1.497192, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 11/200, Batch 582/938\n",
      "D loss: 1.062354, acc: 96% // tar acc: 87% // adv loss: 0.540673, aux loss: 1.492989, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 11/200, Batch 632/938\n",
      "D loss: 1.067104, acc: 92% // tar acc: 95% // adv loss: 0.478966, aux loss: 1.490752, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 11/200, Batch 682/938\n",
      "D loss: 1.086284, acc: 96% // tar acc: 95% // adv loss: 0.443530, aux loss: 1.463122, tar loss: 0.866283\n",
      "=====================\n",
      "Epoch 11/200, Batch 732/938\n",
      "D loss: 1.079085, acc: 98% // tar acc: 96% // adv loss: 0.628863, aux loss: 1.479420, tar loss: 0.821029\n",
      "=====================\n",
      "Epoch 11/200, Batch 782/938\n",
      "D loss: 1.063595, acc: 99% // tar acc: 95% // adv loss: 0.471988, aux loss: 1.463088, tar loss: 0.845897\n",
      "=====================\n",
      "Epoch 11/200, Batch 832/938\n",
      "D loss: 1.036421, acc: 98% // tar acc: 95% // adv loss: 0.407602, aux loss: 1.483163, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 11/200, Batch 882/938\n",
      "D loss: 1.083552, acc: 96% // tar acc: 96% // adv loss: 0.532564, aux loss: 1.492081, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 11/200, Batch 932/938\n",
      "D loss: 1.049029, acc: 99% // tar acc: 96% // adv loss: 0.386290, aux loss: 1.469470, tar loss: 0.885799\n",
      "=====================\n",
      "Epoch 12/200, Batch 44/938\n",
      "D loss: 1.095615, acc: 97% // tar acc: 90% // adv loss: 0.394148, aux loss: 1.470649, tar loss: 0.838278\n",
      "=====================\n",
      "Epoch 12/200, Batch 94/938\n",
      "D loss: 1.042297, acc: 94% // tar acc: 93% // adv loss: 0.596837, aux loss: 1.486639, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 12/200, Batch 144/938\n",
      "D loss: 1.097346, acc: 98% // tar acc: 95% // adv loss: 0.392247, aux loss: 1.465940, tar loss: 0.788737\n",
      "=====================\n",
      "Epoch 12/200, Batch 194/938\n",
      "D loss: 1.052516, acc: 99% // tar acc: 93% // adv loss: 0.472223, aux loss: 1.462654, tar loss: 0.815001\n",
      "=====================\n",
      "Epoch 12/200, Batch 244/938\n",
      "D loss: 1.059960, acc: 96% // tar acc: 92% // adv loss: 0.542635, aux loss: 1.483059, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 12/200, Batch 294/938\n",
      "D loss: 1.097226, acc: 95% // tar acc: 95% // adv loss: 0.506909, aux loss: 1.483917, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 12/200, Batch 344/938\n",
      "D loss: 1.037098, acc: 95% // tar acc: 96% // adv loss: 0.450395, aux loss: 1.492053, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 12/200, Batch 394/938\n",
      "D loss: 1.071507, acc: 94% // tar acc: 96% // adv loss: 0.583560, aux loss: 1.492762, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 12/200, Batch 444/938\n",
      "D loss: 1.024276, acc: 96% // tar acc: 92% // adv loss: 0.525657, aux loss: 1.492948, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 12/200, Batch 494/938\n",
      "D loss: 1.111160, acc: 92% // tar acc: 95% // adv loss: 0.519076, aux loss: 1.486517, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 12/200, Batch 544/938\n",
      "D loss: 1.096622, acc: 99% // tar acc: 92% // adv loss: 0.542982, aux loss: 1.462934, tar loss: 0.830209\n",
      "=====================\n",
      "Epoch 12/200, Batch 594/938\n",
      "D loss: 1.101284, acc: 98% // tar acc: 96% // adv loss: 0.479212, aux loss: 1.470036, tar loss: 0.840554\n",
      "=====================\n",
      "Epoch 12/200, Batch 644/938\n",
      "D loss: 1.052619, acc: 93% // tar acc: 92% // adv loss: 0.476209, aux loss: 1.487547, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 12/200, Batch 694/938\n",
      "D loss: 1.096350, acc: 96% // tar acc: 95% // adv loss: 0.526298, aux loss: 1.473774, tar loss: 0.799932\n",
      "=====================\n",
      "Epoch 12/200, Batch 744/938\n",
      "D loss: 1.056671, acc: 94% // tar acc: 93% // adv loss: 0.609974, aux loss: 1.504490, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 12/200, Batch 794/938\n",
      "D loss: 1.091165, acc: 96% // tar acc: 90% // adv loss: 0.472822, aux loss: 1.464949, tar loss: 0.828139\n",
      "=====================\n",
      "Epoch 12/200, Batch 844/938\n",
      "D loss: 1.056400, acc: 97% // tar acc: 96% // adv loss: 0.510370, aux loss: 1.484195, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 12/200, Batch 894/938\n",
      "D loss: 1.034371, acc: 96% // tar acc: 89% // adv loss: 0.488277, aux loss: 1.470733, tar loss: 0.807121\n",
      "=====================\n",
      "Epoch 13/200, Batch 6/938\n",
      "D loss: 1.037381, acc: 97% // tar acc: 96% // adv loss: 0.560642, aux loss: 1.465182, tar loss: 0.819502\n",
      "=====================\n",
      "Epoch 13/200, Batch 56/938\n",
      "D loss: 1.076476, acc: 97% // tar acc: 95% // adv loss: 0.459375, aux loss: 1.486267, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 13/200, Batch 106/938\n",
      "D loss: 1.055514, acc: 96% // tar acc: 90% // adv loss: 0.397935, aux loss: 1.502150, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 13/200, Batch 156/938\n",
      "D loss: 1.076723, acc: 97% // tar acc: 96% // adv loss: 0.441798, aux loss: 1.478459, tar loss: 0.845733\n",
      "=====================\n",
      "Epoch 13/200, Batch 206/938\n",
      "D loss: 1.009129, acc: 96% // tar acc: 89% // adv loss: 0.642009, aux loss: 1.488751, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 13/200, Batch 256/938\n",
      "D loss: 1.051213, acc: 97% // tar acc: 93% // adv loss: 0.542081, aux loss: 1.483029, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 13/200, Batch 306/938\n",
      "D loss: 1.119287, acc: 95% // tar acc: 92% // adv loss: 0.464449, aux loss: 1.484957, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 13/200, Batch 356/938\n",
      "D loss: 1.072085, acc: 98% // tar acc: 85% // adv loss: 0.439867, aux loss: 1.472088, tar loss: 0.791472\n",
      "=====================\n",
      "Epoch 13/200, Batch 406/938\n",
      "D loss: 1.075957, acc: 96% // tar acc: 98% // adv loss: 0.511158, aux loss: 1.478565, tar loss: 0.853678\n",
      "=====================\n",
      "Epoch 13/200, Batch 456/938\n",
      "D loss: 1.057543, acc: 89% // tar acc: 89% // adv loss: 0.577075, aux loss: 1.545431, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 13/200, Batch 506/938\n",
      "D loss: 0.989165, acc: 97% // tar acc: 95% // adv loss: 0.474113, aux loss: 1.484799, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 13/200, Batch 556/938\n",
      "D loss: 1.034806, acc: 95% // tar acc: 90% // adv loss: 0.520571, aux loss: 1.499303, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 13/200, Batch 606/938\n",
      "D loss: 1.027947, acc: 97% // tar acc: 87% // adv loss: 0.538842, aux loss: 1.484559, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 13/200, Batch 656/938\n",
      "D loss: 1.026550, acc: 96% // tar acc: 96% // adv loss: 0.724764, aux loss: 1.487803, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 13/200, Batch 706/938\n",
      "D loss: 1.050981, acc: 98% // tar acc: 96% // adv loss: 0.440012, aux loss: 1.467172, tar loss: 0.878079\n",
      "=====================\n",
      "Epoch 13/200, Batch 756/938\n",
      "D loss: 1.032860, acc: 97% // tar acc: 93% // adv loss: 0.411705, aux loss: 1.466980, tar loss: 0.785061\n",
      "=====================\n",
      "Epoch 13/200, Batch 806/938\n",
      "D loss: 1.044114, acc: 97% // tar acc: 93% // adv loss: 0.528097, aux loss: 1.481455, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 13/200, Batch 856/938\n",
      "D loss: 1.045387, acc: 96% // tar acc: 92% // adv loss: 0.514731, aux loss: 1.466913, tar loss: 0.864156\n",
      "=====================\n",
      "Epoch 13/200, Batch 906/938\n",
      "D loss: 1.039080, acc: 96% // tar acc: 96% // adv loss: 0.521618, aux loss: 1.480526, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 14/200, Batch 18/938\n",
      "D loss: 1.005829, acc: 100% // tar acc: 89% // adv loss: 0.572535, aux loss: 1.466477, tar loss: 0.789119\n",
      "=====================\n",
      "Epoch 14/200, Batch 68/938\n",
      "D loss: 1.061924, acc: 98% // tar acc: 100% // adv loss: 0.533523, aux loss: 1.463718, tar loss: 0.811908\n",
      "=====================\n",
      "Epoch 14/200, Batch 118/938\n",
      "D loss: 1.068858, acc: 96% // tar acc: 95% // adv loss: 0.547119, aux loss: 1.492873, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 14/200, Batch 168/938\n",
      "D loss: 1.136538, acc: 96% // tar acc: 93% // adv loss: 0.526171, aux loss: 1.488963, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 14/200, Batch 218/938\n",
      "D loss: 1.060045, acc: 97% // tar acc: 85% // adv loss: 0.489522, aux loss: 1.472601, tar loss: 0.728024\n",
      "=====================\n",
      "Epoch 14/200, Batch 268/938\n",
      "D loss: 1.055460, acc: 99% // tar acc: 96% // adv loss: 0.429057, aux loss: 1.463568, tar loss: 0.837986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 14/200, Batch 318/938\n",
      "D loss: 1.107638, acc: 94% // tar acc: 93% // adv loss: 0.435724, aux loss: 1.476336, tar loss: 0.815366\n",
      "=====================\n",
      "Epoch 14/200, Batch 368/938\n",
      "D loss: 1.015589, acc: 97% // tar acc: 93% // adv loss: 0.468448, aux loss: 1.466042, tar loss: 0.827853\n",
      "=====================\n",
      "Epoch 14/200, Batch 418/938\n",
      "D loss: 1.047476, acc: 98% // tar acc: 95% // adv loss: 0.457158, aux loss: 1.470688, tar loss: 0.834009\n",
      "=====================\n",
      "Epoch 14/200, Batch 468/938\n",
      "D loss: 1.006595, acc: 98% // tar acc: 90% // adv loss: 0.496469, aux loss: 1.468117, tar loss: 0.791669\n",
      "=====================\n",
      "Epoch 14/200, Batch 518/938\n",
      "D loss: 1.093361, acc: 95% // tar acc: 100% // adv loss: 0.400446, aux loss: 1.465382, tar loss: 0.830124\n",
      "=====================\n",
      "Epoch 14/200, Batch 568/938\n",
      "D loss: 1.013355, acc: 96% // tar acc: 92% // adv loss: 0.627439, aux loss: 1.522439, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 14/200, Batch 618/938\n",
      "D loss: 1.091029, acc: 96% // tar acc: 95% // adv loss: 0.665167, aux loss: 1.463096, tar loss: 0.831894\n",
      "=====================\n",
      "Epoch 14/200, Batch 668/938\n",
      "D loss: 1.095214, acc: 95% // tar acc: 95% // adv loss: 0.541423, aux loss: 1.490022, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 14/200, Batch 718/938\n",
      "D loss: 1.107347, acc: 96% // tar acc: 96% // adv loss: 0.539968, aux loss: 1.482975, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 14/200, Batch 768/938\n",
      "D loss: 1.126536, acc: 92% // tar acc: 85% // adv loss: 0.611435, aux loss: 1.514448, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 14/200, Batch 818/938\n",
      "D loss: 1.041908, acc: 98% // tar acc: 87% // adv loss: 0.554386, aux loss: 1.469952, tar loss: 0.737177\n",
      "=====================\n",
      "Epoch 14/200, Batch 868/938\n",
      "D loss: 1.025997, acc: 100% // tar acc: 95% // adv loss: 0.444700, aux loss: 1.488079, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 14/200, Batch 918/938\n",
      "D loss: 1.039757, acc: 96% // tar acc: 96% // adv loss: 0.556715, aux loss: 1.470660, tar loss: 0.851343\n",
      "=====================\n",
      "Epoch 15/200, Batch 30/938\n",
      "D loss: 1.012460, acc: 97% // tar acc: 90% // adv loss: 0.430826, aux loss: 1.483189, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 15/200, Batch 80/938\n",
      "D loss: 0.981280, acc: 96% // tar acc: 96% // adv loss: 0.560392, aux loss: 1.482092, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 15/200, Batch 130/938\n",
      "D loss: 1.075342, acc: 96% // tar acc: 89% // adv loss: 0.426390, aux loss: 1.487922, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 15/200, Batch 180/938\n",
      "D loss: 1.111451, acc: 95% // tar acc: 96% // adv loss: 0.609361, aux loss: 1.462847, tar loss: 0.880882\n",
      "=====================\n",
      "Epoch 15/200, Batch 230/938\n",
      "D loss: 1.060922, acc: 96% // tar acc: 93% // adv loss: 0.550420, aux loss: 1.472681, tar loss: 0.828680\n",
      "=====================\n",
      "Epoch 15/200, Batch 280/938\n",
      "D loss: 1.053229, acc: 100% // tar acc: 93% // adv loss: 0.363246, aux loss: 1.467664, tar loss: 0.810701\n",
      "=====================\n",
      "Epoch 15/200, Batch 330/938\n",
      "D loss: 1.079485, acc: 99% // tar acc: 96% // adv loss: 0.388572, aux loss: 1.488857, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 15/200, Batch 380/938\n",
      "D loss: 1.040553, acc: 98% // tar acc: 93% // adv loss: 0.568586, aux loss: 1.464596, tar loss: 0.833645\n",
      "=====================\n",
      "Epoch 15/200, Batch 430/938\n",
      "D loss: 1.068370, acc: 96% // tar acc: 92% // adv loss: 0.474772, aux loss: 1.474027, tar loss: 0.883459\n",
      "=====================\n",
      "Epoch 15/200, Batch 480/938\n",
      "D loss: 1.104182, acc: 98% // tar acc: 92% // adv loss: 0.486149, aux loss: 1.475159, tar loss: 0.788643\n",
      "=====================\n",
      "Epoch 15/200, Batch 530/938\n",
      "D loss: 1.017973, acc: 97% // tar acc: 96% // adv loss: 0.538763, aux loss: 1.468555, tar loss: 0.855978\n",
      "=====================\n",
      "Epoch 15/200, Batch 580/938\n",
      "D loss: 1.081002, acc: 96% // tar acc: 98% // adv loss: 0.443951, aux loss: 1.465289, tar loss: 0.805929\n",
      "=====================\n",
      "Epoch 15/200, Batch 630/938\n",
      "D loss: 1.065961, acc: 96% // tar acc: 93% // adv loss: 0.552179, aux loss: 1.469729, tar loss: 0.841508\n",
      "=====================\n",
      "Epoch 15/200, Batch 680/938\n",
      "D loss: 1.055568, acc: 97% // tar acc: 90% // adv loss: 0.474878, aux loss: 1.479765, tar loss: 0.829043\n",
      "=====================\n",
      "Epoch 15/200, Batch 730/938\n",
      "D loss: 1.053481, acc: 98% // tar acc: 93% // adv loss: 0.379382, aux loss: 1.487249, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 15/200, Batch 780/938\n",
      "D loss: 1.050130, acc: 96% // tar acc: 95% // adv loss: 0.399246, aux loss: 1.467551, tar loss: 0.829624\n",
      "=====================\n",
      "Epoch 15/200, Batch 830/938\n",
      "D loss: 1.032029, acc: 96% // tar acc: 90% // adv loss: 0.445460, aux loss: 1.487575, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 15/200, Batch 880/938\n",
      "D loss: 1.111879, acc: 98% // tar acc: 93% // adv loss: 0.439039, aux loss: 1.462112, tar loss: 0.785888\n",
      "=====================\n",
      "Epoch 15/200, Batch 930/938\n",
      "D loss: 1.073823, acc: 98% // tar acc: 93% // adv loss: 0.432375, aux loss: 1.478341, tar loss: 0.857342\n",
      "=====================\n",
      "Epoch 16/200, Batch 42/938\n",
      "D loss: 1.013330, acc: 96% // tar acc: 95% // adv loss: 0.447451, aux loss: 1.477825, tar loss: 0.766531\n",
      "=====================\n",
      "Epoch 16/200, Batch 92/938\n",
      "D loss: 1.063459, acc: 97% // tar acc: 89% // adv loss: 0.452966, aux loss: 1.483082, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 16/200, Batch 142/938\n",
      "D loss: 1.022396, acc: 97% // tar acc: 95% // adv loss: 0.451546, aux loss: 1.467063, tar loss: 0.847056\n",
      "=====================\n",
      "Epoch 16/200, Batch 192/938\n",
      "D loss: 1.089756, acc: 93% // tar acc: 95% // adv loss: 0.502179, aux loss: 1.490699, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 16/200, Batch 242/938\n",
      "D loss: 1.053059, acc: 100% // tar acc: 92% // adv loss: 0.464840, aux loss: 1.497033, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 16/200, Batch 292/938\n",
      "D loss: 1.055006, acc: 95% // tar acc: 96% // adv loss: 0.425661, aux loss: 1.484667, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 16/200, Batch 342/938\n",
      "D loss: 1.079932, acc: 98% // tar acc: 95% // adv loss: 0.458802, aux loss: 1.478717, tar loss: 0.833691\n",
      "=====================\n",
      "Epoch 16/200, Batch 392/938\n",
      "D loss: 1.050157, acc: 96% // tar acc: 96% // adv loss: 0.546364, aux loss: 1.462958, tar loss: 0.804067\n",
      "=====================\n",
      "Epoch 16/200, Batch 442/938\n",
      "D loss: 1.019780, acc: 98% // tar acc: 96% // adv loss: 0.338755, aux loss: 1.464866, tar loss: 0.839322\n",
      "=====================\n",
      "Epoch 16/200, Batch 492/938\n",
      "D loss: 1.030186, acc: 99% // tar acc: 95% // adv loss: 0.390186, aux loss: 1.466144, tar loss: 0.807340\n",
      "=====================\n",
      "Epoch 16/200, Batch 542/938\n",
      "D loss: 1.016254, acc: 98% // tar acc: 95% // adv loss: 0.660325, aux loss: 1.482301, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 16/200, Batch 592/938\n",
      "D loss: 1.102365, acc: 97% // tar acc: 98% // adv loss: 0.510899, aux loss: 1.461247, tar loss: 0.932133\n",
      "=====================\n",
      "Epoch 16/200, Batch 642/938\n",
      "D loss: 1.095075, acc: 96% // tar acc: 92% // adv loss: 0.537704, aux loss: 1.465704, tar loss: 0.788224\n",
      "=====================\n",
      "Epoch 16/200, Batch 692/938\n",
      "D loss: 1.092669, acc: 98% // tar acc: 95% // adv loss: 0.478988, aux loss: 1.462260, tar loss: 0.946878\n",
      "=====================\n",
      "Epoch 16/200, Batch 742/938\n",
      "D loss: 1.037829, acc: 97% // tar acc: 92% // adv loss: 0.558097, aux loss: 1.465412, tar loss: 0.764541\n",
      "=====================\n",
      "Epoch 16/200, Batch 792/938\n",
      "D loss: 1.090447, acc: 94% // tar acc: 95% // adv loss: 0.453641, aux loss: 1.471666, tar loss: 0.802518\n",
      "=====================\n",
      "Epoch 16/200, Batch 842/938\n",
      "D loss: 1.082762, acc: 98% // tar acc: 92% // adv loss: 0.471698, aux loss: 1.465835, tar loss: 0.874665\n",
      "=====================\n",
      "Epoch 16/200, Batch 892/938\n",
      "D loss: 1.059739, acc: 99% // tar acc: 92% // adv loss: 0.487108, aux loss: 1.473737, tar loss: 0.851184\n",
      "=====================\n",
      "Epoch 17/200, Batch 4/938\n",
      "D loss: 1.055855, acc: 96% // tar acc: 92% // adv loss: 0.485901, aux loss: 1.471861, tar loss: 0.800798\n",
      "=====================\n",
      "Epoch 17/200, Batch 54/938\n",
      "D loss: 1.015146, acc: 96% // tar acc: 85% // adv loss: 0.679738, aux loss: 1.485643, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 17/200, Batch 104/938\n",
      "D loss: 1.060440, acc: 95% // tar acc: 93% // adv loss: 0.399937, aux loss: 1.477367, tar loss: 0.862679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 17/200, Batch 154/938\n",
      "D loss: 1.044625, acc: 97% // tar acc: 93% // adv loss: 0.528385, aux loss: 1.470396, tar loss: 0.858459\n",
      "=====================\n",
      "Epoch 17/200, Batch 204/938\n",
      "D loss: 1.031228, acc: 96% // tar acc: 90% // adv loss: 0.458439, aux loss: 1.464366, tar loss: 0.835344\n",
      "=====================\n",
      "Epoch 17/200, Batch 254/938\n",
      "D loss: 1.031243, acc: 96% // tar acc: 87% // adv loss: 0.511144, aux loss: 1.489859, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 17/200, Batch 304/938\n",
      "D loss: 1.050540, acc: 96% // tar acc: 95% // adv loss: 0.472328, aux loss: 1.478202, tar loss: 0.863680\n",
      "=====================\n",
      "Epoch 17/200, Batch 354/938\n",
      "D loss: 1.077789, acc: 96% // tar acc: 95% // adv loss: 0.423263, aux loss: 1.467936, tar loss: 0.876373\n",
      "=====================\n",
      "Epoch 17/200, Batch 404/938\n",
      "D loss: 1.128628, acc: 97% // tar acc: 89% // adv loss: 0.552692, aux loss: 1.493412, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 17/200, Batch 454/938\n",
      "D loss: 1.003685, acc: 97% // tar acc: 93% // adv loss: 0.438770, aux loss: 1.462394, tar loss: 0.809145\n",
      "=====================\n",
      "Epoch 17/200, Batch 504/938\n",
      "D loss: 1.100450, acc: 98% // tar acc: 96% // adv loss: 0.363047, aux loss: 1.476829, tar loss: 0.839597\n",
      "=====================\n",
      "Epoch 17/200, Batch 554/938\n",
      "D loss: 1.062804, acc: 97% // tar acc: 100% // adv loss: 0.406206, aux loss: 1.464237, tar loss: 0.896570\n",
      "=====================\n",
      "Epoch 17/200, Batch 604/938\n",
      "D loss: 1.056128, acc: 95% // tar acc: 95% // adv loss: 0.473660, aux loss: 1.474666, tar loss: 0.792792\n",
      "=====================\n",
      "Epoch 17/200, Batch 654/938\n",
      "D loss: 1.062512, acc: 99% // tar acc: 95% // adv loss: 0.455273, aux loss: 1.470228, tar loss: 0.796834\n",
      "=====================\n",
      "Epoch 17/200, Batch 704/938\n",
      "D loss: 1.043222, acc: 98% // tar acc: 95% // adv loss: 0.467020, aux loss: 1.462190, tar loss: 0.819908\n",
      "=====================\n",
      "Epoch 17/200, Batch 754/938\n",
      "D loss: 1.022825, acc: 96% // tar acc: 93% // adv loss: 0.521359, aux loss: 1.493760, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 17/200, Batch 804/938\n",
      "D loss: 1.061484, acc: 99% // tar acc: 95% // adv loss: 0.443791, aux loss: 1.510668, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 17/200, Batch 854/938\n",
      "D loss: 1.071255, acc: 94% // tar acc: 92% // adv loss: 0.657101, aux loss: 1.514985, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 17/200, Batch 904/938\n",
      "D loss: 1.066432, acc: 98% // tar acc: 89% // adv loss: 0.560461, aux loss: 1.486867, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 18/200, Batch 16/938\n",
      "D loss: 1.056137, acc: 98% // tar acc: 85% // adv loss: 0.484488, aux loss: 1.465992, tar loss: 0.804225\n",
      "=====================\n",
      "Epoch 18/200, Batch 66/938\n",
      "D loss: 1.070777, acc: 96% // tar acc: 89% // adv loss: 0.435940, aux loss: 1.489666, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 18/200, Batch 116/938\n",
      "D loss: 1.046707, acc: 100% // tar acc: 96% // adv loss: 0.623918, aux loss: 1.471217, tar loss: 0.837491\n",
      "=====================\n",
      "Epoch 18/200, Batch 166/938\n",
      "D loss: 1.035496, acc: 96% // tar acc: 87% // adv loss: 0.489803, aux loss: 1.506149, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 18/200, Batch 216/938\n",
      "D loss: 1.046537, acc: 96% // tar acc: 96% // adv loss: 0.406721, aux loss: 1.470864, tar loss: 0.840308\n",
      "=====================\n",
      "Epoch 18/200, Batch 266/938\n",
      "D loss: 1.067353, acc: 97% // tar acc: 95% // adv loss: 0.582200, aux loss: 1.492016, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 18/200, Batch 316/938\n",
      "D loss: 1.063518, acc: 95% // tar acc: 90% // adv loss: 0.496476, aux loss: 1.506401, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 18/200, Batch 366/938\n",
      "D loss: 1.092782, acc: 100% // tar acc: 92% // adv loss: 0.518275, aux loss: 1.467556, tar loss: 0.781066\n",
      "=====================\n",
      "Epoch 18/200, Batch 416/938\n",
      "D loss: 1.033915, acc: 96% // tar acc: 89% // adv loss: 0.595471, aux loss: 1.463557, tar loss: 0.767314\n",
      "=====================\n",
      "Epoch 18/200, Batch 466/938\n",
      "D loss: 1.068307, acc: 96% // tar acc: 93% // adv loss: 0.383126, aux loss: 1.484855, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 18/200, Batch 516/938\n",
      "D loss: 1.078730, acc: 98% // tar acc: 96% // adv loss: 0.540923, aux loss: 1.462643, tar loss: 0.757420\n",
      "=====================\n",
      "Epoch 18/200, Batch 566/938\n",
      "D loss: 1.041808, acc: 100% // tar acc: 95% // adv loss: 0.519758, aux loss: 1.473094, tar loss: 0.871426\n",
      "=====================\n",
      "Epoch 18/200, Batch 616/938\n",
      "D loss: 1.084539, acc: 96% // tar acc: 95% // adv loss: 0.610213, aux loss: 1.481510, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 18/200, Batch 666/938\n",
      "D loss: 1.011581, acc: 100% // tar acc: 95% // adv loss: 0.413208, aux loss: 1.483326, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 18/200, Batch 716/938\n",
      "D loss: 1.058029, acc: 97% // tar acc: 100% // adv loss: 0.654715, aux loss: 1.462814, tar loss: 0.857194\n",
      "=====================\n",
      "Epoch 18/200, Batch 766/938\n",
      "D loss: 1.044912, acc: 100% // tar acc: 93% // adv loss: 0.405325, aux loss: 1.495890, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 18/200, Batch 816/938\n",
      "D loss: 1.098175, acc: 100% // tar acc: 100% // adv loss: 0.343240, aux loss: 1.470847, tar loss: 0.884427\n",
      "=====================\n",
      "Epoch 18/200, Batch 866/938\n",
      "D loss: 0.998545, acc: 97% // tar acc: 98% // adv loss: 0.502393, aux loss: 1.464303, tar loss: 0.824226\n",
      "=====================\n",
      "Epoch 18/200, Batch 916/938\n",
      "D loss: 1.053212, acc: 96% // tar acc: 95% // adv loss: 0.399892, aux loss: 1.461500, tar loss: 0.888991\n",
      "=====================\n",
      "Epoch 19/200, Batch 28/938\n",
      "D loss: 1.043583, acc: 98% // tar acc: 96% // adv loss: 0.533354, aux loss: 1.471949, tar loss: 0.841916\n",
      "=====================\n",
      "Epoch 19/200, Batch 78/938\n",
      "D loss: 1.046458, acc: 93% // tar acc: 85% // adv loss: 0.561570, aux loss: 1.493544, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 19/200, Batch 128/938\n",
      "D loss: 1.092106, acc: 99% // tar acc: 95% // adv loss: 0.549380, aux loss: 1.472987, tar loss: 0.811646\n",
      "=====================\n",
      "Epoch 19/200, Batch 178/938\n",
      "D loss: 1.087784, acc: 98% // tar acc: 93% // adv loss: 0.593014, aux loss: 1.474883, tar loss: 0.781121\n",
      "=====================\n",
      "Epoch 19/200, Batch 228/938\n",
      "D loss: 1.063444, acc: 96% // tar acc: 92% // adv loss: 0.565322, aux loss: 1.487309, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 19/200, Batch 278/938\n",
      "D loss: 1.010549, acc: 99% // tar acc: 93% // adv loss: 0.460653, aux loss: 1.463307, tar loss: 0.839668\n",
      "=====================\n",
      "Epoch 19/200, Batch 328/938\n",
      "D loss: 1.046109, acc: 98% // tar acc: 92% // adv loss: 0.562969, aux loss: 1.490076, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 19/200, Batch 378/938\n",
      "D loss: 1.095069, acc: 98% // tar acc: 89% // adv loss: 0.446343, aux loss: 1.465088, tar loss: 0.743524\n",
      "=====================\n",
      "Epoch 19/200, Batch 428/938\n",
      "D loss: 1.078587, acc: 94% // tar acc: 95% // adv loss: 0.495032, aux loss: 1.463165, tar loss: 0.858751\n",
      "=====================\n",
      "Epoch 19/200, Batch 478/938\n",
      "D loss: 1.065393, acc: 96% // tar acc: 89% // adv loss: 0.606630, aux loss: 1.472775, tar loss: 0.829606\n",
      "=====================\n",
      "Epoch 19/200, Batch 528/938\n",
      "D loss: 1.046535, acc: 98% // tar acc: 92% // adv loss: 0.539093, aux loss: 1.467413, tar loss: 0.823192\n",
      "=====================\n",
      "Epoch 19/200, Batch 578/938\n",
      "D loss: 1.071704, acc: 96% // tar acc: 93% // adv loss: 0.397704, aux loss: 1.505059, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 19/200, Batch 628/938\n",
      "D loss: 1.076426, acc: 96% // tar acc: 95% // adv loss: 0.588261, aux loss: 1.493422, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 19/200, Batch 678/938\n",
      "D loss: 1.098299, acc: 97% // tar acc: 100% // adv loss: 0.476705, aux loss: 1.475399, tar loss: 0.868876\n",
      "=====================\n",
      "Epoch 19/200, Batch 728/938\n",
      "D loss: 1.082094, acc: 96% // tar acc: 96% // adv loss: 0.439777, aux loss: 1.491513, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 19/200, Batch 778/938\n",
      "D loss: 1.074773, acc: 98% // tar acc: 87% // adv loss: 0.581677, aux loss: 1.494214, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 19/200, Batch 828/938\n",
      "D loss: 1.041945, acc: 95% // tar acc: 92% // adv loss: 0.601003, aux loss: 1.492846, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 19/200, Batch 878/938\n",
      "D loss: 0.992876, acc: 99% // tar acc: 92% // adv loss: 0.446085, aux loss: 1.462236, tar loss: 0.758941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 19/200, Batch 928/938\n",
      "D loss: 1.057839, acc: 97% // tar acc: 96% // adv loss: 0.398038, aux loss: 1.468436, tar loss: 0.882005\n",
      "=====================\n",
      "Epoch 20/200, Batch 40/938\n",
      "D loss: 1.002024, acc: 98% // tar acc: 100% // adv loss: 0.459021, aux loss: 1.486668, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 20/200, Batch 90/938\n",
      "D loss: 1.102981, acc: 97% // tar acc: 98% // adv loss: 0.431400, aux loss: 1.469283, tar loss: 0.840705\n",
      "=====================\n",
      "Epoch 20/200, Batch 140/938\n",
      "D loss: 1.097633, acc: 95% // tar acc: 93% // adv loss: 0.460829, aux loss: 1.478471, tar loss: 0.723888\n",
      "=====================\n",
      "Epoch 20/200, Batch 190/938\n",
      "D loss: 1.084042, acc: 96% // tar acc: 96% // adv loss: 0.369780, aux loss: 1.479822, tar loss: 0.821866\n",
      "=====================\n",
      "Epoch 20/200, Batch 240/938\n",
      "D loss: 1.040588, acc: 96% // tar acc: 96% // adv loss: 0.530445, aux loss: 1.473180, tar loss: 0.860847\n",
      "=====================\n",
      "Epoch 20/200, Batch 290/938\n",
      "D loss: 1.098489, acc: 96% // tar acc: 98% // adv loss: 0.363184, aux loss: 1.462096, tar loss: 0.873383\n",
      "=====================\n",
      "Epoch 20/200, Batch 340/938\n",
      "D loss: 1.048095, acc: 99% // tar acc: 96% // adv loss: 0.365388, aux loss: 1.471816, tar loss: 0.818014\n",
      "=====================\n",
      "Epoch 20/200, Batch 390/938\n",
      "D loss: 1.030614, acc: 97% // tar acc: 92% // adv loss: 0.518222, aux loss: 1.471360, tar loss: 0.796765\n",
      "=====================\n",
      "Epoch 20/200, Batch 440/938\n",
      "D loss: 1.091261, acc: 98% // tar acc: 90% // adv loss: 0.374002, aux loss: 1.479350, tar loss: 0.840567\n",
      "=====================\n",
      "Epoch 20/200, Batch 490/938\n",
      "D loss: 1.049396, acc: 97% // tar acc: 85% // adv loss: 0.480832, aux loss: 1.483524, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 20/200, Batch 540/938\n",
      "D loss: 1.016545, acc: 97% // tar acc: 85% // adv loss: 0.539839, aux loss: 1.476363, tar loss: 0.798080\n",
      "=====================\n",
      "Epoch 20/200, Batch 590/938\n",
      "D loss: 1.055856, acc: 98% // tar acc: 92% // adv loss: 0.450280, aux loss: 1.468602, tar loss: 0.799691\n",
      "=====================\n",
      "Epoch 20/200, Batch 640/938\n",
      "D loss: 1.086773, acc: 97% // tar acc: 89% // adv loss: 0.594117, aux loss: 1.476348, tar loss: 0.831037\n",
      "=====================\n",
      "Epoch 20/200, Batch 690/938\n",
      "D loss: 1.103995, acc: 95% // tar acc: 93% // adv loss: 0.428958, aux loss: 1.521679, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 20/200, Batch 740/938\n",
      "D loss: 1.058122, acc: 97% // tar acc: 100% // adv loss: 0.507376, aux loss: 1.463380, tar loss: 0.861181\n",
      "=====================\n",
      "Epoch 20/200, Batch 790/938\n",
      "D loss: 1.042321, acc: 97% // tar acc: 95% // adv loss: 0.551517, aux loss: 1.468380, tar loss: 0.810169\n",
      "=====================\n",
      "Epoch 20/200, Batch 840/938\n",
      "D loss: 1.050077, acc: 95% // tar acc: 95% // adv loss: 0.298800, aux loss: 1.463407, tar loss: 0.831522\n",
      "=====================\n",
      "Epoch 20/200, Batch 890/938\n",
      "D loss: 1.062721, acc: 98% // tar acc: 95% // adv loss: 0.388665, aux loss: 1.479783, tar loss: 0.837455\n",
      "=====================\n",
      "Epoch 21/200, Batch 2/938\n",
      "D loss: 1.039247, acc: 96% // tar acc: 85% // adv loss: 0.481369, aux loss: 1.485926, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 21/200, Batch 52/938\n",
      "D loss: 1.089154, acc: 99% // tar acc: 93% // adv loss: 0.281483, aux loss: 1.466370, tar loss: 0.821302\n",
      "=====================\n",
      "Epoch 21/200, Batch 102/938\n",
      "D loss: 1.079594, acc: 98% // tar acc: 95% // adv loss: 0.698056, aux loss: 1.462987, tar loss: 0.853330\n",
      "=====================\n",
      "Epoch 21/200, Batch 152/938\n",
      "D loss: 1.064435, acc: 96% // tar acc: 96% // adv loss: 0.539934, aux loss: 1.473701, tar loss: 0.836889\n",
      "=====================\n",
      "Epoch 21/200, Batch 202/938\n",
      "D loss: 1.092078, acc: 96% // tar acc: 90% // adv loss: 0.461919, aux loss: 1.477199, tar loss: 0.884544\n",
      "=====================\n",
      "Epoch 21/200, Batch 252/938\n",
      "D loss: 1.081349, acc: 97% // tar acc: 95% // adv loss: 0.469794, aux loss: 1.476260, tar loss: 0.774709\n",
      "=====================\n",
      "Epoch 21/200, Batch 302/938\n",
      "D loss: 1.035727, acc: 99% // tar acc: 96% // adv loss: 0.453384, aux loss: 1.490874, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 21/200, Batch 352/938\n",
      "D loss: 1.012801, acc: 98% // tar acc: 96% // adv loss: 0.548831, aux loss: 1.465176, tar loss: 0.821995\n",
      "=====================\n",
      "Epoch 21/200, Batch 402/938\n",
      "D loss: 1.050314, acc: 99% // tar acc: 82% // adv loss: 0.398318, aux loss: 1.476096, tar loss: 0.758894\n",
      "=====================\n",
      "Epoch 21/200, Batch 452/938\n",
      "D loss: 1.048280, acc: 97% // tar acc: 85% // adv loss: 0.465129, aux loss: 1.486182, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 21/200, Batch 502/938\n",
      "D loss: 1.066348, acc: 97% // tar acc: 93% // adv loss: 0.414720, aux loss: 1.463370, tar loss: 0.816785\n",
      "=====================\n",
      "Epoch 21/200, Batch 552/938\n",
      "D loss: 1.071872, acc: 97% // tar acc: 92% // adv loss: 0.557133, aux loss: 1.463017, tar loss: 0.777897\n",
      "=====================\n",
      "Epoch 21/200, Batch 602/938\n",
      "D loss: 1.123221, acc: 96% // tar acc: 90% // adv loss: 0.533984, aux loss: 1.485143, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 21/200, Batch 652/938\n",
      "D loss: 0.992698, acc: 96% // tar acc: 93% // adv loss: 0.432797, aux loss: 1.469374, tar loss: 0.818457\n",
      "=====================\n",
      "Epoch 21/200, Batch 702/938\n",
      "D loss: 1.163271, acc: 97% // tar acc: 95% // adv loss: 0.408385, aux loss: 1.468788, tar loss: 0.828373\n",
      "=====================\n",
      "Epoch 21/200, Batch 752/938\n",
      "D loss: 1.075795, acc: 97% // tar acc: 95% // adv loss: 0.401219, aux loss: 1.478383, tar loss: 0.826177\n",
      "=====================\n",
      "Epoch 21/200, Batch 802/938\n",
      "D loss: 1.064215, acc: 98% // tar acc: 98% // adv loss: 0.362985, aux loss: 1.465631, tar loss: 0.864804\n",
      "=====================\n",
      "Epoch 21/200, Batch 852/938\n",
      "D loss: 1.039988, acc: 97% // tar acc: 90% // adv loss: 0.573645, aux loss: 1.464439, tar loss: 0.765392\n",
      "=====================\n",
      "Epoch 21/200, Batch 902/938\n",
      "D loss: 1.141833, acc: 99% // tar acc: 93% // adv loss: 0.470263, aux loss: 1.471141, tar loss: 0.890989\n",
      "=====================\n",
      "Epoch 22/200, Batch 14/938\n",
      "D loss: 1.032449, acc: 100% // tar acc: 96% // adv loss: 0.435189, aux loss: 1.464908, tar loss: 0.893570\n",
      "=====================\n",
      "Epoch 22/200, Batch 64/938\n",
      "D loss: 1.049327, acc: 97% // tar acc: 96% // adv loss: 0.591369, aux loss: 1.483598, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 22/200, Batch 114/938\n",
      "D loss: 1.071916, acc: 96% // tar acc: 92% // adv loss: 0.504619, aux loss: 1.506010, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 22/200, Batch 164/938\n",
      "D loss: 1.129017, acc: 99% // tar acc: 95% // adv loss: 0.491272, aux loss: 1.464407, tar loss: 0.840443\n",
      "=====================\n",
      "Epoch 22/200, Batch 214/938\n",
      "D loss: 1.045102, acc: 97% // tar acc: 89% // adv loss: 0.681093, aux loss: 1.501936, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 22/200, Batch 264/938\n",
      "D loss: 1.074775, acc: 98% // tar acc: 96% // adv loss: 0.511267, aux loss: 1.476499, tar loss: 0.874570\n",
      "=====================\n",
      "Epoch 22/200, Batch 314/938\n",
      "D loss: 1.063084, acc: 96% // tar acc: 93% // adv loss: 0.399339, aux loss: 1.489333, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 22/200, Batch 364/938\n",
      "D loss: 1.004792, acc: 99% // tar acc: 93% // adv loss: 0.472838, aux loss: 1.478426, tar loss: 0.812003\n",
      "=====================\n",
      "Epoch 22/200, Batch 414/938\n",
      "D loss: 1.034731, acc: 96% // tar acc: 89% // adv loss: 0.615310, aux loss: 1.484506, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 22/200, Batch 464/938\n",
      "D loss: 1.061204, acc: 95% // tar acc: 87% // adv loss: 0.445300, aux loss: 1.484777, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 22/200, Batch 514/938\n",
      "D loss: 1.064583, acc: 97% // tar acc: 95% // adv loss: 0.516720, aux loss: 1.470665, tar loss: 0.839274\n",
      "=====================\n",
      "Epoch 22/200, Batch 564/938\n",
      "D loss: 0.959033, acc: 98% // tar acc: 92% // adv loss: 0.557576, aux loss: 1.483970, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 22/200, Batch 614/938\n",
      "D loss: 1.064299, acc: 99% // tar acc: 92% // adv loss: 0.519632, aux loss: 1.487942, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 22/200, Batch 664/938\n",
      "D loss: 1.044580, acc: 98% // tar acc: 92% // adv loss: 0.512455, aux loss: 1.463164, tar loss: 0.828511\n",
      "=====================\n",
      "Epoch 22/200, Batch 714/938\n",
      "D loss: 1.034308, acc: 97% // tar acc: 93% // adv loss: 0.504836, aux loss: 1.466151, tar loss: 0.831080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 22/200, Batch 764/938\n",
      "D loss: 1.057979, acc: 95% // tar acc: 89% // adv loss: 0.542869, aux loss: 1.479635, tar loss: 0.737531\n",
      "=====================\n",
      "Epoch 22/200, Batch 814/938\n",
      "D loss: 1.063071, acc: 98% // tar acc: 92% // adv loss: 0.394349, aux loss: 1.473901, tar loss: 0.834650\n",
      "=====================\n",
      "Epoch 22/200, Batch 864/938\n",
      "D loss: 1.087904, acc: 96% // tar acc: 93% // adv loss: 0.491437, aux loss: 1.485924, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 22/200, Batch 914/938\n",
      "D loss: 1.074576, acc: 95% // tar acc: 93% // adv loss: 0.569989, aux loss: 1.489640, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 23/200, Batch 26/938\n",
      "D loss: 1.056245, acc: 99% // tar acc: 95% // adv loss: 0.560851, aux loss: 1.467506, tar loss: 0.886035\n",
      "=====================\n",
      "Epoch 23/200, Batch 76/938\n",
      "D loss: 1.068094, acc: 98% // tar acc: 93% // adv loss: 0.526117, aux loss: 1.465833, tar loss: 0.819993\n",
      "=====================\n",
      "Epoch 23/200, Batch 126/938\n",
      "D loss: 1.002164, acc: 99% // tar acc: 93% // adv loss: 0.489286, aux loss: 1.462266, tar loss: 0.818478\n",
      "=====================\n",
      "Epoch 23/200, Batch 176/938\n",
      "D loss: 1.087572, acc: 98% // tar acc: 93% // adv loss: 0.465962, aux loss: 1.493523, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 23/200, Batch 226/938\n",
      "D loss: 1.030888, acc: 100% // tar acc: 98% // adv loss: 0.430096, aux loss: 1.464074, tar loss: 0.890140\n",
      "=====================\n",
      "Epoch 23/200, Batch 276/938\n",
      "D loss: 1.069458, acc: 94% // tar acc: 92% // adv loss: 0.481740, aux loss: 1.463976, tar loss: 0.791187\n",
      "=====================\n",
      "Epoch 23/200, Batch 326/938\n",
      "D loss: 0.993652, acc: 98% // tar acc: 98% // adv loss: 0.388199, aux loss: 1.485116, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 23/200, Batch 376/938\n",
      "D loss: 1.092098, acc: 96% // tar acc: 98% // adv loss: 0.454177, aux loss: 1.485295, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 23/200, Batch 426/938\n",
      "D loss: 1.042340, acc: 96% // tar acc: 95% // adv loss: 0.435108, aux loss: 1.483712, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 23/200, Batch 476/938\n",
      "D loss: 1.067156, acc: 98% // tar acc: 96% // adv loss: 0.446262, aux loss: 1.463043, tar loss: 0.813926\n",
      "=====================\n",
      "Epoch 23/200, Batch 526/938\n",
      "D loss: 1.069811, acc: 98% // tar acc: 95% // adv loss: 0.480204, aux loss: 1.487847, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 23/200, Batch 576/938\n",
      "D loss: 1.092446, acc: 96% // tar acc: 96% // adv loss: 0.433103, aux loss: 1.490281, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 23/200, Batch 626/938\n",
      "D loss: 1.067144, acc: 97% // tar acc: 95% // adv loss: 0.440797, aux loss: 1.501663, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 23/200, Batch 676/938\n",
      "D loss: 1.044766, acc: 100% // tar acc: 90% // adv loss: 0.466079, aux loss: 1.464874, tar loss: 0.808508\n",
      "=====================\n",
      "Epoch 23/200, Batch 726/938\n",
      "D loss: 1.031853, acc: 99% // tar acc: 96% // adv loss: 0.499920, aux loss: 1.474451, tar loss: 0.803181\n",
      "=====================\n",
      "Epoch 23/200, Batch 776/938\n",
      "D loss: 1.015296, acc: 99% // tar acc: 95% // adv loss: 0.375780, aux loss: 1.462903, tar loss: 0.766966\n",
      "=====================\n",
      "Epoch 23/200, Batch 826/938\n",
      "D loss: 1.078291, acc: 97% // tar acc: 90% // adv loss: 0.391103, aux loss: 1.469779, tar loss: 0.784508\n",
      "=====================\n",
      "Epoch 23/200, Batch 876/938\n",
      "D loss: 1.048394, acc: 93% // tar acc: 90% // adv loss: 0.458691, aux loss: 1.469215, tar loss: 0.781051\n",
      "=====================\n",
      "Epoch 23/200, Batch 926/938\n",
      "D loss: 1.085999, acc: 98% // tar acc: 93% // adv loss: 0.361030, aux loss: 1.468825, tar loss: 0.827451\n",
      "=====================\n",
      "Epoch 24/200, Batch 38/938\n",
      "D loss: 1.087694, acc: 97% // tar acc: 95% // adv loss: 0.628435, aux loss: 1.467840, tar loss: 0.872165\n",
      "=====================\n",
      "Epoch 24/200, Batch 88/938\n",
      "D loss: 1.039957, acc: 98% // tar acc: 93% // adv loss: 0.619157, aux loss: 1.487485, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 24/200, Batch 138/938\n",
      "D loss: 1.034882, acc: 97% // tar acc: 90% // adv loss: 0.571964, aux loss: 1.467703, tar loss: 0.791735\n",
      "=====================\n",
      "Epoch 24/200, Batch 188/938\n",
      "D loss: 1.071657, acc: 95% // tar acc: 95% // adv loss: 0.446853, aux loss: 1.490354, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 24/200, Batch 238/938\n",
      "D loss: 1.138102, acc: 98% // tar acc: 96% // adv loss: 0.579727, aux loss: 1.493414, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 24/200, Batch 288/938\n",
      "D loss: 1.053433, acc: 97% // tar acc: 90% // adv loss: 0.435315, aux loss: 1.467116, tar loss: 0.825617\n",
      "=====================\n",
      "Epoch 24/200, Batch 338/938\n",
      "D loss: 1.041652, acc: 97% // tar acc: 98% // adv loss: 0.657838, aux loss: 1.467966, tar loss: 0.847527\n",
      "=====================\n",
      "Epoch 24/200, Batch 388/938\n",
      "D loss: 1.071480, acc: 99% // tar acc: 93% // adv loss: 0.522369, aux loss: 1.462162, tar loss: 0.852564\n",
      "=====================\n",
      "Epoch 24/200, Batch 438/938\n",
      "D loss: 1.071440, acc: 98% // tar acc: 96% // adv loss: 0.546308, aux loss: 1.471444, tar loss: 0.876751\n",
      "=====================\n",
      "Epoch 24/200, Batch 488/938\n",
      "D loss: 1.106021, acc: 99% // tar acc: 92% // adv loss: 0.412301, aux loss: 1.483800, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 24/200, Batch 538/938\n",
      "D loss: 1.070564, acc: 99% // tar acc: 96% // adv loss: 0.507347, aux loss: 1.461618, tar loss: 0.863811\n",
      "=====================\n",
      "Epoch 24/200, Batch 588/938\n",
      "D loss: 1.062502, acc: 98% // tar acc: 96% // adv loss: 0.441310, aux loss: 1.477912, tar loss: 0.781748\n",
      "=====================\n",
      "Epoch 24/200, Batch 638/938\n",
      "D loss: 1.089337, acc: 97% // tar acc: 96% // adv loss: 0.571123, aux loss: 1.491273, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 24/200, Batch 688/938\n",
      "D loss: 1.056279, acc: 98% // tar acc: 96% // adv loss: 0.463050, aux loss: 1.461684, tar loss: 0.836647\n",
      "=====================\n",
      "Epoch 24/200, Batch 738/938\n",
      "D loss: 1.067489, acc: 97% // tar acc: 96% // adv loss: 0.409271, aux loss: 1.476927, tar loss: 0.835199\n",
      "=====================\n",
      "Epoch 24/200, Batch 788/938\n",
      "D loss: 1.108313, acc: 96% // tar acc: 92% // adv loss: 0.593478, aux loss: 1.469926, tar loss: 0.819440\n",
      "=====================\n",
      "Epoch 24/200, Batch 838/938\n",
      "D loss: 1.099105, acc: 98% // tar acc: 96% // adv loss: 0.456223, aux loss: 1.462786, tar loss: 0.820014\n",
      "=====================\n",
      "Epoch 24/200, Batch 888/938\n",
      "D loss: 1.102341, acc: 97% // tar acc: 93% // adv loss: 0.579080, aux loss: 1.462875, tar loss: 0.828771\n",
      "=====================\n",
      "Epoch 25/200, Batch 0/938\n",
      "D loss: 1.041545, acc: 97% // tar acc: 93% // adv loss: 0.746234, aux loss: 1.462668, tar loss: 0.849600\n",
      "=====================\n",
      "Epoch 25/200, Batch 50/938\n",
      "D loss: 1.032133, acc: 97% // tar acc: 93% // adv loss: 0.641389, aux loss: 1.462315, tar loss: 0.802018\n",
      "=====================\n",
      "Epoch 25/200, Batch 100/938\n",
      "D loss: 1.071367, acc: 96% // tar acc: 92% // adv loss: 0.486754, aux loss: 1.500145, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 25/200, Batch 150/938\n",
      "D loss: 1.038689, acc: 100% // tar acc: 95% // adv loss: 0.534504, aux loss: 1.462673, tar loss: 0.817896\n",
      "=====================\n",
      "Epoch 25/200, Batch 200/938\n",
      "D loss: 1.025086, acc: 98% // tar acc: 87% // adv loss: 0.643024, aux loss: 1.487170, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 25/200, Batch 250/938\n",
      "D loss: 1.011338, acc: 97% // tar acc: 98% // adv loss: 0.416145, aux loss: 1.462168, tar loss: 0.829554\n",
      "=====================\n",
      "Epoch 25/200, Batch 300/938\n",
      "D loss: 1.078012, acc: 98% // tar acc: 93% // adv loss: 0.542500, aux loss: 1.462148, tar loss: 0.836816\n",
      "=====================\n",
      "Epoch 25/200, Batch 350/938\n",
      "D loss: 1.041474, acc: 99% // tar acc: 90% // adv loss: 0.594941, aux loss: 1.464100, tar loss: 0.788147\n",
      "=====================\n",
      "Epoch 25/200, Batch 400/938\n",
      "D loss: 1.060200, acc: 99% // tar acc: 95% // adv loss: 0.464005, aux loss: 1.471946, tar loss: 0.792444\n",
      "=====================\n",
      "Epoch 25/200, Batch 450/938\n",
      "D loss: 1.067136, acc: 96% // tar acc: 93% // adv loss: 0.527198, aux loss: 1.464840, tar loss: 0.853438\n",
      "=====================\n",
      "Epoch 25/200, Batch 500/938\n",
      "D loss: 1.019474, acc: 99% // tar acc: 92% // adv loss: 0.432884, aux loss: 1.480757, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 25/200, Batch 550/938\n",
      "D loss: 1.049891, acc: 100% // tar acc: 98% // adv loss: 0.475199, aux loss: 1.466200, tar loss: 0.869532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 25/200, Batch 600/938\n",
      "D loss: 0.998095, acc: 98% // tar acc: 96% // adv loss: 0.513509, aux loss: 1.482173, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 25/200, Batch 650/938\n",
      "D loss: 1.124963, acc: 98% // tar acc: 95% // adv loss: 0.335473, aux loss: 1.463502, tar loss: 0.865625\n",
      "=====================\n",
      "Epoch 25/200, Batch 700/938\n",
      "D loss: 1.074377, acc: 96% // tar acc: 95% // adv loss: 0.422112, aux loss: 1.482538, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 25/200, Batch 750/938\n",
      "D loss: 1.026593, acc: 98% // tar acc: 93% // adv loss: 0.463474, aux loss: 1.464296, tar loss: 0.792768\n",
      "=====================\n",
      "Epoch 25/200, Batch 800/938\n",
      "D loss: 1.098616, acc: 98% // tar acc: 95% // adv loss: 0.340561, aux loss: 1.461456, tar loss: 0.910775\n",
      "=====================\n",
      "Epoch 25/200, Batch 850/938\n",
      "D loss: 1.102525, acc: 95% // tar acc: 98% // adv loss: 0.543794, aux loss: 1.461277, tar loss: 0.847389\n",
      "=====================\n",
      "Epoch 25/200, Batch 900/938\n",
      "D loss: 1.040938, acc: 95% // tar acc: 92% // adv loss: 0.382797, aux loss: 1.476690, tar loss: 0.833000\n",
      "=====================\n",
      "Epoch 26/200, Batch 12/938\n",
      "D loss: 1.090149, acc: 96% // tar acc: 92% // adv loss: 0.425122, aux loss: 1.473338, tar loss: 0.770008\n",
      "=====================\n",
      "Epoch 26/200, Batch 62/938\n",
      "D loss: 1.124340, acc: 96% // tar acc: 95% // adv loss: 0.659396, aux loss: 1.462408, tar loss: 0.855445\n",
      "=====================\n",
      "Epoch 26/200, Batch 112/938\n",
      "D loss: 1.041218, acc: 97% // tar acc: 90% // adv loss: 0.685544, aux loss: 1.463969, tar loss: 0.767736\n",
      "=====================\n",
      "Epoch 26/200, Batch 162/938\n",
      "D loss: 1.080094, acc: 97% // tar acc: 85% // adv loss: 0.423082, aux loss: 1.481547, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 26/200, Batch 212/938\n",
      "D loss: 1.039174, acc: 96% // tar acc: 93% // adv loss: 0.509224, aux loss: 1.477558, tar loss: 0.825423\n",
      "=====================\n",
      "Epoch 26/200, Batch 262/938\n",
      "D loss: 1.093994, acc: 96% // tar acc: 87% // adv loss: 0.643849, aux loss: 1.476631, tar loss: 0.788675\n",
      "=====================\n",
      "Epoch 26/200, Batch 312/938\n",
      "D loss: 1.067143, acc: 95% // tar acc: 95% // adv loss: 0.454611, aux loss: 1.480601, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 26/200, Batch 362/938\n",
      "D loss: 1.045595, acc: 97% // tar acc: 93% // adv loss: 0.580940, aux loss: 1.469585, tar loss: 0.807453\n",
      "=====================\n",
      "Epoch 26/200, Batch 412/938\n",
      "D loss: 1.079016, acc: 96% // tar acc: 95% // adv loss: 0.410058, aux loss: 1.462068, tar loss: 0.838459\n",
      "=====================\n",
      "Epoch 26/200, Batch 462/938\n",
      "D loss: 1.092578, acc: 98% // tar acc: 95% // adv loss: 0.442676, aux loss: 1.485281, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 26/200, Batch 512/938\n",
      "D loss: 1.036619, acc: 99% // tar acc: 93% // adv loss: 0.442585, aux loss: 1.471065, tar loss: 0.811663\n",
      "=====================\n",
      "Epoch 26/200, Batch 562/938\n",
      "D loss: 1.044717, acc: 99% // tar acc: 90% // adv loss: 0.443850, aux loss: 1.476110, tar loss: 0.832412\n",
      "=====================\n",
      "Epoch 26/200, Batch 612/938\n",
      "D loss: 1.003061, acc: 97% // tar acc: 93% // adv loss: 0.468165, aux loss: 1.469007, tar loss: 0.837211\n",
      "=====================\n",
      "Epoch 26/200, Batch 662/938\n",
      "D loss: 1.098321, acc: 97% // tar acc: 92% // adv loss: 0.390704, aux loss: 1.475421, tar loss: 0.839839\n",
      "=====================\n",
      "Epoch 26/200, Batch 712/938\n",
      "D loss: 1.058424, acc: 96% // tar acc: 90% // adv loss: 0.566878, aux loss: 1.479511, tar loss: 0.847562\n",
      "=====================\n",
      "Epoch 26/200, Batch 762/938\n",
      "D loss: 1.071469, acc: 99% // tar acc: 96% // adv loss: 0.438451, aux loss: 1.474697, tar loss: 0.861229\n",
      "=====================\n",
      "Epoch 26/200, Batch 812/938\n",
      "D loss: 1.067665, acc: 96% // tar acc: 93% // adv loss: 0.455611, aux loss: 1.468175, tar loss: 0.787682\n",
      "=====================\n",
      "Epoch 26/200, Batch 862/938\n",
      "D loss: 1.062419, acc: 97% // tar acc: 98% // adv loss: 0.364433, aux loss: 1.461243, tar loss: 0.910005\n",
      "=====================\n",
      "Epoch 26/200, Batch 912/938\n",
      "D loss: 1.034537, acc: 100% // tar acc: 95% // adv loss: 0.409005, aux loss: 1.472007, tar loss: 0.799090\n",
      "=====================\n",
      "Epoch 27/200, Batch 24/938\n",
      "D loss: 1.028099, acc: 99% // tar acc: 92% // adv loss: 0.409868, aux loss: 1.471257, tar loss: 0.804871\n",
      "=====================\n",
      "Epoch 27/200, Batch 74/938\n",
      "D loss: 1.042868, acc: 99% // tar acc: 87% // adv loss: 0.567050, aux loss: 1.472289, tar loss: 0.811761\n",
      "=====================\n",
      "Epoch 27/200, Batch 124/938\n",
      "D loss: 1.089831, acc: 96% // tar acc: 93% // adv loss: 0.512349, aux loss: 1.492997, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 27/200, Batch 174/938\n",
      "D loss: 1.090210, acc: 96% // tar acc: 96% // adv loss: 0.464574, aux loss: 1.483035, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 27/200, Batch 224/938\n",
      "D loss: 1.026875, acc: 98% // tar acc: 95% // adv loss: 0.650141, aux loss: 1.495846, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 27/200, Batch 274/938\n",
      "D loss: 1.022605, acc: 97% // tar acc: 96% // adv loss: 0.462112, aux loss: 1.464291, tar loss: 0.815859\n",
      "=====================\n",
      "Epoch 27/200, Batch 324/938\n",
      "D loss: 1.061031, acc: 100% // tar acc: 98% // adv loss: 0.302608, aux loss: 1.476667, tar loss: 0.824208\n",
      "=====================\n",
      "Epoch 27/200, Batch 374/938\n",
      "D loss: 1.045240, acc: 95% // tar acc: 100% // adv loss: 0.467167, aux loss: 1.461960, tar loss: 0.863604\n",
      "=====================\n",
      "Epoch 27/200, Batch 424/938\n",
      "D loss: 1.066930, acc: 96% // tar acc: 98% // adv loss: 0.405477, aux loss: 1.461182, tar loss: 0.855349\n",
      "=====================\n",
      "Epoch 27/200, Batch 474/938\n",
      "D loss: 1.038039, acc: 97% // tar acc: 95% // adv loss: 0.495508, aux loss: 1.472362, tar loss: 0.883590\n",
      "=====================\n",
      "Epoch 27/200, Batch 524/938\n",
      "D loss: 1.042648, acc: 95% // tar acc: 95% // adv loss: 0.630256, aux loss: 1.462628, tar loss: 0.886563\n",
      "=====================\n",
      "Epoch 27/200, Batch 574/938\n",
      "D loss: 1.030265, acc: 97% // tar acc: 92% // adv loss: 0.621142, aux loss: 1.496971, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 27/200, Batch 624/938\n",
      "D loss: 0.975553, acc: 99% // tar acc: 93% // adv loss: 0.454771, aux loss: 1.464104, tar loss: 0.844650\n",
      "=====================\n",
      "Epoch 27/200, Batch 674/938\n",
      "D loss: 1.038442, acc: 98% // tar acc: 92% // adv loss: 0.442397, aux loss: 1.464813, tar loss: 0.838094\n",
      "=====================\n",
      "Epoch 27/200, Batch 724/938\n",
      "D loss: 1.073563, acc: 96% // tar acc: 93% // adv loss: 0.413991, aux loss: 1.476943, tar loss: 0.832525\n",
      "=====================\n",
      "Epoch 27/200, Batch 774/938\n",
      "D loss: 1.048042, acc: 97% // tar acc: 93% // adv loss: 0.542373, aux loss: 1.462402, tar loss: 0.788569\n",
      "=====================\n",
      "Epoch 27/200, Batch 824/938\n",
      "D loss: 1.028523, acc: 98% // tar acc: 95% // adv loss: 0.651449, aux loss: 1.477615, tar loss: 0.889277\n",
      "=====================\n",
      "Epoch 27/200, Batch 874/938\n",
      "D loss: 1.103034, acc: 100% // tar acc: 98% // adv loss: 0.342132, aux loss: 1.471435, tar loss: 0.863279\n",
      "=====================\n",
      "Epoch 27/200, Batch 924/938\n",
      "D loss: 1.005416, acc: 98% // tar acc: 93% // adv loss: 0.458600, aux loss: 1.467015, tar loss: 0.866742\n",
      "=====================\n",
      "Epoch 28/200, Batch 36/938\n",
      "D loss: 1.061933, acc: 96% // tar acc: 92% // adv loss: 0.413437, aux loss: 1.461756, tar loss: 0.812682\n",
      "=====================\n",
      "Epoch 28/200, Batch 86/938\n",
      "D loss: 1.099203, acc: 97% // tar acc: 98% // adv loss: 0.350398, aux loss: 1.465599, tar loss: 0.860604\n",
      "=====================\n",
      "Epoch 28/200, Batch 136/938\n",
      "D loss: 1.111727, acc: 94% // tar acc: 95% // adv loss: 0.459126, aux loss: 1.474530, tar loss: 0.861636\n",
      "=====================\n",
      "Epoch 28/200, Batch 186/938\n",
      "D loss: 1.084587, acc: 99% // tar acc: 95% // adv loss: 0.513194, aux loss: 1.476999, tar loss: 0.869233\n",
      "=====================\n",
      "Epoch 28/200, Batch 236/938\n",
      "D loss: 1.054960, acc: 97% // tar acc: 93% // adv loss: 0.414174, aux loss: 1.469605, tar loss: 0.830002\n",
      "=====================\n",
      "Epoch 28/200, Batch 286/938\n",
      "D loss: 1.082098, acc: 99% // tar acc: 89% // adv loss: 0.498637, aux loss: 1.463002, tar loss: 0.804784\n",
      "=====================\n",
      "Epoch 28/200, Batch 336/938\n",
      "D loss: 1.038536, acc: 96% // tar acc: 95% // adv loss: 0.447554, aux loss: 1.473044, tar loss: 0.852547\n",
      "=====================\n",
      "Epoch 28/200, Batch 386/938\n",
      "D loss: 1.025125, acc: 97% // tar acc: 90% // adv loss: 0.565463, aux loss: 1.481722, tar loss: 1.332000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 28/200, Batch 436/938\n",
      "D loss: 1.020826, acc: 96% // tar acc: 93% // adv loss: 0.480213, aux loss: 1.498437, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 28/200, Batch 486/938\n",
      "D loss: 1.026687, acc: 98% // tar acc: 93% // adv loss: 0.591579, aux loss: 1.473504, tar loss: 0.893330\n",
      "=====================\n",
      "Epoch 28/200, Batch 536/938\n",
      "D loss: 1.084739, acc: 98% // tar acc: 96% // adv loss: 0.427309, aux loss: 1.464118, tar loss: 0.830276\n",
      "=====================\n",
      "Epoch 28/200, Batch 586/938\n",
      "D loss: 1.117259, acc: 95% // tar acc: 96% // adv loss: 0.474882, aux loss: 1.468006, tar loss: 0.876257\n",
      "=====================\n",
      "Epoch 28/200, Batch 636/938\n",
      "D loss: 1.043856, acc: 98% // tar acc: 92% // adv loss: 0.428021, aux loss: 1.464537, tar loss: 0.788771\n",
      "=====================\n",
      "Epoch 28/200, Batch 686/938\n",
      "D loss: 1.042297, acc: 99% // tar acc: 96% // adv loss: 0.508503, aux loss: 1.470408, tar loss: 0.833212\n",
      "=====================\n",
      "Epoch 28/200, Batch 736/938\n",
      "D loss: 1.042424, acc: 97% // tar acc: 90% // adv loss: 0.427492, aux loss: 1.469766, tar loss: 0.830203\n",
      "=====================\n",
      "Epoch 28/200, Batch 786/938\n",
      "D loss: 1.017537, acc: 97% // tar acc: 96% // adv loss: 0.526467, aux loss: 1.461740, tar loss: 0.829727\n",
      "=====================\n",
      "Epoch 28/200, Batch 836/938\n",
      "D loss: 1.052470, acc: 96% // tar acc: 93% // adv loss: 0.577590, aux loss: 1.478194, tar loss: 0.818491\n",
      "=====================\n",
      "Epoch 28/200, Batch 886/938\n",
      "D loss: 1.057310, acc: 100% // tar acc: 90% // adv loss: 0.500198, aux loss: 1.501367, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 28/200, Batch 936/938\n",
      "D loss: 1.065980, acc: 97% // tar acc: 95% // adv loss: 0.359431, aux loss: 1.463689, tar loss: 0.888024\n",
      "=====================\n",
      "Epoch 29/200, Batch 48/938\n",
      "D loss: 1.089431, acc: 98% // tar acc: 96% // adv loss: 0.345619, aux loss: 1.462918, tar loss: 0.873920\n",
      "=====================\n",
      "Epoch 29/200, Batch 98/938\n",
      "D loss: 1.127621, acc: 96% // tar acc: 92% // adv loss: 0.546079, aux loss: 1.476687, tar loss: 0.796098\n",
      "=====================\n",
      "Epoch 29/200, Batch 148/938\n",
      "D loss: 1.047972, acc: 100% // tar acc: 93% // adv loss: 0.508573, aux loss: 1.461296, tar loss: 0.837113\n",
      "=====================\n",
      "Epoch 29/200, Batch 198/938\n",
      "D loss: 1.057607, acc: 99% // tar acc: 100% // adv loss: 0.539742, aux loss: 1.462139, tar loss: 0.832225\n",
      "=====================\n",
      "Epoch 29/200, Batch 248/938\n",
      "D loss: 0.998600, acc: 96% // tar acc: 92% // adv loss: 0.477087, aux loss: 1.464162, tar loss: 0.862248\n",
      "=====================\n",
      "Epoch 29/200, Batch 298/938\n",
      "D loss: 1.075196, acc: 98% // tar acc: 96% // adv loss: 0.466750, aux loss: 1.466958, tar loss: 0.881331\n",
      "=====================\n",
      "Epoch 29/200, Batch 348/938\n",
      "D loss: 0.969458, acc: 100% // tar acc: 92% // adv loss: 0.490148, aux loss: 1.463979, tar loss: 0.817262\n",
      "=====================\n",
      "Epoch 29/200, Batch 398/938\n",
      "D loss: 1.021548, acc: 98% // tar acc: 96% // adv loss: 0.493297, aux loss: 1.471961, tar loss: 0.790868\n",
      "=====================\n",
      "Epoch 29/200, Batch 448/938\n",
      "D loss: 1.084523, acc: 98% // tar acc: 90% // adv loss: 0.517715, aux loss: 1.462949, tar loss: 0.833079\n",
      "=====================\n",
      "Epoch 29/200, Batch 498/938\n",
      "D loss: 1.097767, acc: 96% // tar acc: 93% // adv loss: 0.452087, aux loss: 1.462097, tar loss: 0.821371\n",
      "=====================\n",
      "Epoch 29/200, Batch 548/938\n",
      "D loss: 1.028550, acc: 97% // tar acc: 98% // adv loss: 0.502241, aux loss: 1.462552, tar loss: 0.865750\n",
      "=====================\n",
      "Epoch 29/200, Batch 598/938\n",
      "D loss: 1.117503, acc: 96% // tar acc: 95% // adv loss: 0.391474, aux loss: 1.477676, tar loss: 0.870833\n",
      "=====================\n",
      "Epoch 29/200, Batch 648/938\n",
      "D loss: 1.032388, acc: 100% // tar acc: 81% // adv loss: 0.762552, aux loss: 1.479807, tar loss: 0.785961\n",
      "=====================\n",
      "Epoch 29/200, Batch 698/938\n",
      "D loss: 1.040861, acc: 98% // tar acc: 95% // adv loss: 0.536296, aux loss: 1.468362, tar loss: 0.868394\n",
      "=====================\n",
      "Epoch 29/200, Batch 748/938\n",
      "D loss: 0.986871, acc: 98% // tar acc: 90% // adv loss: 0.425187, aux loss: 1.478787, tar loss: 0.832275\n",
      "=====================\n",
      "Epoch 29/200, Batch 798/938\n",
      "D loss: 1.054797, acc: 98% // tar acc: 92% // adv loss: 0.523056, aux loss: 1.477519, tar loss: 0.820901\n",
      "=====================\n",
      "Epoch 29/200, Batch 848/938\n",
      "D loss: 1.042773, acc: 98% // tar acc: 96% // adv loss: 0.536746, aux loss: 1.473203, tar loss: 0.840065\n",
      "=====================\n",
      "Epoch 29/200, Batch 898/938\n",
      "D loss: 1.033622, acc: 99% // tar acc: 100% // adv loss: 0.444488, aux loss: 1.462375, tar loss: 0.867200\n",
      "=====================\n",
      "Epoch 30/200, Batch 10/938\n",
      "D loss: 1.009882, acc: 98% // tar acc: 93% // adv loss: 0.419528, aux loss: 1.477922, tar loss: 0.802772\n",
      "=====================\n",
      "Epoch 30/200, Batch 60/938\n",
      "D loss: 1.024959, acc: 98% // tar acc: 89% // adv loss: 0.527013, aux loss: 1.469983, tar loss: 0.782334\n",
      "=====================\n",
      "Epoch 30/200, Batch 110/938\n",
      "D loss: 1.115842, acc: 96% // tar acc: 100% // adv loss: 0.391725, aux loss: 1.482589, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 30/200, Batch 160/938\n",
      "D loss: 1.021563, acc: 98% // tar acc: 96% // adv loss: 0.387241, aux loss: 1.477411, tar loss: 0.841594\n",
      "=====================\n",
      "Epoch 30/200, Batch 210/938\n",
      "D loss: 1.077180, acc: 99% // tar acc: 95% // adv loss: 0.600044, aux loss: 1.477169, tar loss: 0.825717\n",
      "=====================\n",
      "Epoch 30/200, Batch 260/938\n",
      "D loss: 1.018576, acc: 96% // tar acc: 92% // adv loss: 0.506298, aux loss: 1.500304, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 30/200, Batch 310/938\n",
      "D loss: 1.027275, acc: 97% // tar acc: 92% // adv loss: 0.391339, aux loss: 1.463385, tar loss: 0.850306\n",
      "=====================\n",
      "Epoch 30/200, Batch 360/938\n",
      "D loss: 1.081226, acc: 99% // tar acc: 92% // adv loss: 0.344398, aux loss: 1.467052, tar loss: 0.767491\n",
      "=====================\n",
      "Epoch 30/200, Batch 410/938\n",
      "D loss: 1.053814, acc: 97% // tar acc: 95% // adv loss: 0.458087, aux loss: 1.462345, tar loss: 0.793467\n",
      "=====================\n",
      "Epoch 30/200, Batch 460/938\n",
      "D loss: 1.026425, acc: 96% // tar acc: 100% // adv loss: 0.464560, aux loss: 1.461748, tar loss: 0.908889\n",
      "=====================\n",
      "Epoch 30/200, Batch 510/938\n",
      "D loss: 1.060427, acc: 97% // tar acc: 98% // adv loss: 0.558170, aux loss: 1.482489, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 30/200, Batch 560/938\n",
      "D loss: 1.043066, acc: 99% // tar acc: 95% // adv loss: 0.639180, aux loss: 1.474394, tar loss: 0.858224\n",
      "=====================\n",
      "Epoch 30/200, Batch 610/938\n",
      "D loss: 1.019700, acc: 97% // tar acc: 93% // adv loss: 0.610952, aux loss: 1.462826, tar loss: 0.838387\n",
      "=====================\n",
      "Epoch 30/200, Batch 660/938\n",
      "D loss: 1.004568, acc: 96% // tar acc: 89% // adv loss: 0.517402, aux loss: 1.497890, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 30/200, Batch 710/938\n",
      "D loss: 0.991334, acc: 99% // tar acc: 96% // adv loss: 0.456731, aux loss: 1.472982, tar loss: 0.873278\n",
      "=====================\n",
      "Epoch 30/200, Batch 760/938\n",
      "D loss: 1.013132, acc: 99% // tar acc: 100% // adv loss: 0.384225, aux loss: 1.474091, tar loss: 0.838679\n",
      "=====================\n",
      "Epoch 30/200, Batch 810/938\n",
      "D loss: 1.011690, acc: 98% // tar acc: 90% // adv loss: 0.574084, aux loss: 1.485423, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 30/200, Batch 860/938\n",
      "D loss: 1.089983, acc: 97% // tar acc: 96% // adv loss: 0.394743, aux loss: 1.465778, tar loss: 0.913885\n",
      "=====================\n",
      "Epoch 30/200, Batch 910/938\n",
      "D loss: 1.012249, acc: 98% // tar acc: 93% // adv loss: 0.731048, aux loss: 1.469199, tar loss: 0.838490\n",
      "=====================\n",
      "Epoch 31/200, Batch 22/938\n",
      "D loss: 1.058140, acc: 96% // tar acc: 93% // adv loss: 0.545848, aux loss: 1.494389, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 31/200, Batch 72/938\n",
      "D loss: 1.055317, acc: 98% // tar acc: 89% // adv loss: 0.538836, aux loss: 1.491940, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 31/200, Batch 122/938\n",
      "D loss: 1.131397, acc: 97% // tar acc: 93% // adv loss: 0.396729, aux loss: 1.472518, tar loss: 0.835350\n",
      "=====================\n",
      "Epoch 31/200, Batch 172/938\n",
      "D loss: 1.030862, acc: 96% // tar acc: 98% // adv loss: 0.385812, aux loss: 1.482797, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 31/200, Batch 222/938\n",
      "D loss: 1.027096, acc: 97% // tar acc: 98% // adv loss: 0.639633, aux loss: 1.473759, tar loss: 0.857504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 31/200, Batch 272/938\n",
      "D loss: 1.069411, acc: 97% // tar acc: 92% // adv loss: 0.421054, aux loss: 1.501038, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 31/200, Batch 322/938\n",
      "D loss: 1.032471, acc: 98% // tar acc: 96% // adv loss: 0.435688, aux loss: 1.463143, tar loss: 0.854117\n",
      "=====================\n",
      "Epoch 31/200, Batch 372/938\n",
      "D loss: 1.005557, acc: 96% // tar acc: 93% // adv loss: 0.407641, aux loss: 1.473746, tar loss: 0.786898\n",
      "=====================\n",
      "Epoch 31/200, Batch 422/938\n",
      "D loss: 1.007594, acc: 99% // tar acc: 93% // adv loss: 0.632996, aux loss: 1.481864, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 31/200, Batch 472/938\n",
      "D loss: 1.035066, acc: 98% // tar acc: 93% // adv loss: 0.484348, aux loss: 1.479060, tar loss: 0.802650\n",
      "=====================\n",
      "Epoch 31/200, Batch 522/938\n",
      "D loss: 1.009651, acc: 98% // tar acc: 98% // adv loss: 0.526642, aux loss: 1.470878, tar loss: 0.886623\n",
      "=====================\n",
      "Epoch 31/200, Batch 572/938\n",
      "D loss: 1.039939, acc: 96% // tar acc: 92% // adv loss: 0.406614, aux loss: 1.475048, tar loss: 0.792135\n",
      "=====================\n",
      "Epoch 31/200, Batch 622/938\n",
      "D loss: 1.083682, acc: 97% // tar acc: 89% // adv loss: 0.539208, aux loss: 1.463217, tar loss: 0.797452\n",
      "=====================\n",
      "Epoch 31/200, Batch 672/938\n",
      "D loss: 1.038895, acc: 99% // tar acc: 95% // adv loss: 0.420065, aux loss: 1.462805, tar loss: 0.786849\n",
      "=====================\n",
      "Epoch 31/200, Batch 722/938\n",
      "D loss: 1.110407, acc: 98% // tar acc: 95% // adv loss: 0.507140, aux loss: 1.470513, tar loss: 0.855722\n",
      "=====================\n",
      "Epoch 31/200, Batch 772/938\n",
      "D loss: 1.102530, acc: 96% // tar acc: 98% // adv loss: 0.422810, aux loss: 1.476394, tar loss: 0.878952\n",
      "=====================\n",
      "Epoch 31/200, Batch 822/938\n",
      "D loss: 1.059762, acc: 98% // tar acc: 95% // adv loss: 0.473005, aux loss: 1.477314, tar loss: 0.818404\n",
      "=====================\n",
      "Epoch 31/200, Batch 872/938\n",
      "D loss: 1.094285, acc: 96% // tar acc: 96% // adv loss: 0.355329, aux loss: 1.466860, tar loss: 0.831952\n",
      "=====================\n",
      "Epoch 31/200, Batch 922/938\n",
      "D loss: 1.034794, acc: 98% // tar acc: 93% // adv loss: 0.496698, aux loss: 1.464457, tar loss: 0.851123\n",
      "=====================\n",
      "Epoch 32/200, Batch 34/938\n",
      "D loss: 1.107661, acc: 96% // tar acc: 93% // adv loss: 0.497234, aux loss: 1.471280, tar loss: 0.849214\n",
      "=====================\n",
      "Epoch 32/200, Batch 84/938\n",
      "D loss: 1.016592, acc: 99% // tar acc: 96% // adv loss: 0.333948, aux loss: 1.461435, tar loss: 0.887589\n",
      "=====================\n",
      "Epoch 32/200, Batch 134/938\n",
      "D loss: 1.124384, acc: 96% // tar acc: 90% // adv loss: 0.484501, aux loss: 1.502506, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 32/200, Batch 184/938\n",
      "D loss: 1.003942, acc: 96% // tar acc: 95% // adv loss: 0.414071, aux loss: 1.478312, tar loss: 0.820034\n",
      "=====================\n",
      "Epoch 32/200, Batch 234/938\n",
      "D loss: 1.066610, acc: 98% // tar acc: 93% // adv loss: 0.370582, aux loss: 1.467671, tar loss: 0.822994\n",
      "=====================\n",
      "Epoch 32/200, Batch 284/938\n",
      "D loss: 1.014082, acc: 96% // tar acc: 95% // adv loss: 0.436750, aux loss: 1.464330, tar loss: 0.801519\n",
      "=====================\n",
      "Epoch 32/200, Batch 334/938\n",
      "D loss: 1.066029, acc: 96% // tar acc: 98% // adv loss: 0.391559, aux loss: 1.470431, tar loss: 0.857772\n",
      "=====================\n",
      "Epoch 32/200, Batch 384/938\n",
      "D loss: 1.090682, acc: 96% // tar acc: 93% // adv loss: 0.612271, aux loss: 1.489426, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 32/200, Batch 434/938\n",
      "D loss: 1.047300, acc: 99% // tar acc: 98% // adv loss: 0.393054, aux loss: 1.477331, tar loss: 0.807582\n",
      "=====================\n",
      "Epoch 32/200, Batch 484/938\n",
      "D loss: 1.061759, acc: 96% // tar acc: 98% // adv loss: 0.445266, aux loss: 1.461354, tar loss: 0.863650\n",
      "=====================\n",
      "Epoch 32/200, Batch 534/938\n",
      "D loss: 1.044728, acc: 98% // tar acc: 93% // adv loss: 0.579569, aux loss: 1.462512, tar loss: 0.842059\n",
      "=====================\n",
      "Epoch 32/200, Batch 584/938\n",
      "D loss: 1.037910, acc: 96% // tar acc: 92% // adv loss: 0.552629, aux loss: 1.481927, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 32/200, Batch 634/938\n",
      "D loss: 1.015013, acc: 96% // tar acc: 98% // adv loss: 0.605719, aux loss: 1.462152, tar loss: 0.866199\n",
      "=====================\n",
      "Epoch 32/200, Batch 684/938\n",
      "D loss: 1.049236, acc: 97% // tar acc: 89% // adv loss: 0.418063, aux loss: 1.475740, tar loss: 0.861093\n",
      "=====================\n",
      "Epoch 32/200, Batch 734/938\n",
      "D loss: 1.083178, acc: 99% // tar acc: 96% // adv loss: 0.410984, aux loss: 1.494963, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 32/200, Batch 784/938\n",
      "D loss: 1.077513, acc: 96% // tar acc: 92% // adv loss: 0.527230, aux loss: 1.470984, tar loss: 0.851574\n",
      "=====================\n",
      "Epoch 32/200, Batch 834/938\n",
      "D loss: 1.028407, acc: 98% // tar acc: 90% // adv loss: 0.541103, aux loss: 1.464292, tar loss: 0.835539\n",
      "=====================\n",
      "Epoch 32/200, Batch 884/938\n",
      "D loss: 1.036030, acc: 98% // tar acc: 96% // adv loss: 0.489962, aux loss: 1.469970, tar loss: 0.783839\n",
      "=====================\n",
      "Epoch 32/200, Batch 934/938\n",
      "D loss: 1.046007, acc: 97% // tar acc: 95% // adv loss: 0.568638, aux loss: 1.466932, tar loss: 0.840963\n",
      "=====================\n",
      "Epoch 33/200, Batch 46/938\n",
      "D loss: 1.094528, acc: 96% // tar acc: 98% // adv loss: 0.515586, aux loss: 1.463336, tar loss: 0.872011\n",
      "=====================\n",
      "Epoch 33/200, Batch 96/938\n",
      "D loss: 1.064407, acc: 99% // tar acc: 89% // adv loss: 0.331388, aux loss: 1.481583, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 33/200, Batch 146/938\n",
      "D loss: 1.055890, acc: 96% // tar acc: 98% // adv loss: 0.267059, aux loss: 1.465269, tar loss: 0.889121\n",
      "=====================\n",
      "Epoch 33/200, Batch 196/938\n",
      "D loss: 1.017702, acc: 97% // tar acc: 92% // adv loss: 0.508105, aux loss: 1.483838, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 33/200, Batch 246/938\n",
      "D loss: 1.075613, acc: 97% // tar acc: 98% // adv loss: 0.584113, aux loss: 1.466761, tar loss: 0.848161\n",
      "=====================\n",
      "Epoch 33/200, Batch 296/938\n",
      "D loss: 1.029897, acc: 99% // tar acc: 93% // adv loss: 0.551970, aux loss: 1.463343, tar loss: 0.821021\n",
      "=====================\n",
      "Epoch 33/200, Batch 346/938\n",
      "D loss: 1.083963, acc: 97% // tar acc: 93% // adv loss: 0.374693, aux loss: 1.461369, tar loss: 0.879938\n",
      "=====================\n",
      "Epoch 33/200, Batch 396/938\n",
      "D loss: 1.101274, acc: 98% // tar acc: 96% // adv loss: 0.432314, aux loss: 1.463358, tar loss: 0.854645\n",
      "=====================\n",
      "Epoch 33/200, Batch 446/938\n",
      "D loss: 1.012534, acc: 98% // tar acc: 98% // adv loss: 0.450432, aux loss: 1.464024, tar loss: 0.772474\n",
      "=====================\n",
      "Epoch 33/200, Batch 496/938\n",
      "D loss: 1.030377, acc: 97% // tar acc: 95% // adv loss: 0.524743, aux loss: 1.461308, tar loss: 0.809810\n",
      "=====================\n",
      "Epoch 33/200, Batch 546/938\n",
      "D loss: 1.046818, acc: 96% // tar acc: 89% // adv loss: 0.451851, aux loss: 1.463656, tar loss: 0.783894\n",
      "=====================\n",
      "Epoch 33/200, Batch 596/938\n",
      "D loss: 1.025370, acc: 98% // tar acc: 87% // adv loss: 0.507848, aux loss: 1.461618, tar loss: 0.805507\n",
      "=====================\n",
      "Epoch 33/200, Batch 646/938\n",
      "D loss: 1.041294, acc: 96% // tar acc: 89% // adv loss: 0.410264, aux loss: 1.471278, tar loss: 0.817686\n",
      "=====================\n",
      "Epoch 33/200, Batch 696/938\n",
      "D loss: 1.096932, acc: 97% // tar acc: 98% // adv loss: 0.385106, aux loss: 1.477144, tar loss: 0.863212\n",
      "=====================\n",
      "Epoch 33/200, Batch 746/938\n",
      "D loss: 1.042308, acc: 96% // tar acc: 95% // adv loss: 0.370186, aux loss: 1.465979, tar loss: 0.878097\n",
      "=====================\n",
      "Epoch 33/200, Batch 796/938\n",
      "D loss: 1.021515, acc: 99% // tar acc: 93% // adv loss: 0.625143, aux loss: 1.462130, tar loss: 0.817017\n",
      "=====================\n",
      "Epoch 33/200, Batch 846/938\n",
      "D loss: 1.044399, acc: 96% // tar acc: 90% // adv loss: 0.354158, aux loss: 1.461515, tar loss: 0.812006\n",
      "=====================\n",
      "Epoch 33/200, Batch 896/938\n",
      "D loss: 1.030627, acc: 97% // tar acc: 96% // adv loss: 0.520401, aux loss: 1.467414, tar loss: 0.875140\n",
      "=====================\n",
      "Epoch 34/200, Batch 8/938\n",
      "D loss: 1.049721, acc: 96% // tar acc: 93% // adv loss: 0.390126, aux loss: 1.477427, tar loss: 0.801184\n",
      "=====================\n",
      "Epoch 34/200, Batch 58/938\n",
      "D loss: 1.066434, acc: 99% // tar acc: 96% // adv loss: 0.380950, aux loss: 1.475110, tar loss: 0.873237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 34/200, Batch 108/938\n",
      "D loss: 1.016169, acc: 97% // tar acc: 98% // adv loss: 0.531126, aux loss: 1.462192, tar loss: 0.863269\n",
      "=====================\n",
      "Epoch 34/200, Batch 158/938\n",
      "D loss: 1.040293, acc: 96% // tar acc: 98% // adv loss: 0.331872, aux loss: 1.462053, tar loss: 0.865421\n",
      "=====================\n",
      "Epoch 34/200, Batch 208/938\n",
      "D loss: 1.045501, acc: 96% // tar acc: 92% // adv loss: 0.564958, aux loss: 1.475179, tar loss: 0.783188\n",
      "=====================\n",
      "Epoch 34/200, Batch 258/938\n",
      "D loss: 1.023189, acc: 99% // tar acc: 92% // adv loss: 0.415623, aux loss: 1.469703, tar loss: 0.823262\n",
      "=====================\n",
      "Epoch 34/200, Batch 308/938\n",
      "D loss: 1.076392, acc: 99% // tar acc: 98% // adv loss: 0.557085, aux loss: 1.471364, tar loss: 0.894494\n",
      "=====================\n",
      "Epoch 34/200, Batch 358/938\n",
      "D loss: 1.062874, acc: 95% // tar acc: 85% // adv loss: 0.538330, aux loss: 1.473617, tar loss: 0.813440\n",
      "=====================\n",
      "Epoch 34/200, Batch 408/938\n",
      "D loss: 1.041236, acc: 100% // tar acc: 95% // adv loss: 0.491304, aux loss: 1.462827, tar loss: 0.823138\n",
      "=====================\n",
      "Epoch 34/200, Batch 458/938\n",
      "D loss: 1.044134, acc: 96% // tar acc: 96% // adv loss: 0.625432, aux loss: 1.465496, tar loss: 0.829014\n",
      "=====================\n",
      "Epoch 34/200, Batch 508/938\n",
      "D loss: 1.095798, acc: 97% // tar acc: 93% // adv loss: 0.463741, aux loss: 1.476854, tar loss: 0.912692\n",
      "=====================\n",
      "Epoch 34/200, Batch 558/938\n",
      "D loss: 1.070672, acc: 96% // tar acc: 96% // adv loss: 0.728673, aux loss: 1.474616, tar loss: 0.849447\n",
      "=====================\n",
      "Epoch 34/200, Batch 608/938\n",
      "D loss: 1.112213, acc: 98% // tar acc: 96% // adv loss: 0.426661, aux loss: 1.478988, tar loss: 0.821477\n",
      "=====================\n",
      "Epoch 34/200, Batch 658/938\n",
      "D loss: 1.054221, acc: 98% // tar acc: 93% // adv loss: 0.482645, aux loss: 1.477298, tar loss: 0.810786\n",
      "=====================\n",
      "Epoch 34/200, Batch 708/938\n",
      "D loss: 1.079000, acc: 95% // tar acc: 95% // adv loss: 0.472193, aux loss: 1.480798, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 34/200, Batch 758/938\n",
      "D loss: 1.036809, acc: 97% // tar acc: 85% // adv loss: 0.536559, aux loss: 1.462187, tar loss: 0.824129\n",
      "=====================\n",
      "Epoch 34/200, Batch 808/938\n",
      "D loss: 1.058454, acc: 99% // tar acc: 93% // adv loss: 0.587248, aux loss: 1.473009, tar loss: 0.774473\n",
      "=====================\n",
      "Epoch 34/200, Batch 858/938\n",
      "D loss: 1.061895, acc: 94% // tar acc: 92% // adv loss: 0.626259, aux loss: 1.480956, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 34/200, Batch 908/938\n",
      "D loss: 1.028717, acc: 99% // tar acc: 90% // adv loss: 0.471632, aux loss: 1.479795, tar loss: 0.776130\n",
      "=====================\n",
      "Epoch 35/200, Batch 20/938\n",
      "D loss: 1.012415, acc: 99% // tar acc: 93% // adv loss: 0.453949, aux loss: 1.463182, tar loss: 0.843409\n",
      "=====================\n",
      "Epoch 35/200, Batch 70/938\n",
      "D loss: 1.022543, acc: 98% // tar acc: 98% // adv loss: 0.523551, aux loss: 1.482219, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 35/200, Batch 120/938\n",
      "D loss: 1.104687, acc: 96% // tar acc: 95% // adv loss: 0.512440, aux loss: 1.485384, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 35/200, Batch 170/938\n",
      "D loss: 1.050153, acc: 99% // tar acc: 95% // adv loss: 0.444954, aux loss: 1.464664, tar loss: 0.869030\n",
      "=====================\n",
      "Epoch 35/200, Batch 220/938\n",
      "D loss: 1.058480, acc: 98% // tar acc: 98% // adv loss: 0.528219, aux loss: 1.477123, tar loss: 0.910301\n",
      "=====================\n",
      "Epoch 35/200, Batch 270/938\n",
      "D loss: 1.014493, acc: 98% // tar acc: 95% // adv loss: 0.448905, aux loss: 1.480303, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 35/200, Batch 320/938\n",
      "D loss: 0.998348, acc: 96% // tar acc: 93% // adv loss: 0.425388, aux loss: 1.482210, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 35/200, Batch 370/938\n",
      "D loss: 1.022998, acc: 97% // tar acc: 89% // adv loss: 0.602493, aux loss: 1.499151, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 35/200, Batch 420/938\n",
      "D loss: 1.061624, acc: 98% // tar acc: 96% // adv loss: 0.542591, aux loss: 1.472037, tar loss: 0.844480\n",
      "=====================\n",
      "Epoch 35/200, Batch 470/938\n",
      "D loss: 1.086061, acc: 96% // tar acc: 95% // adv loss: 0.378334, aux loss: 1.461465, tar loss: 0.857594\n",
      "=====================\n",
      "Epoch 35/200, Batch 520/938\n",
      "D loss: 1.062383, acc: 96% // tar acc: 93% // adv loss: 0.436433, aux loss: 1.483574, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 35/200, Batch 570/938\n",
      "D loss: 1.070669, acc: 97% // tar acc: 96% // adv loss: 0.368745, aux loss: 1.464421, tar loss: 0.881316\n",
      "=====================\n",
      "Epoch 35/200, Batch 620/938\n",
      "D loss: 1.081790, acc: 97% // tar acc: 89% // adv loss: 0.518966, aux loss: 1.461675, tar loss: 0.784523\n",
      "=====================\n",
      "Epoch 35/200, Batch 670/938\n",
      "D loss: 1.114740, acc: 96% // tar acc: 93% // adv loss: 0.364803, aux loss: 1.484075, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 35/200, Batch 720/938\n",
      "D loss: 1.133364, acc: 95% // tar acc: 96% // adv loss: 0.414132, aux loss: 1.475458, tar loss: 0.869490\n",
      "=====================\n",
      "Epoch 35/200, Batch 770/938\n",
      "D loss: 1.051383, acc: 98% // tar acc: 95% // adv loss: 0.445072, aux loss: 1.486342, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 35/200, Batch 820/938\n",
      "D loss: 0.987079, acc: 99% // tar acc: 92% // adv loss: 0.601744, aux loss: 1.472423, tar loss: 0.768307\n",
      "=====================\n",
      "Epoch 35/200, Batch 870/938\n",
      "D loss: 1.045798, acc: 97% // tar acc: 100% // adv loss: 0.425683, aux loss: 1.462671, tar loss: 0.824080\n",
      "=====================\n",
      "Epoch 35/200, Batch 920/938\n",
      "D loss: 1.021580, acc: 98% // tar acc: 89% // adv loss: 0.469550, aux loss: 1.477718, tar loss: 0.816064\n",
      "=====================\n",
      "Epoch 36/200, Batch 32/938\n",
      "D loss: 0.990740, acc: 96% // tar acc: 95% // adv loss: 0.363825, aux loss: 1.476849, tar loss: 0.810163\n",
      "=====================\n",
      "Epoch 36/200, Batch 82/938\n",
      "D loss: 1.051516, acc: 97% // tar acc: 90% // adv loss: 0.446067, aux loss: 1.476004, tar loss: 0.844612\n",
      "=====================\n",
      "Epoch 36/200, Batch 132/938\n",
      "D loss: 1.070305, acc: 97% // tar acc: 95% // adv loss: 0.539224, aux loss: 1.473053, tar loss: 0.858234\n",
      "=====================\n",
      "Epoch 36/200, Batch 182/938\n",
      "D loss: 1.075474, acc: 96% // tar acc: 96% // adv loss: 0.366758, aux loss: 1.464888, tar loss: 0.803185\n",
      "=====================\n",
      "Epoch 36/200, Batch 232/938\n",
      "D loss: 1.052858, acc: 99% // tar acc: 96% // adv loss: 0.509659, aux loss: 1.465812, tar loss: 0.832470\n",
      "=====================\n",
      "Epoch 36/200, Batch 282/938\n",
      "D loss: 1.024447, acc: 100% // tar acc: 96% // adv loss: 0.489148, aux loss: 1.462751, tar loss: 0.882294\n",
      "=====================\n",
      "Epoch 36/200, Batch 332/938\n",
      "D loss: 1.039961, acc: 100% // tar acc: 100% // adv loss: 0.602031, aux loss: 1.461434, tar loss: 0.864847\n",
      "=====================\n",
      "Epoch 36/200, Batch 382/938\n",
      "D loss: 1.066369, acc: 99% // tar acc: 93% // adv loss: 0.699098, aux loss: 1.476898, tar loss: 0.834984\n",
      "=====================\n",
      "Epoch 36/200, Batch 432/938\n",
      "D loss: 1.046884, acc: 97% // tar acc: 96% // adv loss: 0.528864, aux loss: 1.462236, tar loss: 0.827756\n",
      "=====================\n",
      "Epoch 36/200, Batch 482/938\n",
      "D loss: 1.068215, acc: 99% // tar acc: 92% // adv loss: 0.446194, aux loss: 1.461712, tar loss: 0.823727\n",
      "=====================\n",
      "Epoch 36/200, Batch 532/938\n",
      "D loss: 1.040185, acc: 99% // tar acc: 95% // adv loss: 0.552192, aux loss: 1.462108, tar loss: 0.815017\n",
      "=====================\n",
      "Epoch 36/200, Batch 582/938\n",
      "D loss: 1.121393, acc: 98% // tar acc: 95% // adv loss: 0.461305, aux loss: 1.476757, tar loss: 0.825209\n",
      "=====================\n",
      "Epoch 36/200, Batch 632/938\n",
      "D loss: 0.968846, acc: 96% // tar acc: 89% // adv loss: 0.409297, aux loss: 1.478335, tar loss: 0.764075\n",
      "=====================\n",
      "Epoch 36/200, Batch 682/938\n",
      "D loss: 1.049712, acc: 99% // tar acc: 95% // adv loss: 0.349729, aux loss: 1.479113, tar loss: 0.854357\n",
      "=====================\n",
      "Epoch 36/200, Batch 732/938\n",
      "D loss: 1.108812, acc: 97% // tar acc: 96% // adv loss: 0.429900, aux loss: 1.469402, tar loss: 0.869648\n",
      "=====================\n",
      "Epoch 36/200, Batch 782/938\n",
      "D loss: 1.104828, acc: 97% // tar acc: 96% // adv loss: 0.441428, aux loss: 1.472554, tar loss: 0.877150\n",
      "=====================\n",
      "Epoch 36/200, Batch 832/938\n",
      "D loss: 1.078055, acc: 98% // tar acc: 100% // adv loss: 0.502879, aux loss: 1.463466, tar loss: 0.882251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 36/200, Batch 882/938\n",
      "D loss: 1.113237, acc: 98% // tar acc: 93% // adv loss: 0.390070, aux loss: 1.492458, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 36/200, Batch 932/938\n",
      "D loss: 1.081161, acc: 96% // tar acc: 96% // adv loss: 0.493913, aux loss: 1.462866, tar loss: 0.890960\n",
      "=====================\n",
      "Epoch 37/200, Batch 44/938\n",
      "D loss: 1.028352, acc: 99% // tar acc: 95% // adv loss: 0.356305, aux loss: 1.474507, tar loss: 0.869804\n",
      "=====================\n",
      "Epoch 37/200, Batch 94/938\n",
      "D loss: 1.101522, acc: 97% // tar acc: 98% // adv loss: 0.477871, aux loss: 1.461232, tar loss: 0.899112\n",
      "=====================\n",
      "Epoch 37/200, Batch 144/938\n",
      "D loss: 1.061122, acc: 98% // tar acc: 100% // adv loss: 0.436237, aux loss: 1.461815, tar loss: 0.916541\n",
      "=====================\n",
      "Epoch 37/200, Batch 194/938\n",
      "D loss: 1.055031, acc: 97% // tar acc: 96% // adv loss: 0.497657, aux loss: 1.484795, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 37/200, Batch 244/938\n",
      "D loss: 1.038330, acc: 96% // tar acc: 98% // adv loss: 0.390840, aux loss: 1.465723, tar loss: 0.872913\n",
      "=====================\n",
      "Epoch 37/200, Batch 294/938\n",
      "D loss: 1.102272, acc: 97% // tar acc: 93% // adv loss: 0.303157, aux loss: 1.486667, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 37/200, Batch 344/938\n",
      "D loss: 1.127501, acc: 98% // tar acc: 96% // adv loss: 0.356847, aux loss: 1.469671, tar loss: 0.831475\n",
      "=====================\n",
      "Epoch 37/200, Batch 394/938\n",
      "D loss: 1.098793, acc: 97% // tar acc: 96% // adv loss: 0.507687, aux loss: 1.466712, tar loss: 0.870141\n",
      "=====================\n",
      "Epoch 37/200, Batch 444/938\n",
      "D loss: 1.021781, acc: 99% // tar acc: 98% // adv loss: 0.559427, aux loss: 1.475341, tar loss: 0.863534\n",
      "=====================\n",
      "Epoch 37/200, Batch 494/938\n",
      "D loss: 1.053671, acc: 99% // tar acc: 96% // adv loss: 0.663009, aux loss: 1.462584, tar loss: 0.837230\n",
      "=====================\n",
      "Epoch 37/200, Batch 544/938\n",
      "D loss: 1.090303, acc: 96% // tar acc: 96% // adv loss: 0.485025, aux loss: 1.463351, tar loss: 0.833773\n",
      "=====================\n",
      "Epoch 37/200, Batch 594/938\n",
      "D loss: 1.040143, acc: 98% // tar acc: 96% // adv loss: 0.586064, aux loss: 1.477984, tar loss: 0.891047\n",
      "=====================\n",
      "Epoch 37/200, Batch 644/938\n",
      "D loss: 1.117090, acc: 96% // tar acc: 95% // adv loss: 0.489015, aux loss: 1.476233, tar loss: 0.851679\n",
      "=====================\n",
      "Epoch 37/200, Batch 694/938\n",
      "D loss: 1.049416, acc: 99% // tar acc: 95% // adv loss: 0.441390, aux loss: 1.476724, tar loss: 0.880900\n",
      "=====================\n",
      "Epoch 37/200, Batch 744/938\n",
      "D loss: 1.111542, acc: 99% // tar acc: 98% // adv loss: 0.474608, aux loss: 1.485164, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 37/200, Batch 794/938\n",
      "D loss: 1.046109, acc: 96% // tar acc: 93% // adv loss: 0.490077, aux loss: 1.463289, tar loss: 0.816226\n",
      "=====================\n",
      "Epoch 37/200, Batch 844/938\n",
      "D loss: 1.033094, acc: 98% // tar acc: 92% // adv loss: 0.621374, aux loss: 1.507299, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 37/200, Batch 894/938\n",
      "D loss: 1.115653, acc: 96% // tar acc: 93% // adv loss: 0.612676, aux loss: 1.461497, tar loss: 0.859543\n",
      "=====================\n",
      "Epoch 38/200, Batch 6/938\n",
      "D loss: 1.061239, acc: 98% // tar acc: 96% // adv loss: 0.485246, aux loss: 1.477062, tar loss: 0.855371\n",
      "=====================\n",
      "Epoch 38/200, Batch 56/938\n",
      "D loss: 1.053056, acc: 95% // tar acc: 93% // adv loss: 0.473630, aux loss: 1.475935, tar loss: 0.850551\n",
      "=====================\n",
      "Epoch 38/200, Batch 106/938\n",
      "D loss: 1.041523, acc: 97% // tar acc: 95% // adv loss: 0.615059, aux loss: 1.463656, tar loss: 0.867316\n",
      "=====================\n",
      "Epoch 38/200, Batch 156/938\n",
      "D loss: 1.082585, acc: 96% // tar acc: 93% // adv loss: 0.454923, aux loss: 1.471012, tar loss: 0.825858\n",
      "=====================\n",
      "Epoch 38/200, Batch 206/938\n",
      "D loss: 1.085823, acc: 98% // tar acc: 90% // adv loss: 0.727891, aux loss: 1.476587, tar loss: 0.853394\n",
      "=====================\n",
      "Epoch 38/200, Batch 256/938\n",
      "D loss: 1.070963, acc: 96% // tar acc: 92% // adv loss: 0.445439, aux loss: 1.477064, tar loss: 0.816505\n",
      "=====================\n",
      "Epoch 38/200, Batch 306/938\n",
      "D loss: 1.045679, acc: 98% // tar acc: 96% // adv loss: 0.677190, aux loss: 1.461304, tar loss: 0.832780\n",
      "=====================\n",
      "Epoch 38/200, Batch 356/938\n",
      "D loss: 1.057846, acc: 98% // tar acc: 96% // adv loss: 0.480609, aux loss: 1.467074, tar loss: 0.896238\n",
      "=====================\n",
      "Epoch 38/200, Batch 406/938\n",
      "D loss: 1.059006, acc: 97% // tar acc: 98% // adv loss: 0.553016, aux loss: 1.472736, tar loss: 0.892497\n",
      "=====================\n",
      "Epoch 38/200, Batch 456/938\n",
      "D loss: 1.116394, acc: 97% // tar acc: 95% // adv loss: 0.463145, aux loss: 1.462139, tar loss: 0.844603\n",
      "=====================\n",
      "Epoch 38/200, Batch 506/938\n",
      "D loss: 1.082684, acc: 96% // tar acc: 93% // adv loss: 0.454704, aux loss: 1.465724, tar loss: 0.788647\n",
      "=====================\n",
      "Epoch 38/200, Batch 556/938\n",
      "D loss: 1.071793, acc: 97% // tar acc: 92% // adv loss: 0.533106, aux loss: 1.480549, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 38/200, Batch 606/938\n",
      "D loss: 1.040070, acc: 95% // tar acc: 85% // adv loss: 0.637866, aux loss: 1.472262, tar loss: 0.785662\n",
      "=====================\n",
      "Epoch 38/200, Batch 656/938\n",
      "D loss: 1.072602, acc: 95% // tar acc: 87% // adv loss: 0.564576, aux loss: 1.465327, tar loss: 0.859678\n",
      "=====================\n",
      "Epoch 38/200, Batch 706/938\n",
      "D loss: 1.032288, acc: 97% // tar acc: 90% // adv loss: 0.584728, aux loss: 1.463414, tar loss: 0.797637\n",
      "=====================\n",
      "Epoch 38/200, Batch 756/938\n",
      "D loss: 1.057618, acc: 98% // tar acc: 92% // adv loss: 0.435058, aux loss: 1.468369, tar loss: 0.849189\n",
      "=====================\n",
      "Epoch 38/200, Batch 806/938\n",
      "D loss: 1.082797, acc: 99% // tar acc: 95% // adv loss: 0.395155, aux loss: 1.477515, tar loss: 0.823557\n",
      "=====================\n",
      "Epoch 38/200, Batch 856/938\n",
      "D loss: 1.095104, acc: 95% // tar acc: 98% // adv loss: 0.669132, aux loss: 1.493275, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 38/200, Batch 906/938\n",
      "D loss: 1.054677, acc: 98% // tar acc: 95% // adv loss: 0.488936, aux loss: 1.471693, tar loss: 0.887096\n",
      "=====================\n",
      "Epoch 39/200, Batch 18/938\n",
      "D loss: 0.988067, acc: 99% // tar acc: 95% // adv loss: 0.507621, aux loss: 1.469860, tar loss: 0.862605\n",
      "=====================\n",
      "Epoch 39/200, Batch 68/938\n",
      "D loss: 1.034131, acc: 99% // tar acc: 96% // adv loss: 0.520570, aux loss: 1.462155, tar loss: 0.843565\n",
      "=====================\n",
      "Epoch 39/200, Batch 118/938\n",
      "D loss: 1.044286, acc: 97% // tar acc: 92% // adv loss: 0.405822, aux loss: 1.476055, tar loss: 0.856009\n",
      "=====================\n",
      "Epoch 39/200, Batch 168/938\n",
      "D loss: 1.022928, acc: 97% // tar acc: 98% // adv loss: 0.473997, aux loss: 1.463379, tar loss: 0.869635\n",
      "=====================\n",
      "Epoch 39/200, Batch 218/938\n",
      "D loss: 1.024303, acc: 96% // tar acc: 87% // adv loss: 0.699640, aux loss: 1.490890, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 39/200, Batch 268/938\n",
      "D loss: 1.098622, acc: 99% // tar acc: 92% // adv loss: 0.423063, aux loss: 1.461842, tar loss: 0.892761\n",
      "=====================\n",
      "Epoch 39/200, Batch 318/938\n",
      "D loss: 1.105823, acc: 98% // tar acc: 95% // adv loss: 0.550706, aux loss: 1.461611, tar loss: 0.879696\n",
      "=====================\n",
      "Epoch 39/200, Batch 368/938\n",
      "D loss: 1.007941, acc: 99% // tar acc: 96% // adv loss: 0.629170, aux loss: 1.461892, tar loss: 0.875591\n",
      "=====================\n",
      "Epoch 39/200, Batch 418/938\n",
      "D loss: 1.127799, acc: 97% // tar acc: 95% // adv loss: 0.587971, aux loss: 1.472347, tar loss: 0.872180\n",
      "=====================\n",
      "Epoch 39/200, Batch 468/938\n",
      "D loss: 1.031784, acc: 97% // tar acc: 96% // adv loss: 0.530415, aux loss: 1.461896, tar loss: 0.901871\n",
      "=====================\n",
      "Epoch 39/200, Batch 518/938\n",
      "D loss: 1.072235, acc: 98% // tar acc: 92% // adv loss: 0.600037, aux loss: 1.487574, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 39/200, Batch 568/938\n",
      "D loss: 1.051217, acc: 97% // tar acc: 93% // adv loss: 0.464004, aux loss: 1.472678, tar loss: 0.869995\n",
      "=====================\n",
      "Epoch 39/200, Batch 618/938\n",
      "D loss: 1.059474, acc: 98% // tar acc: 96% // adv loss: 0.386766, aux loss: 1.482793, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 39/200, Batch 668/938\n",
      "D loss: 1.059549, acc: 99% // tar acc: 100% // adv loss: 0.512289, aux loss: 1.461209, tar loss: 0.859739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 39/200, Batch 718/938\n",
      "D loss: 1.011153, acc: 96% // tar acc: 96% // adv loss: 0.680935, aux loss: 1.475346, tar loss: 0.851068\n",
      "=====================\n",
      "Epoch 39/200, Batch 768/938\n",
      "D loss: 1.041298, acc: 96% // tar acc: 100% // adv loss: 0.479424, aux loss: 1.461858, tar loss: 0.860237\n",
      "=====================\n",
      "Epoch 39/200, Batch 818/938\n",
      "D loss: 1.041466, acc: 98% // tar acc: 98% // adv loss: 0.386612, aux loss: 1.471186, tar loss: 0.827902\n",
      "=====================\n",
      "Epoch 39/200, Batch 868/938\n",
      "D loss: 1.058175, acc: 96% // tar acc: 90% // adv loss: 0.474634, aux loss: 1.462901, tar loss: 0.807265\n",
      "=====================\n",
      "Epoch 39/200, Batch 918/938\n",
      "D loss: 1.086494, acc: 98% // tar acc: 96% // adv loss: 0.349547, aux loss: 1.470438, tar loss: 0.879366\n",
      "=====================\n",
      "Epoch 40/200, Batch 30/938\n",
      "D loss: 1.040520, acc: 98% // tar acc: 93% // adv loss: 0.415756, aux loss: 1.492503, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 40/200, Batch 80/938\n",
      "D loss: 1.052274, acc: 97% // tar acc: 93% // adv loss: 0.499972, aux loss: 1.474958, tar loss: 0.834114\n",
      "=====================\n",
      "Epoch 40/200, Batch 130/938\n",
      "D loss: 1.061568, acc: 97% // tar acc: 92% // adv loss: 0.457989, aux loss: 1.462454, tar loss: 0.851307\n",
      "=====================\n",
      "Epoch 40/200, Batch 180/938\n",
      "D loss: 1.103548, acc: 96% // tar acc: 90% // adv loss: 0.464551, aux loss: 1.474085, tar loss: 0.855227\n",
      "=====================\n",
      "Epoch 40/200, Batch 230/938\n",
      "D loss: 1.025135, acc: 98% // tar acc: 96% // adv loss: 0.386357, aux loss: 1.472140, tar loss: 0.819264\n",
      "=====================\n",
      "Epoch 40/200, Batch 280/938\n",
      "D loss: 0.975144, acc: 99% // tar acc: 100% // adv loss: 0.535278, aux loss: 1.461992, tar loss: 0.835577\n",
      "=====================\n",
      "Epoch 40/200, Batch 330/938\n",
      "D loss: 1.039353, acc: 96% // tar acc: 95% // adv loss: 0.566608, aux loss: 1.476306, tar loss: 0.848019\n",
      "=====================\n",
      "Epoch 40/200, Batch 380/938\n",
      "D loss: 1.010944, acc: 97% // tar acc: 90% // adv loss: 0.592132, aux loss: 1.464724, tar loss: 0.827229\n",
      "=====================\n",
      "Epoch 40/200, Batch 430/938\n",
      "D loss: 1.068393, acc: 98% // tar acc: 98% // adv loss: 0.352157, aux loss: 1.474948, tar loss: 0.799315\n",
      "=====================\n",
      "Epoch 40/200, Batch 480/938\n",
      "D loss: 1.054383, acc: 99% // tar acc: 93% // adv loss: 0.476078, aux loss: 1.471791, tar loss: 0.846446\n",
      "=====================\n",
      "Epoch 40/200, Batch 530/938\n",
      "D loss: 1.040882, acc: 96% // tar acc: 95% // adv loss: 0.583372, aux loss: 1.480673, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 40/200, Batch 580/938\n",
      "D loss: 1.089700, acc: 98% // tar acc: 100% // adv loss: 0.497193, aux loss: 1.479735, tar loss: 0.867817\n",
      "=====================\n",
      "Epoch 40/200, Batch 630/938\n",
      "D loss: 1.051767, acc: 100% // tar acc: 95% // adv loss: 0.551857, aux loss: 1.462945, tar loss: 0.878292\n",
      "=====================\n",
      "Epoch 40/200, Batch 680/938\n",
      "D loss: 1.020477, acc: 95% // tar acc: 87% // adv loss: 0.436517, aux loss: 1.477787, tar loss: 0.799774\n",
      "=====================\n",
      "Epoch 40/200, Batch 730/938\n",
      "D loss: 1.089130, acc: 98% // tar acc: 95% // adv loss: 0.364189, aux loss: 1.476880, tar loss: 0.813986\n",
      "=====================\n",
      "Epoch 40/200, Batch 780/938\n",
      "D loss: 0.979120, acc: 98% // tar acc: 95% // adv loss: 0.537211, aux loss: 1.468804, tar loss: 0.834162\n",
      "=====================\n",
      "Epoch 40/200, Batch 830/938\n",
      "D loss: 1.039777, acc: 96% // tar acc: 93% // adv loss: 0.445254, aux loss: 1.478532, tar loss: 0.852762\n",
      "=====================\n",
      "Epoch 40/200, Batch 880/938\n",
      "D loss: 1.029642, acc: 98% // tar acc: 96% // adv loss: 0.440361, aux loss: 1.461585, tar loss: 0.888597\n",
      "=====================\n",
      "Epoch 40/200, Batch 930/938\n",
      "D loss: 1.035221, acc: 99% // tar acc: 96% // adv loss: 0.409698, aux loss: 1.461307, tar loss: 0.897502\n",
      "=====================\n",
      "Epoch 41/200, Batch 42/938\n",
      "D loss: 1.064510, acc: 96% // tar acc: 85% // adv loss: 0.427178, aux loss: 1.463277, tar loss: 0.812475\n",
      "=====================\n",
      "Epoch 41/200, Batch 92/938\n",
      "D loss: 1.034392, acc: 98% // tar acc: 89% // adv loss: 0.561027, aux loss: 1.505884, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 41/200, Batch 142/938\n",
      "D loss: 1.084769, acc: 99% // tar acc: 98% // adv loss: 0.534172, aux loss: 1.462356, tar loss: 0.838700\n",
      "=====================\n",
      "Epoch 41/200, Batch 192/938\n",
      "D loss: 1.056465, acc: 98% // tar acc: 93% // adv loss: 0.568182, aux loss: 1.463622, tar loss: 0.885348\n",
      "=====================\n",
      "Epoch 41/200, Batch 242/938\n",
      "D loss: 1.020375, acc: 99% // tar acc: 96% // adv loss: 0.427045, aux loss: 1.478989, tar loss: 0.851316\n",
      "=====================\n",
      "Epoch 41/200, Batch 292/938\n",
      "D loss: 1.092716, acc: 97% // tar acc: 96% // adv loss: 0.472402, aux loss: 1.467146, tar loss: 0.862783\n",
      "=====================\n",
      "Epoch 41/200, Batch 342/938\n",
      "D loss: 1.049229, acc: 98% // tar acc: 93% // adv loss: 0.384229, aux loss: 1.477468, tar loss: 0.845647\n",
      "=====================\n",
      "Epoch 41/200, Batch 392/938\n",
      "D loss: 1.035022, acc: 97% // tar acc: 93% // adv loss: 0.414309, aux loss: 1.463192, tar loss: 0.844090\n",
      "=====================\n",
      "Epoch 41/200, Batch 442/938\n",
      "D loss: 1.065568, acc: 97% // tar acc: 96% // adv loss: 0.566633, aux loss: 1.462543, tar loss: 0.795522\n",
      "=====================\n",
      "Epoch 41/200, Batch 492/938\n",
      "D loss: 1.046464, acc: 97% // tar acc: 96% // adv loss: 0.372155, aux loss: 1.461342, tar loss: 0.864740\n",
      "=====================\n",
      "Epoch 41/200, Batch 542/938\n",
      "D loss: 1.047786, acc: 98% // tar acc: 98% // adv loss: 0.483852, aux loss: 1.466425, tar loss: 0.879314\n",
      "=====================\n",
      "Epoch 41/200, Batch 592/938\n",
      "D loss: 1.057132, acc: 97% // tar acc: 93% // adv loss: 0.491251, aux loss: 1.461269, tar loss: 0.835562\n",
      "=====================\n",
      "Epoch 41/200, Batch 642/938\n",
      "D loss: 1.117445, acc: 96% // tar acc: 93% // adv loss: 0.525713, aux loss: 1.469619, tar loss: 0.810810\n",
      "=====================\n",
      "Epoch 41/200, Batch 692/938\n",
      "D loss: 1.031229, acc: 96% // tar acc: 89% // adv loss: 0.387701, aux loss: 1.478350, tar loss: 0.819442\n",
      "=====================\n",
      "Epoch 41/200, Batch 742/938\n",
      "D loss: 1.037268, acc: 94% // tar acc: 96% // adv loss: 0.404465, aux loss: 1.462077, tar loss: 0.900518\n",
      "=====================\n",
      "Epoch 41/200, Batch 792/938\n",
      "D loss: 1.044828, acc: 96% // tar acc: 90% // adv loss: 0.336696, aux loss: 1.461501, tar loss: 0.808537\n",
      "=====================\n",
      "Epoch 41/200, Batch 842/938\n",
      "D loss: 1.029671, acc: 99% // tar acc: 93% // adv loss: 0.264600, aux loss: 1.462887, tar loss: 0.845674\n",
      "=====================\n",
      "Epoch 41/200, Batch 892/938\n",
      "D loss: 1.071711, acc: 98% // tar acc: 96% // adv loss: 0.478620, aux loss: 1.461263, tar loss: 0.842075\n",
      "=====================\n",
      "Epoch 42/200, Batch 4/938\n",
      "D loss: 1.054006, acc: 96% // tar acc: 95% // adv loss: 0.308259, aux loss: 1.487838, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 42/200, Batch 54/938\n",
      "D loss: 1.057214, acc: 100% // tar acc: 92% // adv loss: 0.379743, aux loss: 1.483441, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 42/200, Batch 104/938\n",
      "D loss: 1.082494, acc: 96% // tar acc: 89% // adv loss: 0.532744, aux loss: 1.461623, tar loss: 0.866212\n",
      "=====================\n",
      "Epoch 42/200, Batch 154/938\n",
      "D loss: 1.057323, acc: 97% // tar acc: 93% // adv loss: 0.617342, aux loss: 1.461407, tar loss: 0.842861\n",
      "=====================\n",
      "Epoch 42/200, Batch 204/938\n",
      "D loss: 1.051504, acc: 100% // tar acc: 95% // adv loss: 0.286906, aux loss: 1.461474, tar loss: 0.822664\n",
      "=====================\n",
      "Epoch 42/200, Batch 254/938\n",
      "D loss: 1.061463, acc: 100% // tar acc: 90% // adv loss: 0.442409, aux loss: 1.461925, tar loss: 0.826488\n",
      "=====================\n",
      "Epoch 42/200, Batch 304/938\n",
      "D loss: 1.083764, acc: 95% // tar acc: 96% // adv loss: 0.403177, aux loss: 1.465433, tar loss: 0.910979\n",
      "=====================\n",
      "Epoch 42/200, Batch 354/938\n",
      "D loss: 1.028118, acc: 97% // tar acc: 93% // adv loss: 0.546818, aux loss: 1.462820, tar loss: 0.843307\n",
      "=====================\n",
      "Epoch 42/200, Batch 404/938\n",
      "D loss: 1.068733, acc: 97% // tar acc: 92% // adv loss: 0.456463, aux loss: 1.461325, tar loss: 0.856468\n",
      "=====================\n",
      "Epoch 42/200, Batch 454/938\n",
      "D loss: 1.042331, acc: 100% // tar acc: 93% // adv loss: 0.608145, aux loss: 1.499425, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 42/200, Batch 504/938\n",
      "D loss: 1.015070, acc: 96% // tar acc: 93% // adv loss: 0.488150, aux loss: 1.462465, tar loss: 0.756393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 42/200, Batch 554/938\n",
      "D loss: 1.071511, acc: 97% // tar acc: 95% // adv loss: 0.477372, aux loss: 1.483187, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 42/200, Batch 604/938\n",
      "D loss: 1.063801, acc: 98% // tar acc: 98% // adv loss: 0.460043, aux loss: 1.461429, tar loss: 0.813886\n",
      "=====================\n",
      "Epoch 42/200, Batch 654/938\n",
      "D loss: 1.032218, acc: 96% // tar acc: 93% // adv loss: 0.655532, aux loss: 1.490922, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 42/200, Batch 704/938\n",
      "D loss: 1.109833, acc: 99% // tar acc: 96% // adv loss: 0.401485, aux loss: 1.484831, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 42/200, Batch 754/938\n",
      "D loss: 1.021461, acc: 98% // tar acc: 93% // adv loss: 0.538050, aux loss: 1.461347, tar loss: 0.848877\n",
      "=====================\n",
      "Epoch 42/200, Batch 804/938\n",
      "D loss: 1.062309, acc: 99% // tar acc: 95% // adv loss: 0.367216, aux loss: 1.475803, tar loss: 0.851298\n",
      "=====================\n",
      "Epoch 42/200, Batch 854/938\n",
      "D loss: 1.009021, acc: 95% // tar acc: 89% // adv loss: 0.564347, aux loss: 1.464502, tar loss: 0.774345\n",
      "=====================\n",
      "Epoch 42/200, Batch 904/938\n",
      "D loss: 1.082454, acc: 97% // tar acc: 93% // adv loss: 0.389133, aux loss: 1.461236, tar loss: 0.864096\n",
      "=====================\n",
      "Epoch 43/200, Batch 16/938\n",
      "D loss: 0.989381, acc: 97% // tar acc: 95% // adv loss: 0.482078, aux loss: 1.464037, tar loss: 0.806678\n",
      "=====================\n",
      "Epoch 43/200, Batch 66/938\n",
      "D loss: 1.063110, acc: 97% // tar acc: 96% // adv loss: 0.381134, aux loss: 1.463465, tar loss: 0.826541\n",
      "=====================\n",
      "Epoch 43/200, Batch 116/938\n",
      "D loss: 1.042864, acc: 96% // tar acc: 92% // adv loss: 0.471285, aux loss: 1.478663, tar loss: 0.826819\n",
      "=====================\n",
      "Epoch 43/200, Batch 166/938\n",
      "D loss: 1.033366, acc: 97% // tar acc: 92% // adv loss: 0.403077, aux loss: 1.504788, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 43/200, Batch 216/938\n",
      "D loss: 1.037090, acc: 97% // tar acc: 96% // adv loss: 0.546359, aux loss: 1.462313, tar loss: 0.821601\n",
      "=====================\n",
      "Epoch 43/200, Batch 266/938\n",
      "D loss: 0.990242, acc: 100% // tar acc: 90% // adv loss: 0.445354, aux loss: 1.469553, tar loss: 0.799582\n",
      "=====================\n",
      "Epoch 43/200, Batch 316/938\n",
      "D loss: 1.084015, acc: 97% // tar acc: 93% // adv loss: 0.513675, aux loss: 1.470891, tar loss: 0.822616\n",
      "=====================\n",
      "Epoch 43/200, Batch 366/938\n",
      "D loss: 1.062088, acc: 99% // tar acc: 96% // adv loss: 0.506971, aux loss: 1.470301, tar loss: 0.796687\n",
      "=====================\n",
      "Epoch 43/200, Batch 416/938\n",
      "D loss: 1.070341, acc: 97% // tar acc: 92% // adv loss: 0.339565, aux loss: 1.462173, tar loss: 0.829415\n",
      "=====================\n",
      "Epoch 43/200, Batch 466/938\n",
      "D loss: 1.078981, acc: 95% // tar acc: 95% // adv loss: 0.489118, aux loss: 1.464370, tar loss: 0.829950\n",
      "=====================\n",
      "Epoch 43/200, Batch 516/938\n",
      "D loss: 1.034638, acc: 99% // tar acc: 96% // adv loss: 0.589146, aux loss: 1.469190, tar loss: 0.863664\n",
      "=====================\n",
      "Epoch 43/200, Batch 566/938\n",
      "D loss: 1.065580, acc: 98% // tar acc: 96% // adv loss: 0.562654, aux loss: 1.461199, tar loss: 0.840759\n",
      "=====================\n",
      "Epoch 43/200, Batch 616/938\n",
      "D loss: 1.066744, acc: 98% // tar acc: 98% // adv loss: 0.451631, aux loss: 1.461203, tar loss: 0.886531\n",
      "=====================\n",
      "Epoch 43/200, Batch 666/938\n",
      "D loss: 1.044759, acc: 99% // tar acc: 96% // adv loss: 0.431818, aux loss: 1.464956, tar loss: 0.884950\n",
      "=====================\n",
      "Epoch 43/200, Batch 716/938\n",
      "D loss: 1.016361, acc: 97% // tar acc: 93% // adv loss: 0.345440, aux loss: 1.476050, tar loss: 0.874743\n",
      "=====================\n",
      "Epoch 43/200, Batch 766/938\n",
      "D loss: 1.052073, acc: 98% // tar acc: 95% // adv loss: 0.384371, aux loss: 1.464949, tar loss: 0.891605\n",
      "=====================\n",
      "Epoch 43/200, Batch 816/938\n",
      "D loss: 1.016241, acc: 96% // tar acc: 85% // adv loss: 0.503934, aux loss: 1.504175, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 43/200, Batch 866/938\n",
      "D loss: 1.086346, acc: 99% // tar acc: 87% // adv loss: 0.395117, aux loss: 1.461768, tar loss: 0.852075\n",
      "=====================\n",
      "Epoch 43/200, Batch 916/938\n",
      "D loss: 1.014094, acc: 97% // tar acc: 98% // adv loss: 0.599012, aux loss: 1.462736, tar loss: 0.820802\n",
      "=====================\n",
      "Epoch 44/200, Batch 28/938\n",
      "D loss: 1.058521, acc: 96% // tar acc: 100% // adv loss: 0.345601, aux loss: 1.464153, tar loss: 0.881578\n",
      "=====================\n",
      "Epoch 44/200, Batch 78/938\n",
      "D loss: 0.992434, acc: 97% // tar acc: 90% // adv loss: 0.426935, aux loss: 1.478628, tar loss: 0.822380\n",
      "=====================\n",
      "Epoch 44/200, Batch 128/938\n",
      "D loss: 1.061612, acc: 96% // tar acc: 98% // adv loss: 0.508178, aux loss: 1.462320, tar loss: 0.887085\n",
      "=====================\n",
      "Epoch 44/200, Batch 178/938\n",
      "D loss: 1.032971, acc: 99% // tar acc: 93% // adv loss: 0.460250, aux loss: 1.476893, tar loss: 0.821661\n",
      "=====================\n",
      "Epoch 44/200, Batch 228/938\n",
      "D loss: 1.121849, acc: 97% // tar acc: 93% // adv loss: 0.433717, aux loss: 1.461913, tar loss: 0.849253\n",
      "=====================\n",
      "Epoch 44/200, Batch 278/938\n",
      "D loss: 1.000651, acc: 99% // tar acc: 95% // adv loss: 0.587558, aux loss: 1.477456, tar loss: 0.837584\n",
      "=====================\n",
      "Epoch 44/200, Batch 328/938\n",
      "D loss: 1.051594, acc: 99% // tar acc: 100% // adv loss: 0.431266, aux loss: 1.474105, tar loss: 0.826665\n",
      "=====================\n",
      "Epoch 44/200, Batch 378/938\n",
      "D loss: 1.049019, acc: 97% // tar acc: 93% // adv loss: 0.460764, aux loss: 1.464549, tar loss: 0.836749\n",
      "=====================\n",
      "Epoch 44/200, Batch 428/938\n",
      "D loss: 1.124072, acc: 98% // tar acc: 96% // adv loss: 0.522811, aux loss: 1.468417, tar loss: 0.845114\n",
      "=====================\n",
      "Epoch 44/200, Batch 478/938\n",
      "D loss: 1.054950, acc: 98% // tar acc: 93% // adv loss: 0.404007, aux loss: 1.471322, tar loss: 0.832914\n",
      "=====================\n",
      "Epoch 44/200, Batch 528/938\n",
      "D loss: 1.095896, acc: 98% // tar acc: 93% // adv loss: 0.470302, aux loss: 1.467052, tar loss: 0.852305\n",
      "=====================\n",
      "Epoch 44/200, Batch 578/938\n",
      "D loss: 1.110007, acc: 97% // tar acc: 98% // adv loss: 0.352610, aux loss: 1.467074, tar loss: 0.843756\n",
      "=====================\n",
      "Epoch 44/200, Batch 628/938\n",
      "D loss: 1.040547, acc: 99% // tar acc: 95% // adv loss: 0.459253, aux loss: 1.475852, tar loss: 0.848094\n",
      "=====================\n",
      "Epoch 44/200, Batch 678/938\n",
      "D loss: 1.070785, acc: 98% // tar acc: 95% // adv loss: 0.493310, aux loss: 1.461552, tar loss: 0.820375\n",
      "=====================\n",
      "Epoch 44/200, Batch 728/938\n",
      "D loss: 1.019725, acc: 97% // tar acc: 93% // adv loss: 0.630241, aux loss: 1.478054, tar loss: 0.808006\n",
      "=====================\n",
      "Epoch 44/200, Batch 778/938\n",
      "D loss: 1.095785, acc: 96% // tar acc: 95% // adv loss: 0.450203, aux loss: 1.475993, tar loss: 0.889559\n",
      "=====================\n",
      "Epoch 44/200, Batch 828/938\n",
      "D loss: 1.046410, acc: 99% // tar acc: 96% // adv loss: 0.585232, aux loss: 1.461642, tar loss: 0.880317\n",
      "=====================\n",
      "Epoch 44/200, Batch 878/938\n",
      "D loss: 1.004895, acc: 98% // tar acc: 96% // adv loss: 0.544187, aux loss: 1.461498, tar loss: 0.791521\n",
      "=====================\n",
      "Epoch 44/200, Batch 928/938\n",
      "D loss: 1.041656, acc: 99% // tar acc: 90% // adv loss: 0.481149, aux loss: 1.461579, tar loss: 0.833769\n",
      "=====================\n",
      "Epoch 45/200, Batch 40/938\n",
      "D loss: 1.003815, acc: 97% // tar acc: 90% // adv loss: 0.417494, aux loss: 1.483026, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 45/200, Batch 90/938\n",
      "D loss: 1.008965, acc: 98% // tar acc: 96% // adv loss: 0.430163, aux loss: 1.461159, tar loss: 0.903464\n",
      "=====================\n",
      "Epoch 45/200, Batch 140/938\n",
      "D loss: 1.026357, acc: 98% // tar acc: 96% // adv loss: 0.399908, aux loss: 1.464033, tar loss: 0.904167\n",
      "=====================\n",
      "Epoch 45/200, Batch 190/938\n",
      "D loss: 1.088333, acc: 98% // tar acc: 92% // adv loss: 0.544333, aux loss: 1.489294, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 45/200, Batch 240/938\n",
      "D loss: 1.050474, acc: 97% // tar acc: 98% // adv loss: 0.646946, aux loss: 1.461231, tar loss: 0.849527\n",
      "=====================\n",
      "Epoch 45/200, Batch 290/938\n",
      "D loss: 1.108684, acc: 99% // tar acc: 95% // adv loss: 0.571524, aux loss: 1.477115, tar loss: 0.828632\n",
      "=====================\n",
      "Epoch 45/200, Batch 340/938\n",
      "D loss: 1.028107, acc: 100% // tar acc: 98% // adv loss: 0.380090, aux loss: 1.467064, tar loss: 0.835914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 45/200, Batch 390/938\n",
      "D loss: 1.046013, acc: 98% // tar acc: 93% // adv loss: 0.454302, aux loss: 1.476720, tar loss: 0.866646\n",
      "=====================\n",
      "Epoch 45/200, Batch 440/938\n",
      "D loss: 1.064235, acc: 96% // tar acc: 92% // adv loss: 0.501293, aux loss: 1.477557, tar loss: 0.853527\n",
      "=====================\n",
      "Epoch 45/200, Batch 490/938\n",
      "D loss: 1.092166, acc: 98% // tar acc: 96% // adv loss: 0.373194, aux loss: 1.463011, tar loss: 0.861882\n",
      "=====================\n",
      "Epoch 45/200, Batch 540/938\n",
      "D loss: 1.007354, acc: 97% // tar acc: 95% // adv loss: 0.479570, aux loss: 1.462834, tar loss: 0.829707\n",
      "=====================\n",
      "Epoch 45/200, Batch 590/938\n",
      "D loss: 1.070852, acc: 96% // tar acc: 89% // adv loss: 0.368878, aux loss: 1.464286, tar loss: 0.860291\n",
      "=====================\n",
      "Epoch 45/200, Batch 640/938\n",
      "D loss: 1.072504, acc: 96% // tar acc: 95% // adv loss: 0.617200, aux loss: 1.467952, tar loss: 0.835008\n",
      "=====================\n",
      "Epoch 45/200, Batch 690/938\n",
      "D loss: 1.100006, acc: 98% // tar acc: 96% // adv loss: 0.492066, aux loss: 1.461187, tar loss: 0.876914\n",
      "=====================\n",
      "Epoch 45/200, Batch 740/938\n",
      "D loss: 1.041814, acc: 98% // tar acc: 95% // adv loss: 0.511669, aux loss: 1.479735, tar loss: 0.869512\n",
      "=====================\n",
      "Epoch 45/200, Batch 790/938\n",
      "D loss: 0.982323, acc: 99% // tar acc: 93% // adv loss: 0.600549, aux loss: 1.488604, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 45/200, Batch 840/938\n",
      "D loss: 1.077642, acc: 99% // tar acc: 96% // adv loss: 0.419174, aux loss: 1.466879, tar loss: 0.896189\n",
      "=====================\n",
      "Epoch 45/200, Batch 890/938\n",
      "D loss: 1.058266, acc: 96% // tar acc: 92% // adv loss: 0.447812, aux loss: 1.503736, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 46/200, Batch 2/938\n",
      "D loss: 1.036783, acc: 97% // tar acc: 93% // adv loss: 0.570401, aux loss: 1.462933, tar loss: 0.804818\n",
      "=====================\n",
      "Epoch 46/200, Batch 52/938\n",
      "D loss: 1.055087, acc: 100% // tar acc: 98% // adv loss: 0.533184, aux loss: 1.464471, tar loss: 0.878851\n",
      "=====================\n",
      "Epoch 46/200, Batch 102/938\n",
      "D loss: 1.040363, acc: 98% // tar acc: 98% // adv loss: 0.441427, aux loss: 1.475374, tar loss: 0.884864\n",
      "=====================\n",
      "Epoch 46/200, Batch 152/938\n",
      "D loss: 1.003276, acc: 99% // tar acc: 92% // adv loss: 0.512931, aux loss: 1.472227, tar loss: 0.858401\n",
      "=====================\n",
      "Epoch 46/200, Batch 202/938\n",
      "D loss: 1.063827, acc: 99% // tar acc: 95% // adv loss: 0.366965, aux loss: 1.466959, tar loss: 0.883698\n",
      "=====================\n",
      "Epoch 46/200, Batch 252/938\n",
      "D loss: 1.083489, acc: 97% // tar acc: 93% // adv loss: 0.508400, aux loss: 1.463629, tar loss: 0.872664\n",
      "=====================\n",
      "Epoch 46/200, Batch 302/938\n",
      "D loss: 1.074684, acc: 98% // tar acc: 96% // adv loss: 0.349928, aux loss: 1.461290, tar loss: 0.825072\n",
      "=====================\n",
      "Epoch 46/200, Batch 352/938\n",
      "D loss: 1.035723, acc: 98% // tar acc: 98% // adv loss: 0.528334, aux loss: 1.477430, tar loss: 0.888017\n",
      "=====================\n",
      "Epoch 46/200, Batch 402/938\n",
      "D loss: 1.064947, acc: 99% // tar acc: 95% // adv loss: 0.418795, aux loss: 1.461829, tar loss: 0.851173\n",
      "=====================\n",
      "Epoch 46/200, Batch 452/938\n",
      "D loss: 1.007438, acc: 98% // tar acc: 98% // adv loss: 0.382784, aux loss: 1.461285, tar loss: 0.920925\n",
      "=====================\n",
      "Epoch 46/200, Batch 502/938\n",
      "D loss: 1.161404, acc: 93% // tar acc: 93% // adv loss: 0.476242, aux loss: 1.484674, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 46/200, Batch 552/938\n",
      "D loss: 1.061038, acc: 99% // tar acc: 95% // adv loss: 0.557423, aux loss: 1.461437, tar loss: 0.865434\n",
      "=====================\n",
      "Epoch 46/200, Batch 602/938\n",
      "D loss: 1.093460, acc: 97% // tar acc: 93% // adv loss: 0.534252, aux loss: 1.470044, tar loss: 0.829822\n",
      "=====================\n",
      "Epoch 46/200, Batch 652/938\n",
      "D loss: 0.992320, acc: 99% // tar acc: 93% // adv loss: 0.483469, aux loss: 1.494300, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 46/200, Batch 702/938\n",
      "D loss: 1.036818, acc: 96% // tar acc: 93% // adv loss: 0.484096, aux loss: 1.461297, tar loss: 0.896240\n",
      "=====================\n",
      "Epoch 46/200, Batch 752/938\n",
      "D loss: 1.044512, acc: 96% // tar acc: 93% // adv loss: 0.470362, aux loss: 1.461495, tar loss: 0.823515\n",
      "=====================\n",
      "Epoch 46/200, Batch 802/938\n",
      "D loss: 1.083675, acc: 100% // tar acc: 96% // adv loss: 0.319441, aux loss: 1.463355, tar loss: 0.852576\n",
      "=====================\n",
      "Epoch 46/200, Batch 852/938\n",
      "D loss: 1.092757, acc: 98% // tar acc: 93% // adv loss: 0.415343, aux loss: 1.469199, tar loss: 0.830341\n",
      "=====================\n",
      "Epoch 46/200, Batch 902/938\n",
      "D loss: 1.079266, acc: 96% // tar acc: 96% // adv loss: 0.393457, aux loss: 1.461809, tar loss: 0.873178\n",
      "=====================\n",
      "Epoch 47/200, Batch 14/938\n",
      "D loss: 1.066297, acc: 98% // tar acc: 96% // adv loss: 0.302377, aux loss: 1.462314, tar loss: 0.896337\n",
      "=====================\n",
      "Epoch 47/200, Batch 64/938\n",
      "D loss: 1.057271, acc: 98% // tar acc: 87% // adv loss: 0.607536, aux loss: 1.483295, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 47/200, Batch 114/938\n",
      "D loss: 1.004866, acc: 96% // tar acc: 90% // adv loss: 0.454145, aux loss: 1.478126, tar loss: 0.811007\n",
      "=====================\n",
      "Epoch 47/200, Batch 164/938\n",
      "D loss: 1.072646, acc: 100% // tar acc: 93% // adv loss: 0.491183, aux loss: 1.462852, tar loss: 0.840799\n",
      "=====================\n",
      "Epoch 47/200, Batch 214/938\n",
      "D loss: 1.049839, acc: 99% // tar acc: 95% // adv loss: 0.445824, aux loss: 1.461978, tar loss: 0.850402\n",
      "=====================\n",
      "Epoch 47/200, Batch 264/938\n",
      "D loss: 1.046851, acc: 97% // tar acc: 100% // adv loss: 0.424424, aux loss: 1.461455, tar loss: 0.868447\n",
      "=====================\n",
      "Epoch 47/200, Batch 314/938\n",
      "D loss: 1.005584, acc: 100% // tar acc: 96% // adv loss: 0.366944, aux loss: 1.483037, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 47/200, Batch 364/938\n",
      "D loss: 1.051183, acc: 99% // tar acc: 93% // adv loss: 0.333647, aux loss: 1.466960, tar loss: 0.841440\n",
      "=====================\n",
      "Epoch 47/200, Batch 414/938\n",
      "D loss: 1.062317, acc: 97% // tar acc: 93% // adv loss: 0.392859, aux loss: 1.467619, tar loss: 0.848949\n",
      "=====================\n",
      "Epoch 47/200, Batch 464/938\n",
      "D loss: 1.114200, acc: 98% // tar acc: 90% // adv loss: 0.501890, aux loss: 1.478964, tar loss: 0.810657\n",
      "=====================\n",
      "Epoch 47/200, Batch 514/938\n",
      "D loss: 1.040463, acc: 97% // tar acc: 92% // adv loss: 0.476156, aux loss: 1.490557, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 47/200, Batch 564/938\n",
      "D loss: 1.047251, acc: 98% // tar acc: 98% // adv loss: 0.579654, aux loss: 1.461378, tar loss: 0.863977\n",
      "=====================\n",
      "Epoch 47/200, Batch 614/938\n",
      "D loss: 1.084506, acc: 100% // tar acc: 98% // adv loss: 0.516907, aux loss: 1.461675, tar loss: 0.892046\n",
      "=====================\n",
      "Epoch 47/200, Batch 664/938\n",
      "D loss: 1.011828, acc: 98% // tar acc: 93% // adv loss: 0.521371, aux loss: 1.461871, tar loss: 0.804837\n",
      "=====================\n",
      "Epoch 47/200, Batch 714/938\n",
      "D loss: 1.081099, acc: 99% // tar acc: 95% // adv loss: 0.343066, aux loss: 1.468817, tar loss: 0.824370\n",
      "=====================\n",
      "Epoch 47/200, Batch 764/938\n",
      "D loss: 1.059225, acc: 98% // tar acc: 96% // adv loss: 0.446555, aux loss: 1.488033, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 47/200, Batch 814/938\n",
      "D loss: 1.057089, acc: 99% // tar acc: 96% // adv loss: 0.488226, aux loss: 1.475639, tar loss: 0.860280\n",
      "=====================\n",
      "Epoch 47/200, Batch 864/938\n",
      "D loss: 1.026909, acc: 97% // tar acc: 90% // adv loss: 0.539960, aux loss: 1.465334, tar loss: 0.774827\n",
      "=====================\n",
      "Epoch 47/200, Batch 914/938\n",
      "D loss: 1.011945, acc: 98% // tar acc: 87% // adv loss: 0.399251, aux loss: 1.496317, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 48/200, Batch 26/938\n",
      "D loss: 1.054069, acc: 98% // tar acc: 100% // adv loss: 0.377785, aux loss: 1.462156, tar loss: 0.846580\n",
      "=====================\n",
      "Epoch 48/200, Batch 76/938\n",
      "D loss: 1.102965, acc: 97% // tar acc: 92% // adv loss: 0.273742, aux loss: 1.478960, tar loss: 0.852008\n",
      "=====================\n",
      "Epoch 48/200, Batch 126/938\n",
      "D loss: 1.069018, acc: 96% // tar acc: 89% // adv loss: 0.395707, aux loss: 1.483501, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 48/200, Batch 176/938\n",
      "D loss: 1.095064, acc: 99% // tar acc: 96% // adv loss: 0.362893, aux loss: 1.481223, tar loss: 1.332000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 48/200, Batch 226/938\n",
      "D loss: 1.051554, acc: 99% // tar acc: 95% // adv loss: 0.396994, aux loss: 1.461373, tar loss: 0.853559\n",
      "=====================\n",
      "Epoch 48/200, Batch 276/938\n",
      "D loss: 1.051345, acc: 99% // tar acc: 96% // adv loss: 0.627183, aux loss: 1.466664, tar loss: 0.826075\n",
      "=====================\n",
      "Epoch 48/200, Batch 326/938\n",
      "D loss: 1.084786, acc: 98% // tar acc: 96% // adv loss: 0.779379, aux loss: 1.461753, tar loss: 0.887404\n",
      "=====================\n",
      "Epoch 48/200, Batch 376/938\n",
      "D loss: 1.056971, acc: 97% // tar acc: 92% // adv loss: 0.427695, aux loss: 1.478795, tar loss: 0.828706\n",
      "=====================\n",
      "Epoch 48/200, Batch 426/938\n",
      "D loss: 1.034169, acc: 100% // tar acc: 95% // adv loss: 0.513967, aux loss: 1.482424, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 48/200, Batch 476/938\n",
      "D loss: 0.993547, acc: 100% // tar acc: 95% // adv loss: 0.537455, aux loss: 1.478959, tar loss: 0.818730\n",
      "=====================\n",
      "Epoch 48/200, Batch 526/938\n",
      "D loss: 1.094740, acc: 96% // tar acc: 92% // adv loss: 0.462629, aux loss: 1.478349, tar loss: 0.824505\n",
      "=====================\n",
      "Epoch 48/200, Batch 576/938\n",
      "D loss: 1.050866, acc: 99% // tar acc: 96% // adv loss: 0.380552, aux loss: 1.472170, tar loss: 0.844034\n",
      "=====================\n",
      "Epoch 48/200, Batch 626/938\n",
      "D loss: 0.990641, acc: 98% // tar acc: 89% // adv loss: 0.392541, aux loss: 1.464332, tar loss: 0.795762\n",
      "=====================\n",
      "Epoch 48/200, Batch 676/938\n",
      "D loss: 1.132461, acc: 98% // tar acc: 95% // adv loss: 0.418630, aux loss: 1.481127, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 48/200, Batch 726/938\n",
      "D loss: 1.027645, acc: 96% // tar acc: 87% // adv loss: 0.497106, aux loss: 1.461215, tar loss: 0.785371\n",
      "=====================\n",
      "Epoch 48/200, Batch 776/938\n",
      "D loss: 1.042491, acc: 98% // tar acc: 92% // adv loss: 0.466558, aux loss: 1.477175, tar loss: 0.887082\n",
      "=====================\n",
      "Epoch 48/200, Batch 826/938\n",
      "D loss: 1.037408, acc: 99% // tar acc: 98% // adv loss: 0.534312, aux loss: 1.461258, tar loss: 0.845123\n",
      "=====================\n",
      "Epoch 48/200, Batch 876/938\n",
      "D loss: 1.055633, acc: 98% // tar acc: 93% // adv loss: 0.443990, aux loss: 1.481144, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 48/200, Batch 926/938\n",
      "D loss: 1.100758, acc: 98% // tar acc: 98% // adv loss: 0.296381, aux loss: 1.476031, tar loss: 0.839225\n",
      "=====================\n",
      "Epoch 49/200, Batch 38/938\n",
      "D loss: 1.059615, acc: 97% // tar acc: 95% // adv loss: 0.466893, aux loss: 1.462681, tar loss: 0.805713\n",
      "=====================\n",
      "Epoch 49/200, Batch 88/938\n",
      "D loss: 1.002130, acc: 95% // tar acc: 95% // adv loss: 0.373260, aux loss: 1.476342, tar loss: 0.858865\n",
      "=====================\n",
      "Epoch 49/200, Batch 138/938\n",
      "D loss: 1.029320, acc: 98% // tar acc: 96% // adv loss: 0.336371, aux loss: 1.478896, tar loss: 0.833252\n",
      "=====================\n",
      "Epoch 49/200, Batch 188/938\n",
      "D loss: 1.099681, acc: 93% // tar acc: 93% // adv loss: 0.373659, aux loss: 1.494413, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 49/200, Batch 238/938\n",
      "D loss: 1.073745, acc: 96% // tar acc: 90% // adv loss: 0.534222, aux loss: 1.461284, tar loss: 0.879076\n",
      "=====================\n",
      "Epoch 49/200, Batch 288/938\n",
      "D loss: 1.069044, acc: 98% // tar acc: 95% // adv loss: 0.492469, aux loss: 1.476143, tar loss: 0.862556\n",
      "=====================\n",
      "Epoch 49/200, Batch 338/938\n",
      "D loss: 1.042084, acc: 99% // tar acc: 85% // adv loss: 0.374134, aux loss: 1.508470, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 49/200, Batch 388/938\n",
      "D loss: 1.077794, acc: 99% // tar acc: 89% // adv loss: 0.596904, aux loss: 1.467278, tar loss: 0.795064\n",
      "=====================\n",
      "Epoch 49/200, Batch 438/938\n",
      "D loss: 1.003289, acc: 97% // tar acc: 93% // adv loss: 0.442815, aux loss: 1.491131, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 49/200, Batch 488/938\n",
      "D loss: 1.139118, acc: 95% // tar acc: 90% // adv loss: 0.330038, aux loss: 1.491675, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 49/200, Batch 538/938\n",
      "D loss: 1.033134, acc: 98% // tar acc: 92% // adv loss: 0.611582, aux loss: 1.464152, tar loss: 0.815513\n",
      "=====================\n",
      "Epoch 49/200, Batch 588/938\n",
      "D loss: 1.010432, acc: 99% // tar acc: 92% // adv loss: 0.514993, aux loss: 1.476011, tar loss: 0.788022\n",
      "=====================\n",
      "Epoch 49/200, Batch 638/938\n",
      "D loss: 1.073726, acc: 97% // tar acc: 90% // adv loss: 0.459893, aux loss: 1.477289, tar loss: 0.855291\n",
      "=====================\n",
      "Epoch 49/200, Batch 688/938\n",
      "D loss: 1.043499, acc: 97% // tar acc: 93% // adv loss: 0.369757, aux loss: 1.462345, tar loss: 0.825218\n",
      "=====================\n",
      "Epoch 49/200, Batch 738/938\n",
      "D loss: 1.089556, acc: 100% // tar acc: 95% // adv loss: 0.547116, aux loss: 1.476286, tar loss: 0.778686\n",
      "=====================\n",
      "Epoch 49/200, Batch 788/938\n",
      "D loss: 1.054291, acc: 99% // tar acc: 95% // adv loss: 0.320056, aux loss: 1.478583, tar loss: 0.848455\n",
      "=====================\n",
      "Epoch 49/200, Batch 838/938\n",
      "D loss: 1.046868, acc: 98% // tar acc: 87% // adv loss: 0.440862, aux loss: 1.477767, tar loss: 0.840081\n",
      "=====================\n",
      "Epoch 49/200, Batch 888/938\n",
      "D loss: 0.991383, acc: 99% // tar acc: 98% // adv loss: 0.511669, aux loss: 1.468213, tar loss: 0.896275\n",
      "=====================\n",
      "Epoch 50/200, Batch 0/938\n",
      "D loss: 1.049130, acc: 96% // tar acc: 95% // adv loss: 0.522884, aux loss: 1.481593, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 50/200, Batch 50/938\n",
      "D loss: 1.053831, acc: 99% // tar acc: 93% // adv loss: 0.421312, aux loss: 1.462734, tar loss: 0.827873\n",
      "=====================\n",
      "Epoch 50/200, Batch 100/938\n",
      "D loss: 1.077126, acc: 93% // tar acc: 92% // adv loss: 0.406152, aux loss: 1.476848, tar loss: 0.835374\n",
      "=====================\n",
      "Epoch 50/200, Batch 150/938\n",
      "D loss: 1.049350, acc: 97% // tar acc: 92% // adv loss: 0.708242, aux loss: 1.477697, tar loss: 0.801654\n",
      "=====================\n",
      "Epoch 50/200, Batch 200/938\n",
      "D loss: 1.130674, acc: 98% // tar acc: 98% // adv loss: 0.372362, aux loss: 1.483325, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 50/200, Batch 250/938\n",
      "D loss: 1.053312, acc: 99% // tar acc: 96% // adv loss: 0.467611, aux loss: 1.464708, tar loss: 0.842762\n",
      "=====================\n",
      "Epoch 50/200, Batch 300/938\n",
      "D loss: 1.061740, acc: 97% // tar acc: 93% // adv loss: 0.325489, aux loss: 1.466905, tar loss: 0.845999\n",
      "=====================\n",
      "Epoch 50/200, Batch 350/938\n",
      "D loss: 1.042032, acc: 99% // tar acc: 90% // adv loss: 0.496001, aux loss: 1.464620, tar loss: 0.845128\n",
      "=====================\n",
      "Epoch 50/200, Batch 400/938\n",
      "D loss: 1.047021, acc: 94% // tar acc: 96% // adv loss: 0.342254, aux loss: 1.473538, tar loss: 0.876449\n",
      "=====================\n",
      "Epoch 50/200, Batch 450/938\n",
      "D loss: 1.009987, acc: 100% // tar acc: 95% // adv loss: 0.426407, aux loss: 1.483397, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 50/200, Batch 500/938\n",
      "D loss: 1.045435, acc: 99% // tar acc: 98% // adv loss: 0.614828, aux loss: 1.463749, tar loss: 0.830623\n",
      "=====================\n",
      "Epoch 50/200, Batch 550/938\n",
      "D loss: 1.083422, acc: 97% // tar acc: 96% // adv loss: 0.390194, aux loss: 1.493725, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 50/200, Batch 600/938\n",
      "D loss: 1.106451, acc: 96% // tar acc: 96% // adv loss: 0.519034, aux loss: 1.462301, tar loss: 0.878246\n",
      "=====================\n",
      "Epoch 50/200, Batch 650/938\n",
      "D loss: 1.083675, acc: 99% // tar acc: 96% // adv loss: 0.500331, aux loss: 1.466732, tar loss: 0.843293\n",
      "=====================\n",
      "Epoch 50/200, Batch 700/938\n",
      "D loss: 1.035247, acc: 96% // tar acc: 96% // adv loss: 0.354726, aux loss: 1.462233, tar loss: 0.850925\n",
      "=====================\n",
      "Epoch 50/200, Batch 750/938\n",
      "D loss: 1.073419, acc: 97% // tar acc: 96% // adv loss: 0.769731, aux loss: 1.461801, tar loss: 0.831258\n",
      "=====================\n",
      "Epoch 50/200, Batch 800/938\n",
      "D loss: 1.051766, acc: 97% // tar acc: 100% // adv loss: 0.392447, aux loss: 1.461821, tar loss: 0.862458\n",
      "=====================\n",
      "Epoch 50/200, Batch 850/938\n",
      "D loss: 1.095786, acc: 99% // tar acc: 93% // adv loss: 0.487392, aux loss: 1.487188, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 50/200, Batch 900/938\n",
      "D loss: 1.012714, acc: 98% // tar acc: 92% // adv loss: 0.420100, aux loss: 1.485032, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 51/200, Batch 12/938\n",
      "D loss: 1.041158, acc: 100% // tar acc: 95% // adv loss: 0.522640, aux loss: 1.503251, tar loss: 1.332000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 51/200, Batch 62/938\n",
      "D loss: 1.040140, acc: 97% // tar acc: 92% // adv loss: 0.427058, aux loss: 1.480176, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 51/200, Batch 112/938\n",
      "D loss: 1.086838, acc: 99% // tar acc: 90% // adv loss: 0.345802, aux loss: 1.471396, tar loss: 0.813835\n",
      "=====================\n",
      "Epoch 51/200, Batch 162/938\n",
      "D loss: 1.004254, acc: 98% // tar acc: 90% // adv loss: 0.399561, aux loss: 1.463565, tar loss: 0.839183\n",
      "=====================\n",
      "Epoch 51/200, Batch 212/938\n",
      "D loss: 1.035173, acc: 98% // tar acc: 95% // adv loss: 0.357739, aux loss: 1.461605, tar loss: 0.849507\n",
      "=====================\n",
      "Epoch 51/200, Batch 262/938\n",
      "D loss: 1.086543, acc: 99% // tar acc: 96% // adv loss: 0.584732, aux loss: 1.480723, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 51/200, Batch 312/938\n",
      "D loss: 1.052628, acc: 94% // tar acc: 98% // adv loss: 0.418524, aux loss: 1.462068, tar loss: 0.877622\n",
      "=====================\n",
      "Epoch 51/200, Batch 362/938\n",
      "D loss: 1.035240, acc: 99% // tar acc: 96% // adv loss: 0.291250, aux loss: 1.480517, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 51/200, Batch 412/938\n",
      "D loss: 1.048820, acc: 97% // tar acc: 100% // adv loss: 0.366909, aux loss: 1.477924, tar loss: 0.892133\n",
      "=====================\n",
      "Epoch 51/200, Batch 462/938\n",
      "D loss: 1.033137, acc: 99% // tar acc: 93% // adv loss: 0.642018, aux loss: 1.461581, tar loss: 0.868430\n",
      "=====================\n",
      "Epoch 51/200, Batch 512/938\n",
      "D loss: 1.063968, acc: 98% // tar acc: 93% // adv loss: 0.345205, aux loss: 1.483053, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 51/200, Batch 562/938\n",
      "D loss: 1.052854, acc: 99% // tar acc: 98% // adv loss: 0.582016, aux loss: 1.461379, tar loss: 0.866009\n",
      "=====================\n",
      "Epoch 51/200, Batch 612/938\n",
      "D loss: 1.048631, acc: 99% // tar acc: 95% // adv loss: 0.562006, aux loss: 1.466219, tar loss: 0.815598\n",
      "=====================\n",
      "Epoch 51/200, Batch 662/938\n",
      "D loss: 1.044478, acc: 97% // tar acc: 95% // adv loss: 0.545431, aux loss: 1.461952, tar loss: 0.856030\n",
      "=====================\n",
      "Epoch 51/200, Batch 712/938\n",
      "D loss: 1.084331, acc: 99% // tar acc: 95% // adv loss: 0.441690, aux loss: 1.516602, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 51/200, Batch 762/938\n",
      "D loss: 1.026279, acc: 99% // tar acc: 96% // adv loss: 0.534337, aux loss: 1.461269, tar loss: 0.789280\n",
      "=====================\n",
      "Epoch 51/200, Batch 812/938\n",
      "D loss: 1.033929, acc: 98% // tar acc: 89% // adv loss: 0.406558, aux loss: 1.479515, tar loss: 0.856242\n",
      "=====================\n",
      "Epoch 51/200, Batch 862/938\n",
      "D loss: 1.035176, acc: 98% // tar acc: 90% // adv loss: 0.512922, aux loss: 1.464755, tar loss: 0.863975\n",
      "=====================\n",
      "Epoch 51/200, Batch 912/938\n",
      "D loss: 1.066660, acc: 99% // tar acc: 93% // adv loss: 0.398977, aux loss: 1.468099, tar loss: 0.845508\n",
      "=====================\n",
      "Epoch 52/200, Batch 24/938\n",
      "D loss: 1.029499, acc: 95% // tar acc: 92% // adv loss: 0.546325, aux loss: 1.491670, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 52/200, Batch 74/938\n",
      "D loss: 1.055109, acc: 96% // tar acc: 98% // adv loss: 0.487870, aux loss: 1.494213, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 52/200, Batch 124/938\n",
      "D loss: 1.042008, acc: 98% // tar acc: 96% // adv loss: 0.372433, aux loss: 1.464019, tar loss: 0.852712\n",
      "=====================\n",
      "Epoch 52/200, Batch 174/938\n",
      "D loss: 0.971752, acc: 99% // tar acc: 98% // adv loss: 0.600669, aux loss: 1.462419, tar loss: 0.826057\n",
      "=====================\n",
      "Epoch 52/200, Batch 224/938\n",
      "D loss: 1.077687, acc: 100% // tar acc: 92% // adv loss: 0.698695, aux loss: 1.467638, tar loss: 0.813483\n",
      "=====================\n",
      "Epoch 52/200, Batch 274/938\n",
      "D loss: 1.057256, acc: 97% // tar acc: 95% // adv loss: 0.339298, aux loss: 1.483665, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 52/200, Batch 324/938\n",
      "D loss: 1.029170, acc: 96% // tar acc: 87% // adv loss: 0.340709, aux loss: 1.464060, tar loss: 0.838987\n",
      "=====================\n",
      "Epoch 52/200, Batch 374/938\n",
      "D loss: 1.023862, acc: 100% // tar acc: 98% // adv loss: 0.432430, aux loss: 1.476807, tar loss: 0.881954\n",
      "=====================\n",
      "Epoch 52/200, Batch 424/938\n",
      "D loss: 1.042978, acc: 99% // tar acc: 100% // adv loss: 0.356734, aux loss: 1.469931, tar loss: 0.917350\n",
      "=====================\n",
      "Epoch 52/200, Batch 474/938\n",
      "D loss: 0.988791, acc: 98% // tar acc: 93% // adv loss: 0.314593, aux loss: 1.461881, tar loss: 0.858515\n",
      "=====================\n",
      "Epoch 52/200, Batch 524/938\n",
      "D loss: 1.018443, acc: 97% // tar acc: 98% // adv loss: 0.385580, aux loss: 1.462843, tar loss: 0.852492\n",
      "=====================\n",
      "Epoch 52/200, Batch 574/938\n",
      "D loss: 1.033641, acc: 97% // tar acc: 93% // adv loss: 0.361652, aux loss: 1.466366, tar loss: 0.887975\n",
      "=====================\n",
      "Epoch 52/200, Batch 624/938\n",
      "D loss: 1.072325, acc: 99% // tar acc: 98% // adv loss: 0.424783, aux loss: 1.463639, tar loss: 0.881313\n",
      "=====================\n",
      "Epoch 52/200, Batch 674/938\n",
      "D loss: 1.021184, acc: 99% // tar acc: 93% // adv loss: 0.444088, aux loss: 1.467341, tar loss: 0.879134\n",
      "=====================\n",
      "Epoch 52/200, Batch 724/938\n",
      "D loss: 1.030877, acc: 99% // tar acc: 92% // adv loss: 0.394248, aux loss: 1.463633, tar loss: 0.829007\n",
      "=====================\n",
      "Epoch 52/200, Batch 774/938\n",
      "D loss: 1.096713, acc: 97% // tar acc: 98% // adv loss: 0.369527, aux loss: 1.466792, tar loss: 0.882514\n",
      "=====================\n",
      "Epoch 52/200, Batch 824/938\n",
      "D loss: 1.130476, acc: 96% // tar acc: 100% // adv loss: 0.511010, aux loss: 1.461468, tar loss: 0.934795\n",
      "=====================\n",
      "Epoch 52/200, Batch 874/938\n",
      "D loss: 1.055989, acc: 98% // tar acc: 96% // adv loss: 0.524492, aux loss: 1.461580, tar loss: 0.887850\n",
      "=====================\n",
      "Epoch 52/200, Batch 924/938\n",
      "D loss: 1.039291, acc: 97% // tar acc: 95% // adv loss: 0.538522, aux loss: 1.474681, tar loss: 0.860548\n",
      "=====================\n",
      "Epoch 53/200, Batch 36/938\n",
      "D loss: 1.117539, acc: 99% // tar acc: 96% // adv loss: 0.529963, aux loss: 1.464519, tar loss: 0.914171\n",
      "=====================\n",
      "Epoch 53/200, Batch 86/938\n",
      "D loss: 1.052845, acc: 99% // tar acc: 90% // adv loss: 0.441002, aux loss: 1.474217, tar loss: 0.832217\n",
      "=====================\n",
      "Epoch 53/200, Batch 136/938\n",
      "D loss: 1.028162, acc: 100% // tar acc: 93% // adv loss: 0.759725, aux loss: 1.480847, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 53/200, Batch 186/938\n",
      "D loss: 1.087972, acc: 98% // tar acc: 96% // adv loss: 0.610319, aux loss: 1.478848, tar loss: 0.867458\n",
      "=====================\n",
      "Epoch 53/200, Batch 236/938\n",
      "D loss: 1.071372, acc: 98% // tar acc: 93% // adv loss: 0.627724, aux loss: 1.462182, tar loss: 0.848188\n",
      "=====================\n",
      "Epoch 53/200, Batch 286/938\n",
      "D loss: 1.035553, acc: 100% // tar acc: 93% // adv loss: 0.550510, aux loss: 1.477590, tar loss: 0.865979\n",
      "=====================\n",
      "Epoch 53/200, Batch 336/938\n",
      "D loss: 1.046447, acc: 96% // tar acc: 95% // adv loss: 0.354970, aux loss: 1.476131, tar loss: 0.843895\n",
      "=====================\n",
      "Epoch 53/200, Batch 386/938\n",
      "D loss: 0.985029, acc: 99% // tar acc: 96% // adv loss: 0.461430, aux loss: 1.470315, tar loss: 0.831860\n",
      "=====================\n",
      "Epoch 53/200, Batch 436/938\n",
      "D loss: 1.097847, acc: 98% // tar acc: 96% // adv loss: 0.539790, aux loss: 1.469613, tar loss: 0.899217\n",
      "=====================\n",
      "Epoch 53/200, Batch 486/938\n",
      "D loss: 1.137218, acc: 96% // tar acc: 96% // adv loss: 0.451211, aux loss: 1.466534, tar loss: 0.882686\n",
      "=====================\n",
      "Epoch 53/200, Batch 536/938\n",
      "D loss: 1.113659, acc: 98% // tar acc: 98% // adv loss: 0.370644, aux loss: 1.461245, tar loss: 0.903122\n",
      "=====================\n",
      "Epoch 53/200, Batch 586/938\n",
      "D loss: 1.032237, acc: 99% // tar acc: 90% // adv loss: 0.374612, aux loss: 1.468211, tar loss: 0.845016\n",
      "=====================\n",
      "Epoch 53/200, Batch 636/938\n",
      "D loss: 1.102510, acc: 98% // tar acc: 96% // adv loss: 0.626534, aux loss: 1.462202, tar loss: 0.848274\n",
      "=====================\n",
      "Epoch 53/200, Batch 686/938\n",
      "D loss: 1.017061, acc: 97% // tar acc: 93% // adv loss: 0.474006, aux loss: 1.461731, tar loss: 0.747922\n",
      "=====================\n",
      "Epoch 53/200, Batch 736/938\n",
      "D loss: 1.015383, acc: 99% // tar acc: 95% // adv loss: 0.369900, aux loss: 1.473722, tar loss: 0.866907\n",
      "=====================\n",
      "Epoch 53/200, Batch 786/938\n",
      "D loss: 0.990040, acc: 96% // tar acc: 98% // adv loss: 0.443555, aux loss: 1.461492, tar loss: 0.864315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 53/200, Batch 836/938\n",
      "D loss: 1.057670, acc: 99% // tar acc: 92% // adv loss: 0.519091, aux loss: 1.466640, tar loss: 0.847083\n",
      "=====================\n",
      "Epoch 53/200, Batch 886/938\n",
      "D loss: 0.984225, acc: 98% // tar acc: 92% // adv loss: 0.335899, aux loss: 1.474901, tar loss: 0.801101\n",
      "=====================\n",
      "Epoch 53/200, Batch 936/938\n",
      "D loss: 1.022062, acc: 97% // tar acc: 92% // adv loss: 0.646098, aux loss: 1.461764, tar loss: 0.827620\n",
      "=====================\n",
      "Epoch 54/200, Batch 48/938\n",
      "D loss: 1.120870, acc: 99% // tar acc: 92% // adv loss: 0.611932, aux loss: 1.465980, tar loss: 0.889469\n",
      "=====================\n",
      "Epoch 54/200, Batch 98/938\n",
      "D loss: 1.127551, acc: 96% // tar acc: 93% // adv loss: 0.563047, aux loss: 1.496186, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 54/200, Batch 148/938\n",
      "D loss: 0.992530, acc: 99% // tar acc: 98% // adv loss: 0.489777, aux loss: 1.463752, tar loss: 0.816102\n",
      "=====================\n",
      "Epoch 54/200, Batch 198/938\n",
      "D loss: 1.019917, acc: 98% // tar acc: 93% // adv loss: 0.476274, aux loss: 1.476362, tar loss: 0.845422\n",
      "=====================\n",
      "Epoch 54/200, Batch 248/938\n",
      "D loss: 1.046133, acc: 99% // tar acc: 95% // adv loss: 0.408002, aux loss: 1.462140, tar loss: 0.908506\n",
      "=====================\n",
      "Epoch 54/200, Batch 298/938\n",
      "D loss: 0.997240, acc: 99% // tar acc: 93% // adv loss: 0.445233, aux loss: 1.462782, tar loss: 0.826739\n",
      "=====================\n",
      "Epoch 54/200, Batch 348/938\n",
      "D loss: 1.060199, acc: 98% // tar acc: 90% // adv loss: 0.356364, aux loss: 1.462883, tar loss: 0.837044\n",
      "=====================\n",
      "Epoch 54/200, Batch 398/938\n",
      "D loss: 1.085747, acc: 99% // tar acc: 98% // adv loss: 0.502872, aux loss: 1.462009, tar loss: 0.925105\n",
      "=====================\n",
      "Epoch 54/200, Batch 448/938\n",
      "D loss: 1.068292, acc: 99% // tar acc: 96% // adv loss: 0.569849, aux loss: 1.500839, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 54/200, Batch 498/938\n",
      "D loss: 1.044877, acc: 99% // tar acc: 93% // adv loss: 0.505113, aux loss: 1.475396, tar loss: 0.897963\n",
      "=====================\n",
      "Epoch 54/200, Batch 548/938\n",
      "D loss: 1.066889, acc: 98% // tar acc: 96% // adv loss: 0.359300, aux loss: 1.464082, tar loss: 0.882810\n",
      "=====================\n",
      "Epoch 54/200, Batch 598/938\n",
      "D loss: 1.163955, acc: 99% // tar acc: 98% // adv loss: 0.428708, aux loss: 1.480551, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 54/200, Batch 648/938\n",
      "D loss: 1.049550, acc: 99% // tar acc: 95% // adv loss: 0.666780, aux loss: 1.481914, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 54/200, Batch 698/938\n",
      "D loss: 1.110589, acc: 99% // tar acc: 96% // adv loss: 0.356426, aux loss: 1.461515, tar loss: 0.846166\n",
      "=====================\n",
      "Epoch 54/200, Batch 748/938\n",
      "D loss: 1.057371, acc: 96% // tar acc: 95% // adv loss: 0.355733, aux loss: 1.463843, tar loss: 0.839359\n",
      "=====================\n",
      "Epoch 54/200, Batch 798/938\n",
      "D loss: 1.047888, acc: 97% // tar acc: 93% // adv loss: 0.370364, aux loss: 1.472636, tar loss: 0.846305\n",
      "=====================\n",
      "Epoch 54/200, Batch 848/938\n",
      "D loss: 1.013513, acc: 98% // tar acc: 100% // adv loss: 0.431846, aux loss: 1.463718, tar loss: 0.882125\n",
      "=====================\n",
      "Epoch 54/200, Batch 898/938\n",
      "D loss: 1.107750, acc: 96% // tar acc: 96% // adv loss: 0.408067, aux loss: 1.461526, tar loss: 0.845812\n",
      "=====================\n",
      "Epoch 55/200, Batch 10/938\n",
      "D loss: 1.135764, acc: 98% // tar acc: 93% // adv loss: 0.650221, aux loss: 1.462913, tar loss: 0.858153\n",
      "=====================\n",
      "Epoch 55/200, Batch 60/938\n",
      "D loss: 1.117643, acc: 96% // tar acc: 96% // adv loss: 0.460731, aux loss: 1.461593, tar loss: 0.840168\n",
      "=====================\n",
      "Epoch 55/200, Batch 110/938\n",
      "D loss: 1.063806, acc: 96% // tar acc: 92% // adv loss: 0.594184, aux loss: 1.481408, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 55/200, Batch 160/938\n",
      "D loss: 1.061369, acc: 96% // tar acc: 92% // adv loss: 0.711324, aux loss: 1.500451, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 55/200, Batch 210/938\n",
      "D loss: 1.064561, acc: 98% // tar acc: 92% // adv loss: 0.460125, aux loss: 1.462763, tar loss: 0.859277\n",
      "=====================\n",
      "Epoch 55/200, Batch 260/938\n",
      "D loss: 1.115246, acc: 98% // tar acc: 92% // adv loss: 0.587587, aux loss: 1.464640, tar loss: 0.882694\n",
      "=====================\n",
      "Epoch 55/200, Batch 310/938\n",
      "D loss: 1.058119, acc: 96% // tar acc: 95% // adv loss: 0.561874, aux loss: 1.470416, tar loss: 0.810500\n",
      "=====================\n",
      "Epoch 55/200, Batch 360/938\n",
      "D loss: 1.078418, acc: 96% // tar acc: 95% // adv loss: 0.590910, aux loss: 1.477690, tar loss: 0.839393\n",
      "=====================\n",
      "Epoch 55/200, Batch 410/938\n",
      "D loss: 0.996637, acc: 98% // tar acc: 98% // adv loss: 0.456405, aux loss: 1.461336, tar loss: 0.849215\n",
      "=====================\n",
      "Epoch 55/200, Batch 460/938\n",
      "D loss: 1.081278, acc: 99% // tar acc: 95% // adv loss: 0.496153, aux loss: 1.482063, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 55/200, Batch 510/938\n",
      "D loss: 1.015486, acc: 99% // tar acc: 96% // adv loss: 0.752673, aux loss: 1.461358, tar loss: 0.801873\n",
      "=====================\n",
      "Epoch 55/200, Batch 560/938\n",
      "D loss: 1.034764, acc: 96% // tar acc: 95% // adv loss: 0.363429, aux loss: 1.461534, tar loss: 0.800554\n",
      "=====================\n",
      "Epoch 55/200, Batch 610/938\n",
      "D loss: 1.036268, acc: 99% // tar acc: 95% // adv loss: 0.395005, aux loss: 1.462218, tar loss: 0.904515\n",
      "=====================\n",
      "Epoch 55/200, Batch 660/938\n",
      "D loss: 1.056188, acc: 97% // tar acc: 98% // adv loss: 0.530860, aux loss: 1.461323, tar loss: 0.874268\n",
      "=====================\n",
      "Epoch 55/200, Batch 710/938\n",
      "D loss: 1.004403, acc: 99% // tar acc: 96% // adv loss: 0.558621, aux loss: 1.462315, tar loss: 0.886644\n",
      "=====================\n",
      "Epoch 55/200, Batch 760/938\n",
      "D loss: 1.070989, acc: 96% // tar acc: 95% // adv loss: 0.536676, aux loss: 1.484851, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 55/200, Batch 810/938\n",
      "D loss: 1.030490, acc: 96% // tar acc: 98% // adv loss: 0.653561, aux loss: 1.474345, tar loss: 0.893630\n",
      "=====================\n",
      "Epoch 55/200, Batch 860/938\n",
      "D loss: 1.055332, acc: 99% // tar acc: 95% // adv loss: 0.496667, aux loss: 1.473161, tar loss: 0.838744\n",
      "=====================\n",
      "Epoch 55/200, Batch 910/938\n",
      "D loss: 1.026686, acc: 99% // tar acc: 93% // adv loss: 0.484045, aux loss: 1.475136, tar loss: 0.833141\n",
      "=====================\n",
      "Epoch 56/200, Batch 22/938\n",
      "D loss: 1.061176, acc: 97% // tar acc: 90% // adv loss: 0.419648, aux loss: 1.462011, tar loss: 0.878751\n",
      "=====================\n",
      "Epoch 56/200, Batch 72/938\n",
      "D loss: 1.030217, acc: 97% // tar acc: 96% // adv loss: 0.626400, aux loss: 1.488118, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 56/200, Batch 122/938\n",
      "D loss: 1.080092, acc: 99% // tar acc: 93% // adv loss: 0.577742, aux loss: 1.461275, tar loss: 0.839287\n",
      "=====================\n",
      "Epoch 56/200, Batch 172/938\n",
      "D loss: 1.058270, acc: 98% // tar acc: 92% // adv loss: 0.515521, aux loss: 1.472598, tar loss: 0.915574\n",
      "=====================\n",
      "Epoch 56/200, Batch 222/938\n",
      "D loss: 1.029663, acc: 97% // tar acc: 95% // adv loss: 0.361827, aux loss: 1.462157, tar loss: 0.855876\n",
      "=====================\n",
      "Epoch 56/200, Batch 272/938\n",
      "D loss: 1.003741, acc: 95% // tar acc: 92% // adv loss: 0.432253, aux loss: 1.489098, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 56/200, Batch 322/938\n",
      "D loss: 1.051031, acc: 96% // tar acc: 95% // adv loss: 0.619815, aux loss: 1.462422, tar loss: 0.863837\n",
      "=====================\n",
      "Epoch 56/200, Batch 372/938\n",
      "D loss: 1.075711, acc: 96% // tar acc: 98% // adv loss: 0.523074, aux loss: 1.468489, tar loss: 0.876658\n",
      "=====================\n",
      "Epoch 56/200, Batch 422/938\n",
      "D loss: 1.054814, acc: 98% // tar acc: 98% // adv loss: 0.486023, aux loss: 1.461218, tar loss: 0.874173\n",
      "=====================\n",
      "Epoch 56/200, Batch 472/938\n",
      "D loss: 1.077072, acc: 96% // tar acc: 93% // adv loss: 0.560633, aux loss: 1.461277, tar loss: 0.880978\n",
      "=====================\n",
      "Epoch 56/200, Batch 522/938\n",
      "D loss: 1.038009, acc: 98% // tar acc: 98% // adv loss: 0.550457, aux loss: 1.470709, tar loss: 0.881753\n",
      "=====================\n",
      "Epoch 56/200, Batch 572/938\n",
      "D loss: 1.060670, acc: 100% // tar acc: 95% // adv loss: 0.422614, aux loss: 1.461249, tar loss: 0.853478\n",
      "=====================\n",
      "Epoch 56/200, Batch 622/938\n",
      "D loss: 1.070076, acc: 95% // tar acc: 93% // adv loss: 0.728443, aux loss: 1.463557, tar loss: 0.780557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 56/200, Batch 672/938\n",
      "D loss: 1.070868, acc: 98% // tar acc: 92% // adv loss: 0.485688, aux loss: 1.462558, tar loss: 0.861767\n",
      "=====================\n",
      "Epoch 56/200, Batch 722/938\n",
      "D loss: 0.997996, acc: 96% // tar acc: 95% // adv loss: 0.578269, aux loss: 1.471337, tar loss: 0.805642\n",
      "=====================\n",
      "Epoch 56/200, Batch 772/938\n",
      "D loss: 1.032096, acc: 97% // tar acc: 85% // adv loss: 0.416223, aux loss: 1.486241, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 56/200, Batch 822/938\n",
      "D loss: 1.008562, acc: 98% // tar acc: 96% // adv loss: 0.561437, aux loss: 1.461552, tar loss: 0.839718\n",
      "=====================\n",
      "Epoch 56/200, Batch 872/938\n",
      "D loss: 1.097554, acc: 96% // tar acc: 96% // adv loss: 0.394324, aux loss: 1.462538, tar loss: 0.863893\n",
      "=====================\n",
      "Epoch 56/200, Batch 922/938\n",
      "D loss: 1.088180, acc: 98% // tar acc: 96% // adv loss: 0.321780, aux loss: 1.462370, tar loss: 0.895854\n",
      "=====================\n",
      "Epoch 57/200, Batch 34/938\n",
      "D loss: 1.049971, acc: 97% // tar acc: 93% // adv loss: 0.480416, aux loss: 1.481862, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 57/200, Batch 84/938\n",
      "D loss: 1.008022, acc: 99% // tar acc: 85% // adv loss: 0.556051, aux loss: 1.469504, tar loss: 0.828796\n",
      "=====================\n",
      "Epoch 57/200, Batch 134/938\n",
      "D loss: 0.997819, acc: 98% // tar acc: 95% // adv loss: 0.507009, aux loss: 1.465911, tar loss: 0.843777\n",
      "=====================\n",
      "Epoch 57/200, Batch 184/938\n",
      "D loss: 1.076817, acc: 97% // tar acc: 96% // adv loss: 0.388911, aux loss: 1.474237, tar loss: 0.848385\n",
      "=====================\n",
      "Epoch 57/200, Batch 234/938\n",
      "D loss: 1.104906, acc: 97% // tar acc: 95% // adv loss: 0.627817, aux loss: 1.501199, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 57/200, Batch 284/938\n",
      "D loss: 1.031187, acc: 99% // tar acc: 95% // adv loss: 0.420822, aux loss: 1.462582, tar loss: 0.849443\n",
      "=====================\n",
      "Epoch 57/200, Batch 334/938\n",
      "D loss: 1.088995, acc: 96% // tar acc: 96% // adv loss: 0.330746, aux loss: 1.461989, tar loss: 0.844997\n",
      "=====================\n",
      "Epoch 57/200, Batch 384/938\n",
      "D loss: 1.043391, acc: 97% // tar acc: 93% // adv loss: 0.476222, aux loss: 1.473323, tar loss: 0.845741\n",
      "=====================\n",
      "Epoch 57/200, Batch 434/938\n",
      "D loss: 0.999085, acc: 96% // tar acc: 93% // adv loss: 0.520678, aux loss: 1.497415, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 57/200, Batch 484/938\n",
      "D loss: 1.061346, acc: 98% // tar acc: 96% // adv loss: 0.455738, aux loss: 1.470435, tar loss: 0.851621\n",
      "=====================\n",
      "Epoch 57/200, Batch 534/938\n",
      "D loss: 1.089406, acc: 96% // tar acc: 96% // adv loss: 0.463442, aux loss: 1.478002, tar loss: 0.845055\n",
      "=====================\n",
      "Epoch 57/200, Batch 584/938\n",
      "D loss: 1.060425, acc: 97% // tar acc: 96% // adv loss: 0.332300, aux loss: 1.490506, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 57/200, Batch 634/938\n",
      "D loss: 1.081964, acc: 96% // tar acc: 96% // adv loss: 0.572805, aux loss: 1.476951, tar loss: 0.891023\n",
      "=====================\n",
      "Epoch 57/200, Batch 684/938\n",
      "D loss: 1.084760, acc: 98% // tar acc: 95% // adv loss: 0.443748, aux loss: 1.481406, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 57/200, Batch 734/938\n",
      "D loss: 1.079374, acc: 96% // tar acc: 95% // adv loss: 0.593226, aux loss: 1.478091, tar loss: 0.867390\n",
      "=====================\n",
      "Epoch 57/200, Batch 784/938\n",
      "D loss: 1.078302, acc: 95% // tar acc: 95% // adv loss: 0.380865, aux loss: 1.470910, tar loss: 0.850666\n",
      "=====================\n",
      "Epoch 57/200, Batch 834/938\n",
      "D loss: 1.089430, acc: 98% // tar acc: 95% // adv loss: 0.280194, aux loss: 1.468136, tar loss: 0.862238\n",
      "=====================\n",
      "Epoch 57/200, Batch 884/938\n",
      "D loss: 1.004096, acc: 98% // tar acc: 98% // adv loss: 0.616121, aux loss: 1.470897, tar loss: 0.817411\n",
      "=====================\n",
      "Epoch 57/200, Batch 934/938\n",
      "D loss: 1.039095, acc: 98% // tar acc: 100% // adv loss: 0.405505, aux loss: 1.461536, tar loss: 0.880076\n",
      "=====================\n",
      "Epoch 58/200, Batch 46/938\n",
      "D loss: 1.069862, acc: 96% // tar acc: 95% // adv loss: 0.469263, aux loss: 1.463626, tar loss: 0.856733\n",
      "=====================\n",
      "Epoch 58/200, Batch 96/938\n",
      "D loss: 1.087060, acc: 98% // tar acc: 90% // adv loss: 0.587419, aux loss: 1.463692, tar loss: 0.861392\n",
      "=====================\n",
      "Epoch 58/200, Batch 146/938\n",
      "D loss: 1.073024, acc: 98% // tar acc: 95% // adv loss: 0.402970, aux loss: 1.461784, tar loss: 0.863135\n",
      "=====================\n",
      "Epoch 58/200, Batch 196/938\n",
      "D loss: 1.122579, acc: 97% // tar acc: 96% // adv loss: 0.406982, aux loss: 1.476772, tar loss: 0.859061\n",
      "=====================\n",
      "Epoch 58/200, Batch 246/938\n",
      "D loss: 1.049113, acc: 99% // tar acc: 98% // adv loss: 0.501245, aux loss: 1.474717, tar loss: 0.872437\n",
      "=====================\n",
      "Epoch 58/200, Batch 296/938\n",
      "D loss: 1.057071, acc: 98% // tar acc: 96% // adv loss: 0.374623, aux loss: 1.469256, tar loss: 0.864095\n",
      "=====================\n",
      "Epoch 58/200, Batch 346/938\n",
      "D loss: 1.052565, acc: 97% // tar acc: 96% // adv loss: 0.428211, aux loss: 1.471935, tar loss: 0.859263\n",
      "=====================\n",
      "Epoch 58/200, Batch 396/938\n",
      "D loss: 1.061782, acc: 99% // tar acc: 96% // adv loss: 0.362108, aux loss: 1.461504, tar loss: 0.865744\n",
      "=====================\n",
      "Epoch 58/200, Batch 446/938\n",
      "D loss: 1.071276, acc: 98% // tar acc: 95% // adv loss: 0.396448, aux loss: 1.461787, tar loss: 0.798992\n",
      "=====================\n",
      "Epoch 58/200, Batch 496/938\n",
      "D loss: 1.036825, acc: 96% // tar acc: 92% // adv loss: 0.574357, aux loss: 1.475104, tar loss: 0.825111\n",
      "=====================\n",
      "Epoch 58/200, Batch 546/938\n",
      "D loss: 1.048302, acc: 98% // tar acc: 98% // adv loss: 0.384637, aux loss: 1.471828, tar loss: 0.869198\n",
      "=====================\n",
      "Epoch 58/200, Batch 596/938\n",
      "D loss: 1.113125, acc: 94% // tar acc: 98% // adv loss: 0.394503, aux loss: 1.461877, tar loss: 0.877570\n",
      "=====================\n",
      "Epoch 58/200, Batch 646/938\n",
      "D loss: 0.982021, acc: 99% // tar acc: 100% // adv loss: 0.353657, aux loss: 1.461242, tar loss: 0.854144\n",
      "=====================\n",
      "Epoch 58/200, Batch 696/938\n",
      "D loss: 1.075984, acc: 93% // tar acc: 95% // adv loss: 0.544112, aux loss: 1.477374, tar loss: 0.887499\n",
      "=====================\n",
      "Epoch 58/200, Batch 746/938\n",
      "D loss: 0.957426, acc: 98% // tar acc: 96% // adv loss: 0.484843, aux loss: 1.463124, tar loss: 0.834141\n",
      "=====================\n",
      "Epoch 58/200, Batch 796/938\n",
      "D loss: 1.041489, acc: 99% // tar acc: 95% // adv loss: 0.602812, aux loss: 1.465158, tar loss: 0.848398\n",
      "=====================\n",
      "Epoch 58/200, Batch 846/938\n",
      "D loss: 1.049069, acc: 95% // tar acc: 98% // adv loss: 0.318667, aux loss: 1.474283, tar loss: 0.872760\n",
      "=====================\n",
      "Epoch 58/200, Batch 896/938\n",
      "D loss: 1.076701, acc: 98% // tar acc: 95% // adv loss: 0.448730, aux loss: 1.462494, tar loss: 0.884223\n",
      "=====================\n",
      "Epoch 59/200, Batch 8/938\n",
      "D loss: 0.988017, acc: 99% // tar acc: 95% // adv loss: 0.460024, aux loss: 1.472864, tar loss: 0.849609\n",
      "=====================\n",
      "Epoch 59/200, Batch 58/938\n",
      "D loss: 1.019146, acc: 93% // tar acc: 93% // adv loss: 0.440323, aux loss: 1.475312, tar loss: 0.803079\n",
      "=====================\n",
      "Epoch 59/200, Batch 108/938\n",
      "D loss: 0.987076, acc: 97% // tar acc: 95% // adv loss: 0.499738, aux loss: 1.476736, tar loss: 0.838322\n",
      "=====================\n",
      "Epoch 59/200, Batch 158/938\n",
      "D loss: 1.065109, acc: 98% // tar acc: 93% // adv loss: 0.596920, aux loss: 1.478997, tar loss: 0.868578\n",
      "=====================\n",
      "Epoch 59/200, Batch 208/938\n",
      "D loss: 1.050049, acc: 98% // tar acc: 93% // adv loss: 0.495533, aux loss: 1.485324, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 59/200, Batch 258/938\n",
      "D loss: 1.027027, acc: 97% // tar acc: 98% // adv loss: 0.380080, aux loss: 1.467060, tar loss: 0.896286\n",
      "=====================\n",
      "Epoch 59/200, Batch 308/938\n",
      "D loss: 1.028285, acc: 96% // tar acc: 95% // adv loss: 0.573498, aux loss: 1.476249, tar loss: 0.827354\n",
      "=====================\n",
      "Epoch 59/200, Batch 358/938\n",
      "D loss: 1.067022, acc: 98% // tar acc: 100% // adv loss: 0.416248, aux loss: 1.461374, tar loss: 0.886732\n",
      "=====================\n",
      "Epoch 59/200, Batch 408/938\n",
      "D loss: 0.998916, acc: 99% // tar acc: 96% // adv loss: 0.531986, aux loss: 1.469239, tar loss: 0.869949\n",
      "=====================\n",
      "Epoch 59/200, Batch 458/938\n",
      "D loss: 1.012605, acc: 95% // tar acc: 89% // adv loss: 0.531451, aux loss: 1.503209, tar loss: 1.332000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 59/200, Batch 508/938\n",
      "D loss: 1.017788, acc: 97% // tar acc: 96% // adv loss: 0.338603, aux loss: 1.463360, tar loss: 0.854179\n",
      "=====================\n",
      "Epoch 59/200, Batch 558/938\n",
      "D loss: 1.018388, acc: 99% // tar acc: 93% // adv loss: 0.357163, aux loss: 1.476788, tar loss: 0.861165\n",
      "=====================\n",
      "Epoch 59/200, Batch 608/938\n",
      "D loss: 1.076711, acc: 96% // tar acc: 96% // adv loss: 0.472289, aux loss: 1.475933, tar loss: 0.830215\n",
      "=====================\n",
      "Epoch 59/200, Batch 658/938\n",
      "D loss: 1.076718, acc: 98% // tar acc: 96% // adv loss: 0.401618, aux loss: 1.476395, tar loss: 0.870418\n",
      "=====================\n",
      "Epoch 59/200, Batch 708/938\n",
      "D loss: 1.055079, acc: 100% // tar acc: 95% // adv loss: 0.402246, aux loss: 1.461601, tar loss: 0.841788\n",
      "=====================\n",
      "Epoch 59/200, Batch 758/938\n",
      "D loss: 1.092692, acc: 97% // tar acc: 96% // adv loss: 0.362560, aux loss: 1.465618, tar loss: 0.878502\n",
      "=====================\n",
      "Epoch 59/200, Batch 808/938\n",
      "D loss: 1.086115, acc: 96% // tar acc: 93% // adv loss: 0.429129, aux loss: 1.462554, tar loss: 0.862512\n",
      "=====================\n",
      "Epoch 59/200, Batch 858/938\n",
      "D loss: 1.068580, acc: 99% // tar acc: 95% // adv loss: 0.755430, aux loss: 1.477938, tar loss: 0.817388\n",
      "=====================\n",
      "Epoch 59/200, Batch 908/938\n",
      "D loss: 1.040302, acc: 98% // tar acc: 96% // adv loss: 0.508264, aux loss: 1.473170, tar loss: 0.873060\n",
      "=====================\n",
      "Epoch 60/200, Batch 20/938\n",
      "D loss: 1.026665, acc: 99% // tar acc: 98% // adv loss: 0.544143, aux loss: 1.473204, tar loss: 0.881736\n",
      "=====================\n",
      "Epoch 60/200, Batch 70/938\n",
      "D loss: 1.094063, acc: 95% // tar acc: 95% // adv loss: 0.535608, aux loss: 1.461822, tar loss: 0.838490\n",
      "=====================\n",
      "Epoch 60/200, Batch 120/938\n",
      "D loss: 1.069482, acc: 99% // tar acc: 96% // adv loss: 0.526351, aux loss: 1.462918, tar loss: 0.802997\n",
      "=====================\n",
      "Epoch 60/200, Batch 170/938\n",
      "D loss: 1.015025, acc: 98% // tar acc: 100% // adv loss: 0.512726, aux loss: 1.464589, tar loss: 0.867797\n",
      "=====================\n",
      "Epoch 60/200, Batch 220/938\n",
      "D loss: 1.069210, acc: 96% // tar acc: 98% // adv loss: 0.512599, aux loss: 1.464029, tar loss: 0.869955\n",
      "=====================\n",
      "Epoch 60/200, Batch 270/938\n",
      "D loss: 1.104091, acc: 98% // tar acc: 98% // adv loss: 0.494719, aux loss: 1.470531, tar loss: 0.891673\n",
      "=====================\n",
      "Epoch 60/200, Batch 320/938\n",
      "D loss: 1.010782, acc: 99% // tar acc: 92% // adv loss: 0.487338, aux loss: 1.461217, tar loss: 0.882974\n",
      "=====================\n",
      "Epoch 60/200, Batch 370/938\n",
      "D loss: 1.050540, acc: 99% // tar acc: 98% // adv loss: 0.277772, aux loss: 1.461386, tar loss: 0.938695\n",
      "=====================\n",
      "Epoch 60/200, Batch 420/938\n",
      "D loss: 1.010143, acc: 96% // tar acc: 96% // adv loss: 0.280324, aux loss: 1.461192, tar loss: 0.842647\n",
      "=====================\n",
      "Epoch 60/200, Batch 470/938\n",
      "D loss: 1.080401, acc: 95% // tar acc: 95% // adv loss: 0.450324, aux loss: 1.464482, tar loss: 0.875445\n",
      "=====================\n",
      "Epoch 60/200, Batch 520/938\n",
      "D loss: 1.073317, acc: 99% // tar acc: 100% // adv loss: 0.520959, aux loss: 1.461314, tar loss: 0.890575\n",
      "=====================\n",
      "Epoch 60/200, Batch 570/938\n",
      "D loss: 1.037480, acc: 98% // tar acc: 96% // adv loss: 0.318030, aux loss: 1.484976, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 60/200, Batch 620/938\n",
      "D loss: 0.949659, acc: 100% // tar acc: 93% // adv loss: 0.497777, aux loss: 1.481843, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 60/200, Batch 670/938\n",
      "D loss: 1.045736, acc: 99% // tar acc: 98% // adv loss: 0.424583, aux loss: 1.462771, tar loss: 0.890161\n",
      "=====================\n",
      "Epoch 60/200, Batch 720/938\n",
      "D loss: 1.012183, acc: 99% // tar acc: 98% // adv loss: 0.330633, aux loss: 1.462097, tar loss: 0.910168\n",
      "=====================\n",
      "Epoch 60/200, Batch 770/938\n",
      "D loss: 1.007383, acc: 96% // tar acc: 93% // adv loss: 0.377949, aux loss: 1.462019, tar loss: 0.847920\n",
      "=====================\n",
      "Epoch 60/200, Batch 820/938\n",
      "D loss: 1.021232, acc: 99% // tar acc: 98% // adv loss: 0.433535, aux loss: 1.474548, tar loss: 0.895366\n",
      "=====================\n",
      "Epoch 60/200, Batch 870/938\n",
      "D loss: 1.109743, acc: 96% // tar acc: 98% // adv loss: 0.446845, aux loss: 1.463439, tar loss: 0.835870\n",
      "=====================\n",
      "Epoch 60/200, Batch 920/938\n",
      "D loss: 1.099595, acc: 96% // tar acc: 95% // adv loss: 0.467901, aux loss: 1.465996, tar loss: 0.814438\n",
      "=====================\n",
      "Epoch 61/200, Batch 32/938\n",
      "D loss: 1.051453, acc: 97% // tar acc: 92% // adv loss: 0.581948, aux loss: 1.484413, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 61/200, Batch 82/938\n",
      "D loss: 1.047779, acc: 98% // tar acc: 96% // adv loss: 0.501831, aux loss: 1.476906, tar loss: 0.881587\n",
      "=====================\n",
      "Epoch 61/200, Batch 132/938\n",
      "D loss: 1.034335, acc: 98% // tar acc: 98% // adv loss: 0.467986, aux loss: 1.464357, tar loss: 0.812973\n",
      "=====================\n",
      "Epoch 61/200, Batch 182/938\n",
      "D loss: 1.012216, acc: 98% // tar acc: 100% // adv loss: 0.312528, aux loss: 1.461168, tar loss: 0.858463\n",
      "=====================\n",
      "Epoch 61/200, Batch 232/938\n",
      "D loss: 1.087626, acc: 96% // tar acc: 95% // adv loss: 0.477216, aux loss: 1.468258, tar loss: 0.848335\n",
      "=====================\n",
      "Epoch 61/200, Batch 282/938\n",
      "D loss: 1.085635, acc: 96% // tar acc: 90% // adv loss: 0.418376, aux loss: 1.461726, tar loss: 0.861171\n",
      "=====================\n",
      "Epoch 61/200, Batch 332/938\n",
      "D loss: 1.047608, acc: 98% // tar acc: 93% // adv loss: 0.601960, aux loss: 1.476347, tar loss: 0.841029\n",
      "=====================\n",
      "Epoch 61/200, Batch 382/938\n",
      "D loss: 1.072991, acc: 98% // tar acc: 98% // adv loss: 0.628249, aux loss: 1.461340, tar loss: 0.851196\n",
      "=====================\n",
      "Epoch 61/200, Batch 432/938\n",
      "D loss: 1.057166, acc: 98% // tar acc: 98% // adv loss: 0.372265, aux loss: 1.492473, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 61/200, Batch 482/938\n",
      "D loss: 0.993770, acc: 98% // tar acc: 92% // adv loss: 0.696081, aux loss: 1.473521, tar loss: 0.841162\n",
      "=====================\n",
      "Epoch 61/200, Batch 532/938\n",
      "D loss: 1.040346, acc: 99% // tar acc: 96% // adv loss: 0.517393, aux loss: 1.474362, tar loss: 0.856681\n",
      "=====================\n",
      "Epoch 61/200, Batch 582/938\n",
      "D loss: 1.088996, acc: 98% // tar acc: 98% // adv loss: 0.525851, aux loss: 1.461172, tar loss: 0.898721\n",
      "=====================\n",
      "Epoch 61/200, Batch 632/938\n",
      "D loss: 1.019339, acc: 98% // tar acc: 96% // adv loss: 0.460920, aux loss: 1.462317, tar loss: 0.846703\n",
      "=====================\n",
      "Epoch 61/200, Batch 682/938\n",
      "D loss: 1.047925, acc: 98% // tar acc: 96% // adv loss: 0.771865, aux loss: 1.491287, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 61/200, Batch 732/938\n",
      "D loss: 1.124894, acc: 96% // tar acc: 95% // adv loss: 0.427244, aux loss: 1.488212, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 61/200, Batch 782/938\n",
      "D loss: 1.030616, acc: 98% // tar acc: 93% // adv loss: 0.596021, aux loss: 1.506045, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 61/200, Batch 832/938\n",
      "D loss: 1.050910, acc: 96% // tar acc: 96% // adv loss: 0.439241, aux loss: 1.476612, tar loss: 0.872198\n",
      "=====================\n",
      "Epoch 61/200, Batch 882/938\n",
      "D loss: 1.088433, acc: 100% // tar acc: 96% // adv loss: 0.488478, aux loss: 1.476981, tar loss: 0.841496\n",
      "=====================\n",
      "Epoch 61/200, Batch 932/938\n",
      "D loss: 1.025664, acc: 95% // tar acc: 98% // adv loss: 0.674682, aux loss: 1.465600, tar loss: 0.882579\n",
      "=====================\n",
      "Epoch 62/200, Batch 44/938\n",
      "D loss: 1.066233, acc: 100% // tar acc: 96% // adv loss: 0.442469, aux loss: 1.461416, tar loss: 0.857195\n",
      "=====================\n",
      "Epoch 62/200, Batch 94/938\n",
      "D loss: 1.017800, acc: 100% // tar acc: 95% // adv loss: 0.661795, aux loss: 1.486931, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 62/200, Batch 144/938\n",
      "D loss: 1.086894, acc: 99% // tar acc: 96% // adv loss: 0.569628, aux loss: 1.476360, tar loss: 0.823501\n",
      "=====================\n",
      "Epoch 62/200, Batch 194/938\n",
      "D loss: 1.054511, acc: 98% // tar acc: 96% // adv loss: 0.517297, aux loss: 1.461214, tar loss: 0.905628\n",
      "=====================\n",
      "Epoch 62/200, Batch 244/938\n",
      "D loss: 1.068663, acc: 96% // tar acc: 95% // adv loss: 0.773074, aux loss: 1.462233, tar loss: 0.844010\n",
      "=====================\n",
      "Epoch 62/200, Batch 294/938\n",
      "D loss: 1.136245, acc: 97% // tar acc: 96% // adv loss: 0.539001, aux loss: 1.489751, tar loss: 1.332000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 62/200, Batch 344/938\n",
      "D loss: 1.109707, acc: 99% // tar acc: 96% // adv loss: 0.493214, aux loss: 1.477697, tar loss: 0.910482\n",
      "=====================\n",
      "Epoch 62/200, Batch 394/938\n",
      "D loss: 1.098162, acc: 94% // tar acc: 96% // adv loss: 0.479449, aux loss: 1.476961, tar loss: 0.874491\n",
      "=====================\n",
      "Epoch 62/200, Batch 444/938\n",
      "D loss: 1.015384, acc: 100% // tar acc: 93% // adv loss: 0.481282, aux loss: 1.461480, tar loss: 0.842616\n",
      "=====================\n",
      "Epoch 62/200, Batch 494/938\n",
      "D loss: 1.066285, acc: 99% // tar acc: 98% // adv loss: 0.365024, aux loss: 1.468422, tar loss: 0.850172\n",
      "=====================\n",
      "Epoch 62/200, Batch 544/938\n",
      "D loss: 1.092874, acc: 96% // tar acc: 100% // adv loss: 0.543787, aux loss: 1.480378, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 62/200, Batch 594/938\n",
      "D loss: 1.045190, acc: 100% // tar acc: 95% // adv loss: 0.776575, aux loss: 1.470774, tar loss: 0.856295\n",
      "=====================\n",
      "Epoch 62/200, Batch 644/938\n",
      "D loss: 1.047703, acc: 97% // tar acc: 98% // adv loss: 0.421267, aux loss: 1.473437, tar loss: 0.920197\n",
      "=====================\n",
      "Epoch 62/200, Batch 694/938\n",
      "D loss: 1.029380, acc: 98% // tar acc: 93% // adv loss: 0.420400, aux loss: 1.490035, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 62/200, Batch 744/938\n",
      "D loss: 1.063369, acc: 99% // tar acc: 90% // adv loss: 0.495448, aux loss: 1.493374, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 62/200, Batch 794/938\n",
      "D loss: 0.971485, acc: 97% // tar acc: 92% // adv loss: 0.647023, aux loss: 1.479981, tar loss: 0.835110\n",
      "=====================\n",
      "Epoch 62/200, Batch 844/938\n",
      "D loss: 1.007672, acc: 97% // tar acc: 93% // adv loss: 0.574403, aux loss: 1.461941, tar loss: 0.810557\n",
      "=====================\n",
      "Epoch 62/200, Batch 894/938\n",
      "D loss: 1.097138, acc: 98% // tar acc: 98% // adv loss: 0.380932, aux loss: 1.474788, tar loss: 0.846680\n",
      "=====================\n",
      "Epoch 63/200, Batch 6/938\n",
      "D loss: 1.125807, acc: 97% // tar acc: 95% // adv loss: 0.360050, aux loss: 1.475866, tar loss: 0.892655\n",
      "=====================\n",
      "Epoch 63/200, Batch 56/938\n",
      "D loss: 1.045772, acc: 97% // tar acc: 95% // adv loss: 0.512603, aux loss: 1.480259, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 63/200, Batch 106/938\n",
      "D loss: 1.035216, acc: 98% // tar acc: 98% // adv loss: 0.445438, aux loss: 1.472539, tar loss: 0.892463\n",
      "=====================\n",
      "Epoch 63/200, Batch 156/938\n",
      "D loss: 1.043075, acc: 97% // tar acc: 92% // adv loss: 0.442223, aux loss: 1.461200, tar loss: 0.816272\n",
      "=====================\n",
      "Epoch 63/200, Batch 206/938\n",
      "D loss: 1.108479, acc: 95% // tar acc: 93% // adv loss: 0.428360, aux loss: 1.482454, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 63/200, Batch 256/938\n",
      "D loss: 1.039873, acc: 100% // tar acc: 93% // adv loss: 0.390568, aux loss: 1.476076, tar loss: 0.840449\n",
      "=====================\n",
      "Epoch 63/200, Batch 306/938\n",
      "D loss: 0.951277, acc: 99% // tar acc: 90% // adv loss: 0.517983, aux loss: 1.477560, tar loss: 0.788678\n",
      "=====================\n",
      "Epoch 63/200, Batch 356/938\n",
      "D loss: 1.004825, acc: 100% // tar acc: 90% // adv loss: 0.427705, aux loss: 1.474283, tar loss: 0.835656\n",
      "=====================\n",
      "Epoch 63/200, Batch 406/938\n",
      "D loss: 1.072669, acc: 99% // tar acc: 98% // adv loss: 0.639043, aux loss: 1.462215, tar loss: 0.841898\n",
      "=====================\n",
      "Epoch 63/200, Batch 456/938\n",
      "D loss: 1.052757, acc: 100% // tar acc: 95% // adv loss: 0.442874, aux loss: 1.468224, tar loss: 0.904884\n",
      "=====================\n",
      "Epoch 63/200, Batch 506/938\n",
      "D loss: 1.048970, acc: 99% // tar acc: 100% // adv loss: 0.479965, aux loss: 1.462569, tar loss: 0.880057\n",
      "=====================\n",
      "Epoch 63/200, Batch 556/938\n",
      "D loss: 1.051470, acc: 99% // tar acc: 90% // adv loss: 0.673504, aux loss: 1.462268, tar loss: 0.871641\n",
      "=====================\n",
      "Epoch 63/200, Batch 606/938\n",
      "D loss: 1.035447, acc: 99% // tar acc: 98% // adv loss: 0.429688, aux loss: 1.461284, tar loss: 0.904751\n",
      "=====================\n",
      "Epoch 63/200, Batch 656/938\n",
      "D loss: 1.066815, acc: 98% // tar acc: 96% // adv loss: 0.342317, aux loss: 1.461202, tar loss: 0.864890\n",
      "=====================\n",
      "Epoch 63/200, Batch 706/938\n",
      "D loss: 1.054841, acc: 96% // tar acc: 93% // adv loss: 0.416292, aux loss: 1.480591, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 63/200, Batch 756/938\n",
      "D loss: 1.000044, acc: 99% // tar acc: 87% // adv loss: 0.375655, aux loss: 1.477537, tar loss: 0.822037\n",
      "=====================\n",
      "Epoch 63/200, Batch 806/938\n",
      "D loss: 1.062169, acc: 96% // tar acc: 96% // adv loss: 0.583459, aux loss: 1.481738, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 63/200, Batch 856/938\n",
      "D loss: 1.131173, acc: 96% // tar acc: 96% // adv loss: 0.435500, aux loss: 1.477099, tar loss: 0.867248\n",
      "=====================\n",
      "Epoch 63/200, Batch 906/938\n",
      "D loss: 1.111173, acc: 99% // tar acc: 95% // adv loss: 0.412270, aux loss: 1.465641, tar loss: 0.860833\n",
      "=====================\n",
      "Epoch 64/200, Batch 18/938\n",
      "D loss: 1.027717, acc: 97% // tar acc: 95% // adv loss: 0.493993, aux loss: 1.481965, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 64/200, Batch 68/938\n",
      "D loss: 1.056468, acc: 95% // tar acc: 96% // adv loss: 0.426245, aux loss: 1.464632, tar loss: 0.832380\n",
      "=====================\n",
      "Epoch 64/200, Batch 118/938\n",
      "D loss: 1.047291, acc: 97% // tar acc: 96% // adv loss: 0.374565, aux loss: 1.484898, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 64/200, Batch 168/938\n",
      "D loss: 1.076918, acc: 97% // tar acc: 96% // adv loss: 0.448973, aux loss: 1.465296, tar loss: 0.870993\n",
      "=====================\n",
      "Epoch 64/200, Batch 218/938\n",
      "D loss: 0.999377, acc: 98% // tar acc: 98% // adv loss: 0.401374, aux loss: 1.498085, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 64/200, Batch 268/938\n",
      "D loss: 1.100304, acc: 98% // tar acc: 92% // adv loss: 0.419338, aux loss: 1.477665, tar loss: 0.837087\n",
      "=====================\n",
      "Epoch 64/200, Batch 318/938\n",
      "D loss: 1.092407, acc: 96% // tar acc: 95% // adv loss: 0.539655, aux loss: 1.469374, tar loss: 0.866023\n",
      "=====================\n",
      "Epoch 64/200, Batch 368/938\n",
      "D loss: 1.040379, acc: 100% // tar acc: 98% // adv loss: 0.547589, aux loss: 1.476936, tar loss: 0.823626\n",
      "=====================\n",
      "Epoch 64/200, Batch 418/938\n",
      "D loss: 1.053023, acc: 98% // tar acc: 96% // adv loss: 0.586992, aux loss: 1.465010, tar loss: 0.855896\n",
      "=====================\n",
      "Epoch 64/200, Batch 468/938\n",
      "D loss: 1.088828, acc: 96% // tar acc: 95% // adv loss: 0.429399, aux loss: 1.471087, tar loss: 0.841224\n",
      "=====================\n",
      "Epoch 64/200, Batch 518/938\n",
      "D loss: 1.163021, acc: 96% // tar acc: 96% // adv loss: 0.588464, aux loss: 1.461428, tar loss: 0.854676\n",
      "=====================\n",
      "Epoch 64/200, Batch 568/938\n",
      "D loss: 1.015400, acc: 98% // tar acc: 93% // adv loss: 0.323198, aux loss: 1.461201, tar loss: 0.856184\n",
      "=====================\n",
      "Epoch 64/200, Batch 618/938\n",
      "D loss: 1.036734, acc: 97% // tar acc: 95% // adv loss: 0.375075, aux loss: 1.476159, tar loss: 0.860054\n",
      "=====================\n",
      "Epoch 64/200, Batch 668/938\n",
      "D loss: 1.111024, acc: 98% // tar acc: 95% // adv loss: 0.409576, aux loss: 1.461305, tar loss: 0.874016\n",
      "=====================\n",
      "Epoch 64/200, Batch 718/938\n",
      "D loss: 1.034720, acc: 97% // tar acc: 92% // adv loss: 0.671124, aux loss: 1.467120, tar loss: 0.837984\n",
      "=====================\n",
      "Epoch 64/200, Batch 768/938\n",
      "D loss: 1.103799, acc: 96% // tar acc: 93% // adv loss: 0.309716, aux loss: 1.478408, tar loss: 0.830065\n",
      "=====================\n",
      "Epoch 64/200, Batch 818/938\n",
      "D loss: 1.030379, acc: 98% // tar acc: 98% // adv loss: 0.371261, aux loss: 1.468233, tar loss: 0.852038\n",
      "=====================\n",
      "Epoch 64/200, Batch 868/938\n",
      "D loss: 0.972338, acc: 98% // tar acc: 96% // adv loss: 0.597598, aux loss: 1.461888, tar loss: 0.848082\n",
      "=====================\n",
      "Epoch 64/200, Batch 918/938\n",
      "D loss: 0.979621, acc: 100% // tar acc: 90% // adv loss: 0.641108, aux loss: 1.461966, tar loss: 0.845370\n",
      "=====================\n",
      "Epoch 65/200, Batch 30/938\n",
      "D loss: 1.002592, acc: 98% // tar acc: 95% // adv loss: 0.673901, aux loss: 1.464224, tar loss: 0.858119\n",
      "=====================\n",
      "Epoch 65/200, Batch 80/938\n",
      "D loss: 1.054341, acc: 98% // tar acc: 93% // adv loss: 0.326049, aux loss: 1.462872, tar loss: 0.910737\n",
      "=====================\n",
      "Epoch 65/200, Batch 130/938\n",
      "D loss: 1.067365, acc: 99% // tar acc: 95% // adv loss: 0.440870, aux loss: 1.476251, tar loss: 0.835492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 65/200, Batch 180/938\n",
      "D loss: 1.052936, acc: 97% // tar acc: 98% // adv loss: 0.330609, aux loss: 1.462356, tar loss: 0.861440\n",
      "=====================\n",
      "Epoch 65/200, Batch 230/938\n",
      "D loss: 1.125998, acc: 97% // tar acc: 98% // adv loss: 0.427660, aux loss: 1.461666, tar loss: 0.910653\n",
      "=====================\n",
      "Epoch 65/200, Batch 280/938\n",
      "D loss: 0.998605, acc: 98% // tar acc: 93% // adv loss: 0.583515, aux loss: 1.464720, tar loss: 0.845429\n",
      "=====================\n",
      "Epoch 65/200, Batch 330/938\n",
      "D loss: 1.013609, acc: 99% // tar acc: 96% // adv loss: 0.536215, aux loss: 1.463278, tar loss: 0.848243\n",
      "=====================\n",
      "Epoch 65/200, Batch 380/938\n",
      "D loss: 1.076328, acc: 98% // tar acc: 93% // adv loss: 0.623254, aux loss: 1.462239, tar loss: 0.847018\n",
      "=====================\n",
      "Epoch 65/200, Batch 430/938\n",
      "D loss: 1.026462, acc: 98% // tar acc: 98% // adv loss: 0.618010, aux loss: 1.473226, tar loss: 0.911842\n",
      "=====================\n",
      "Epoch 65/200, Batch 480/938\n",
      "D loss: 1.040092, acc: 98% // tar acc: 95% // adv loss: 0.442899, aux loss: 1.476596, tar loss: 0.854954\n",
      "=====================\n",
      "Epoch 65/200, Batch 530/938\n",
      "D loss: 1.018073, acc: 99% // tar acc: 95% // adv loss: 0.768469, aux loss: 1.461359, tar loss: 0.824044\n",
      "=====================\n",
      "Epoch 65/200, Batch 580/938\n",
      "D loss: 1.024997, acc: 98% // tar acc: 98% // adv loss: 0.338810, aux loss: 1.488056, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 65/200, Batch 630/938\n",
      "D loss: 1.015435, acc: 99% // tar acc: 95% // adv loss: 0.415716, aux loss: 1.471702, tar loss: 0.871827\n",
      "=====================\n",
      "Epoch 65/200, Batch 680/938\n",
      "D loss: 1.022525, acc: 98% // tar acc: 98% // adv loss: 0.257283, aux loss: 1.463612, tar loss: 0.872292\n",
      "=====================\n",
      "Epoch 65/200, Batch 730/938\n",
      "D loss: 1.004699, acc: 98% // tar acc: 96% // adv loss: 0.571557, aux loss: 1.461300, tar loss: 0.850050\n",
      "=====================\n",
      "Epoch 65/200, Batch 780/938\n",
      "D loss: 1.030881, acc: 98% // tar acc: 95% // adv loss: 0.321862, aux loss: 1.492105, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 65/200, Batch 830/938\n",
      "D loss: 1.070328, acc: 96% // tar acc: 95% // adv loss: 0.356226, aux loss: 1.462986, tar loss: 0.900740\n",
      "=====================\n",
      "Epoch 65/200, Batch 880/938\n",
      "D loss: 1.045938, acc: 99% // tar acc: 98% // adv loss: 0.345855, aux loss: 1.463632, tar loss: 0.908422\n",
      "=====================\n",
      "Epoch 65/200, Batch 930/938\n",
      "D loss: 1.082343, acc: 99% // tar acc: 95% // adv loss: 0.657560, aux loss: 1.463062, tar loss: 0.855997\n",
      "=====================\n",
      "Epoch 66/200, Batch 42/938\n",
      "D loss: 1.060328, acc: 100% // tar acc: 96% // adv loss: 0.607651, aux loss: 1.462668, tar loss: 0.878361\n",
      "=====================\n",
      "Epoch 66/200, Batch 92/938\n",
      "D loss: 1.066867, acc: 96% // tar acc: 95% // adv loss: 0.386844, aux loss: 1.467952, tar loss: 0.887833\n",
      "=====================\n",
      "Epoch 66/200, Batch 142/938\n",
      "D loss: 1.016113, acc: 98% // tar acc: 100% // adv loss: 0.695116, aux loss: 1.465539, tar loss: 0.868817\n",
      "=====================\n",
      "Epoch 66/200, Batch 192/938\n",
      "D loss: 1.081597, acc: 96% // tar acc: 92% // adv loss: 0.684898, aux loss: 1.461401, tar loss: 0.820117\n",
      "=====================\n",
      "Epoch 66/200, Batch 242/938\n",
      "D loss: 1.033958, acc: 99% // tar acc: 95% // adv loss: 0.361120, aux loss: 1.478733, tar loss: 0.894048\n",
      "=====================\n",
      "Epoch 66/200, Batch 292/938\n",
      "D loss: 1.153242, acc: 97% // tar acc: 95% // adv loss: 0.469979, aux loss: 1.468660, tar loss: 0.862926\n",
      "=====================\n",
      "Epoch 66/200, Batch 342/938\n",
      "D loss: 1.009976, acc: 97% // tar acc: 100% // adv loss: 0.347006, aux loss: 1.477598, tar loss: 0.817739\n",
      "=====================\n",
      "Epoch 66/200, Batch 392/938\n",
      "D loss: 1.113165, acc: 97% // tar acc: 92% // adv loss: 0.399288, aux loss: 1.482501, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 66/200, Batch 442/938\n",
      "D loss: 1.080733, acc: 98% // tar acc: 89% // adv loss: 0.507846, aux loss: 1.493700, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 66/200, Batch 492/938\n",
      "D loss: 1.099441, acc: 98% // tar acc: 90% // adv loss: 0.373958, aux loss: 1.461854, tar loss: 0.850065\n",
      "=====================\n",
      "Epoch 66/200, Batch 542/938\n",
      "D loss: 1.055346, acc: 98% // tar acc: 95% // adv loss: 0.548147, aux loss: 1.474267, tar loss: 0.883187\n",
      "=====================\n",
      "Epoch 66/200, Batch 592/938\n",
      "D loss: 1.029990, acc: 97% // tar acc: 96% // adv loss: 0.458033, aux loss: 1.479161, tar loss: 0.850572\n",
      "=====================\n",
      "Epoch 66/200, Batch 642/938\n",
      "D loss: 1.102699, acc: 97% // tar acc: 96% // adv loss: 0.555974, aux loss: 1.468170, tar loss: 0.866769\n",
      "=====================\n",
      "Epoch 66/200, Batch 692/938\n",
      "D loss: 1.063167, acc: 98% // tar acc: 96% // adv loss: 0.598369, aux loss: 1.480598, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 66/200, Batch 742/938\n",
      "D loss: 1.067948, acc: 98% // tar acc: 98% // adv loss: 0.528264, aux loss: 1.473405, tar loss: 0.839818\n",
      "=====================\n",
      "Epoch 66/200, Batch 792/938\n",
      "D loss: 1.059858, acc: 98% // tar acc: 92% // adv loss: 0.484809, aux loss: 1.470436, tar loss: 0.898167\n",
      "=====================\n",
      "Epoch 66/200, Batch 842/938\n",
      "D loss: 1.107317, acc: 96% // tar acc: 96% // adv loss: 0.442008, aux loss: 1.463966, tar loss: 0.864824\n",
      "=====================\n",
      "Epoch 66/200, Batch 892/938\n",
      "D loss: 1.018225, acc: 98% // tar acc: 98% // adv loss: 0.444098, aux loss: 1.463037, tar loss: 0.870054\n",
      "=====================\n",
      "Epoch 67/200, Batch 4/938\n",
      "D loss: 1.005211, acc: 100% // tar acc: 98% // adv loss: 0.481715, aux loss: 1.461798, tar loss: 0.870021\n",
      "=====================\n",
      "Epoch 67/200, Batch 54/938\n",
      "D loss: 0.969242, acc: 99% // tar acc: 96% // adv loss: 0.349995, aux loss: 1.474790, tar loss: 0.825721\n",
      "=====================\n",
      "Epoch 67/200, Batch 104/938\n",
      "D loss: 0.997001, acc: 99% // tar acc: 98% // adv loss: 0.491226, aux loss: 1.472740, tar loss: 0.857674\n",
      "=====================\n",
      "Epoch 67/200, Batch 154/938\n",
      "D loss: 1.025231, acc: 96% // tar acc: 95% // adv loss: 0.417997, aux loss: 1.466838, tar loss: 0.823726\n",
      "=====================\n",
      "Epoch 67/200, Batch 204/938\n",
      "D loss: 1.065736, acc: 98% // tar acc: 98% // adv loss: 0.450753, aux loss: 1.474945, tar loss: 0.857581\n",
      "=====================\n",
      "Epoch 67/200, Batch 254/938\n",
      "D loss: 1.119947, acc: 98% // tar acc: 98% // adv loss: 0.345491, aux loss: 1.462905, tar loss: 0.867657\n",
      "=====================\n",
      "Epoch 67/200, Batch 304/938\n",
      "D loss: 1.098069, acc: 96% // tar acc: 96% // adv loss: 0.539586, aux loss: 1.476880, tar loss: 0.838762\n",
      "=====================\n",
      "Epoch 67/200, Batch 354/938\n",
      "D loss: 1.071016, acc: 99% // tar acc: 98% // adv loss: 0.515282, aux loss: 1.462434, tar loss: 0.882935\n",
      "=====================\n",
      "Epoch 67/200, Batch 404/938\n",
      "D loss: 1.125260, acc: 98% // tar acc: 96% // adv loss: 0.684173, aux loss: 1.472448, tar loss: 0.834393\n",
      "=====================\n",
      "Epoch 67/200, Batch 454/938\n",
      "D loss: 1.047287, acc: 99% // tar acc: 96% // adv loss: 0.333004, aux loss: 1.484722, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 67/200, Batch 504/938\n",
      "D loss: 1.138968, acc: 99% // tar acc: 98% // adv loss: 0.426633, aux loss: 1.462260, tar loss: 0.846966\n",
      "=====================\n",
      "Epoch 67/200, Batch 554/938\n",
      "D loss: 1.062972, acc: 97% // tar acc: 95% // adv loss: 0.362709, aux loss: 1.495266, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 67/200, Batch 604/938\n",
      "D loss: 1.111251, acc: 96% // tar acc: 95% // adv loss: 0.446372, aux loss: 1.462097, tar loss: 0.852596\n",
      "=====================\n",
      "Epoch 67/200, Batch 654/938\n",
      "D loss: 1.092443, acc: 97% // tar acc: 92% // adv loss: 0.801918, aux loss: 1.482061, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 67/200, Batch 704/938\n",
      "D loss: 1.069982, acc: 97% // tar acc: 95% // adv loss: 0.465223, aux loss: 1.461643, tar loss: 0.853622\n",
      "=====================\n",
      "Epoch 67/200, Batch 754/938\n",
      "D loss: 0.993099, acc: 97% // tar acc: 95% // adv loss: 0.483231, aux loss: 1.462835, tar loss: 0.832667\n",
      "=====================\n",
      "Epoch 67/200, Batch 804/938\n",
      "D loss: 1.098150, acc: 95% // tar acc: 93% // adv loss: 0.507287, aux loss: 1.468653, tar loss: 0.878453\n",
      "=====================\n",
      "Epoch 67/200, Batch 854/938\n",
      "D loss: 1.162548, acc: 96% // tar acc: 90% // adv loss: 0.334573, aux loss: 1.473251, tar loss: 0.861500\n",
      "=====================\n",
      "Epoch 67/200, Batch 904/938\n",
      "D loss: 1.133448, acc: 100% // tar acc: 96% // adv loss: 0.389038, aux loss: 1.466025, tar loss: 0.841356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 68/200, Batch 16/938\n",
      "D loss: 1.117809, acc: 99% // tar acc: 96% // adv loss: 0.348480, aux loss: 1.466431, tar loss: 0.893546\n",
      "=====================\n",
      "Epoch 68/200, Batch 66/938\n",
      "D loss: 1.021270, acc: 99% // tar acc: 98% // adv loss: 0.566518, aux loss: 1.461273, tar loss: 0.887887\n",
      "=====================\n",
      "Epoch 68/200, Batch 116/938\n",
      "D loss: 1.002058, acc: 98% // tar acc: 98% // adv loss: 0.439315, aux loss: 1.461284, tar loss: 0.874269\n",
      "=====================\n",
      "Epoch 68/200, Batch 166/938\n",
      "D loss: 1.037395, acc: 98% // tar acc: 98% // adv loss: 0.542620, aux loss: 1.461189, tar loss: 0.883946\n",
      "=====================\n",
      "Epoch 68/200, Batch 216/938\n",
      "D loss: 1.069922, acc: 97% // tar acc: 96% // adv loss: 0.550445, aux loss: 1.465363, tar loss: 0.868492\n",
      "=====================\n",
      "Epoch 68/200, Batch 266/938\n",
      "D loss: 1.004147, acc: 97% // tar acc: 98% // adv loss: 0.421829, aux loss: 1.483790, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 68/200, Batch 316/938\n",
      "D loss: 1.049241, acc: 95% // tar acc: 95% // adv loss: 0.470230, aux loss: 1.476292, tar loss: 0.814221\n",
      "=====================\n",
      "Epoch 68/200, Batch 366/938\n",
      "D loss: 1.146490, acc: 99% // tar acc: 93% // adv loss: 0.561884, aux loss: 1.464667, tar loss: 0.826136\n",
      "=====================\n",
      "Epoch 68/200, Batch 416/938\n",
      "D loss: 1.069921, acc: 98% // tar acc: 93% // adv loss: 0.481076, aux loss: 1.461312, tar loss: 0.845857\n",
      "=====================\n",
      "Epoch 68/200, Batch 466/938\n",
      "D loss: 0.981721, acc: 99% // tar acc: 95% // adv loss: 0.392143, aux loss: 1.465044, tar loss: 0.857303\n",
      "=====================\n",
      "Epoch 68/200, Batch 516/938\n",
      "D loss: 1.077720, acc: 97% // tar acc: 96% // adv loss: 0.597640, aux loss: 1.519227, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 68/200, Batch 566/938\n",
      "D loss: 0.985593, acc: 96% // tar acc: 89% // adv loss: 0.467682, aux loss: 1.477227, tar loss: 0.807937\n",
      "=====================\n",
      "Epoch 68/200, Batch 616/938\n",
      "D loss: 1.075831, acc: 97% // tar acc: 92% // adv loss: 0.566440, aux loss: 1.466939, tar loss: 0.826702\n",
      "=====================\n",
      "Epoch 68/200, Batch 666/938\n",
      "D loss: 0.986481, acc: 97% // tar acc: 92% // adv loss: 0.445021, aux loss: 1.472335, tar loss: 0.818629\n",
      "=====================\n",
      "Epoch 68/200, Batch 716/938\n",
      "D loss: 1.103287, acc: 98% // tar acc: 93% // adv loss: 0.682726, aux loss: 1.476830, tar loss: 0.852835\n",
      "=====================\n",
      "Epoch 68/200, Batch 766/938\n",
      "D loss: 1.035563, acc: 98% // tar acc: 90% // adv loss: 0.445666, aux loss: 1.466365, tar loss: 0.865954\n",
      "=====================\n",
      "Epoch 68/200, Batch 816/938\n",
      "D loss: 1.085825, acc: 98% // tar acc: 95% // adv loss: 0.312893, aux loss: 1.461168, tar loss: 0.907867\n",
      "=====================\n",
      "Epoch 68/200, Batch 866/938\n",
      "D loss: 1.110848, acc: 97% // tar acc: 100% // adv loss: 0.565109, aux loss: 1.466238, tar loss: 0.863120\n",
      "=====================\n",
      "Epoch 68/200, Batch 916/938\n",
      "D loss: 1.056210, acc: 97% // tar acc: 96% // adv loss: 0.426410, aux loss: 1.461551, tar loss: 0.866665\n",
      "=====================\n",
      "Epoch 69/200, Batch 28/938\n",
      "D loss: 1.107287, acc: 97% // tar acc: 100% // adv loss: 0.503781, aux loss: 1.465810, tar loss: 0.875928\n",
      "=====================\n",
      "Epoch 69/200, Batch 78/938\n",
      "D loss: 1.088860, acc: 99% // tar acc: 100% // adv loss: 0.512172, aux loss: 1.463456, tar loss: 0.866870\n",
      "=====================\n",
      "Epoch 69/200, Batch 128/938\n",
      "D loss: 1.089483, acc: 99% // tar acc: 98% // adv loss: 0.399016, aux loss: 1.461394, tar loss: 0.841931\n",
      "=====================\n",
      "Epoch 69/200, Batch 178/938\n",
      "D loss: 1.062322, acc: 99% // tar acc: 93% // adv loss: 0.362021, aux loss: 1.476068, tar loss: 0.916107\n",
      "=====================\n",
      "Epoch 69/200, Batch 228/938\n",
      "D loss: 1.136774, acc: 96% // tar acc: 96% // adv loss: 0.310446, aux loss: 1.461352, tar loss: 0.897679\n",
      "=====================\n",
      "Epoch 69/200, Batch 278/938\n",
      "D loss: 1.108589, acc: 94% // tar acc: 96% // adv loss: 0.334371, aux loss: 1.491536, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 69/200, Batch 328/938\n",
      "D loss: 1.076513, acc: 98% // tar acc: 96% // adv loss: 0.510315, aux loss: 1.495579, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 69/200, Batch 378/938\n",
      "D loss: 1.160239, acc: 96% // tar acc: 95% // adv loss: 0.294387, aux loss: 1.462253, tar loss: 0.901203\n",
      "=====================\n",
      "Epoch 69/200, Batch 428/938\n",
      "D loss: 1.040578, acc: 99% // tar acc: 98% // adv loss: 0.294437, aux loss: 1.462127, tar loss: 0.909740\n",
      "=====================\n",
      "Epoch 69/200, Batch 478/938\n",
      "D loss: 1.020667, acc: 100% // tar acc: 98% // adv loss: 0.479863, aux loss: 1.461893, tar loss: 0.856297\n",
      "=====================\n",
      "Epoch 69/200, Batch 528/938\n",
      "D loss: 1.061955, acc: 97% // tar acc: 96% // adv loss: 0.587535, aux loss: 1.461157, tar loss: 0.858757\n",
      "=====================\n",
      "Epoch 69/200, Batch 578/938\n",
      "D loss: 1.063261, acc: 97% // tar acc: 100% // adv loss: 0.412361, aux loss: 1.474967, tar loss: 0.879909\n",
      "=====================\n",
      "Epoch 69/200, Batch 628/938\n",
      "D loss: 1.068947, acc: 97% // tar acc: 95% // adv loss: 0.317822, aux loss: 1.478734, tar loss: 0.893154\n",
      "=====================\n",
      "Epoch 69/200, Batch 678/938\n",
      "D loss: 1.095963, acc: 98% // tar acc: 92% // adv loss: 0.539683, aux loss: 1.464793, tar loss: 0.821015\n",
      "=====================\n",
      "Epoch 69/200, Batch 728/938\n",
      "D loss: 1.093950, acc: 96% // tar acc: 98% // adv loss: 0.757475, aux loss: 1.462760, tar loss: 0.846746\n",
      "=====================\n",
      "Epoch 69/200, Batch 778/938\n",
      "D loss: 1.066224, acc: 100% // tar acc: 93% // adv loss: 0.277558, aux loss: 1.462651, tar loss: 0.887474\n",
      "=====================\n",
      "Epoch 69/200, Batch 828/938\n",
      "D loss: 1.097767, acc: 98% // tar acc: 96% // adv loss: 0.531460, aux loss: 1.464676, tar loss: 0.817035\n",
      "=====================\n",
      "Epoch 69/200, Batch 878/938\n",
      "D loss: 1.100619, acc: 100% // tar acc: 98% // adv loss: 0.340613, aux loss: 1.469629, tar loss: 0.839263\n",
      "=====================\n",
      "Epoch 69/200, Batch 928/938\n",
      "D loss: 1.000592, acc: 96% // tar acc: 89% // adv loss: 0.480576, aux loss: 1.465019, tar loss: 0.851826\n",
      "=====================\n",
      "Epoch 70/200, Batch 40/938\n",
      "D loss: 1.092957, acc: 97% // tar acc: 98% // adv loss: 0.616408, aux loss: 1.461261, tar loss: 0.875752\n",
      "=====================\n",
      "Epoch 70/200, Batch 90/938\n",
      "D loss: 1.012703, acc: 99% // tar acc: 95% // adv loss: 0.406457, aux loss: 1.475805, tar loss: 0.885023\n",
      "=====================\n",
      "Epoch 70/200, Batch 140/938\n",
      "D loss: 1.026643, acc: 98% // tar acc: 98% // adv loss: 0.490468, aux loss: 1.471626, tar loss: 0.848337\n",
      "=====================\n",
      "Epoch 70/200, Batch 190/938\n",
      "D loss: 1.139254, acc: 96% // tar acc: 95% // adv loss: 0.523099, aux loss: 1.461851, tar loss: 0.832866\n",
      "=====================\n",
      "Epoch 70/200, Batch 240/938\n",
      "D loss: 1.021587, acc: 97% // tar acc: 93% // adv loss: 0.525921, aux loss: 1.465192, tar loss: 0.833708\n",
      "=====================\n",
      "Epoch 70/200, Batch 290/938\n",
      "D loss: 1.089320, acc: 96% // tar acc: 93% // adv loss: 0.485728, aux loss: 1.487312, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 70/200, Batch 340/938\n",
      "D loss: 1.036834, acc: 93% // tar acc: 96% // adv loss: 0.312554, aux loss: 1.462877, tar loss: 0.840277\n",
      "=====================\n",
      "Epoch 70/200, Batch 390/938\n",
      "D loss: 1.018661, acc: 98% // tar acc: 96% // adv loss: 0.478237, aux loss: 1.479011, tar loss: 0.875376\n",
      "=====================\n",
      "Epoch 70/200, Batch 440/938\n",
      "D loss: 1.085792, acc: 98% // tar acc: 93% // adv loss: 0.508490, aux loss: 1.477153, tar loss: 0.850318\n",
      "=====================\n",
      "Epoch 70/200, Batch 490/938\n",
      "D loss: 1.063929, acc: 94% // tar acc: 93% // adv loss: 0.264475, aux loss: 1.474938, tar loss: 0.831519\n",
      "=====================\n",
      "Epoch 70/200, Batch 540/938\n",
      "D loss: 1.034081, acc: 97% // tar acc: 96% // adv loss: 0.434290, aux loss: 1.472189, tar loss: 0.862588\n",
      "=====================\n",
      "Epoch 70/200, Batch 590/938\n",
      "D loss: 1.115229, acc: 100% // tar acc: 90% // adv loss: 0.511619, aux loss: 1.498151, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 70/200, Batch 640/938\n",
      "D loss: 1.037816, acc: 98% // tar acc: 90% // adv loss: 0.451367, aux loss: 1.476170, tar loss: 0.828089\n",
      "=====================\n",
      "Epoch 70/200, Batch 690/938\n",
      "D loss: 1.012866, acc: 98% // tar acc: 96% // adv loss: 0.516968, aux loss: 1.461949, tar loss: 0.909098\n",
      "=====================\n",
      "Epoch 70/200, Batch 740/938\n",
      "D loss: 1.080840, acc: 94% // tar acc: 98% // adv loss: 0.367271, aux loss: 1.462855, tar loss: 0.896576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 70/200, Batch 790/938\n",
      "D loss: 1.052935, acc: 99% // tar acc: 92% // adv loss: 0.476345, aux loss: 1.465043, tar loss: 0.818456\n",
      "=====================\n",
      "Epoch 70/200, Batch 840/938\n",
      "D loss: 1.008905, acc: 99% // tar acc: 96% // adv loss: 0.386871, aux loss: 1.475656, tar loss: 0.865515\n",
      "=====================\n",
      "Epoch 70/200, Batch 890/938\n",
      "D loss: 1.038913, acc: 97% // tar acc: 96% // adv loss: 0.453210, aux loss: 1.491142, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 71/200, Batch 2/938\n",
      "D loss: 1.028206, acc: 99% // tar acc: 95% // adv loss: 0.553703, aux loss: 1.485809, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 71/200, Batch 52/938\n",
      "D loss: 1.108162, acc: 98% // tar acc: 100% // adv loss: 0.431887, aux loss: 1.462709, tar loss: 0.933938\n",
      "=====================\n",
      "Epoch 71/200, Batch 102/938\n",
      "D loss: 1.039451, acc: 97% // tar acc: 98% // adv loss: 0.324077, aux loss: 1.461337, tar loss: 0.920478\n",
      "=====================\n",
      "Epoch 71/200, Batch 152/938\n",
      "D loss: 1.091491, acc: 99% // tar acc: 98% // adv loss: 0.629067, aux loss: 1.472401, tar loss: 0.892958\n",
      "=====================\n",
      "Epoch 71/200, Batch 202/938\n",
      "D loss: 1.057648, acc: 97% // tar acc: 96% // adv loss: 0.559810, aux loss: 1.461165, tar loss: 0.839781\n",
      "=====================\n",
      "Epoch 71/200, Batch 252/938\n",
      "D loss: 1.111213, acc: 94% // tar acc: 92% // adv loss: 0.388394, aux loss: 1.480107, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 71/200, Batch 302/938\n",
      "D loss: 1.155807, acc: 99% // tar acc: 93% // adv loss: 0.439175, aux loss: 1.463620, tar loss: 0.809473\n",
      "=====================\n",
      "Epoch 71/200, Batch 352/938\n",
      "D loss: 1.012074, acc: 98% // tar acc: 92% // adv loss: 0.600267, aux loss: 1.485551, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 71/200, Batch 402/938\n",
      "D loss: 1.043573, acc: 98% // tar acc: 93% // adv loss: 0.572347, aux loss: 1.467260, tar loss: 0.842408\n",
      "=====================\n",
      "Epoch 71/200, Batch 452/938\n",
      "D loss: 1.099016, acc: 98% // tar acc: 92% // adv loss: 0.566494, aux loss: 1.469284, tar loss: 0.846284\n",
      "=====================\n",
      "Epoch 71/200, Batch 502/938\n",
      "D loss: 1.044232, acc: 97% // tar acc: 93% // adv loss: 0.419603, aux loss: 1.478243, tar loss: 0.831584\n",
      "=====================\n",
      "Epoch 71/200, Batch 552/938\n",
      "D loss: 1.012769, acc: 100% // tar acc: 96% // adv loss: 0.486580, aux loss: 1.474709, tar loss: 0.888554\n",
      "=====================\n",
      "Epoch 71/200, Batch 602/938\n",
      "D loss: 1.065797, acc: 96% // tar acc: 98% // adv loss: 0.676169, aux loss: 1.465017, tar loss: 0.805955\n",
      "=====================\n",
      "Epoch 71/200, Batch 652/938\n",
      "D loss: 1.055306, acc: 100% // tar acc: 90% // adv loss: 0.465973, aux loss: 1.480883, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 71/200, Batch 702/938\n",
      "D loss: 0.999656, acc: 100% // tar acc: 95% // adv loss: 0.587930, aux loss: 1.462987, tar loss: 0.850762\n",
      "=====================\n",
      "Epoch 71/200, Batch 752/938\n",
      "D loss: 1.060591, acc: 96% // tar acc: 96% // adv loss: 0.326162, aux loss: 1.492564, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 71/200, Batch 802/938\n",
      "D loss: 1.066232, acc: 96% // tar acc: 93% // adv loss: 0.416106, aux loss: 1.471286, tar loss: 0.818750\n",
      "=====================\n",
      "Epoch 71/200, Batch 852/938\n",
      "D loss: 1.018086, acc: 98% // tar acc: 98% // adv loss: 0.436224, aux loss: 1.476799, tar loss: 0.835046\n",
      "=====================\n",
      "Epoch 71/200, Batch 902/938\n",
      "D loss: 1.058302, acc: 97% // tar acc: 90% // adv loss: 0.486075, aux loss: 1.504939, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 72/200, Batch 14/938\n",
      "D loss: 1.032908, acc: 99% // tar acc: 98% // adv loss: 0.563872, aux loss: 1.476760, tar loss: 0.892913\n",
      "=====================\n",
      "Epoch 72/200, Batch 64/938\n",
      "D loss: 1.024525, acc: 99% // tar acc: 100% // adv loss: 0.349898, aux loss: 1.474698, tar loss: 0.909557\n",
      "=====================\n",
      "Epoch 72/200, Batch 114/938\n",
      "D loss: 1.070631, acc: 97% // tar acc: 100% // adv loss: 0.432377, aux loss: 1.463501, tar loss: 0.890565\n",
      "=====================\n",
      "Epoch 72/200, Batch 164/938\n",
      "D loss: 1.068604, acc: 98% // tar acc: 95% // adv loss: 0.597085, aux loss: 1.464023, tar loss: 0.846407\n",
      "=====================\n",
      "Epoch 72/200, Batch 214/938\n",
      "D loss: 1.062284, acc: 97% // tar acc: 96% // adv loss: 0.313139, aux loss: 1.468483, tar loss: 0.864951\n",
      "=====================\n",
      "Epoch 72/200, Batch 264/938\n",
      "D loss: 1.104739, acc: 96% // tar acc: 95% // adv loss: 0.415429, aux loss: 1.477729, tar loss: 0.848474\n",
      "=====================\n",
      "Epoch 72/200, Batch 314/938\n",
      "D loss: 0.946138, acc: 100% // tar acc: 95% // adv loss: 0.300468, aux loss: 1.463791, tar loss: 0.844895\n",
      "=====================\n",
      "Epoch 72/200, Batch 364/938\n",
      "D loss: 1.090094, acc: 96% // tar acc: 96% // adv loss: 0.298257, aux loss: 1.463848, tar loss: 0.831207\n",
      "=====================\n",
      "Epoch 72/200, Batch 414/938\n",
      "D loss: 1.038805, acc: 97% // tar acc: 96% // adv loss: 0.535446, aux loss: 1.461983, tar loss: 0.866325\n",
      "=====================\n",
      "Epoch 72/200, Batch 464/938\n",
      "D loss: 1.037736, acc: 96% // tar acc: 96% // adv loss: 0.515725, aux loss: 1.503810, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 72/200, Batch 514/938\n",
      "D loss: 1.085292, acc: 96% // tar acc: 95% // adv loss: 0.445909, aux loss: 1.466501, tar loss: 0.846446\n",
      "=====================\n",
      "Epoch 72/200, Batch 564/938\n",
      "D loss: 1.058188, acc: 98% // tar acc: 93% // adv loss: 0.715481, aux loss: 1.461317, tar loss: 0.860502\n",
      "=====================\n",
      "Epoch 72/200, Batch 614/938\n",
      "D loss: 1.071609, acc: 100% // tar acc: 100% // adv loss: 0.384963, aux loss: 1.461806, tar loss: 0.855700\n",
      "=====================\n",
      "Epoch 72/200, Batch 664/938\n",
      "D loss: 1.051257, acc: 97% // tar acc: 96% // adv loss: 0.378186, aux loss: 1.462479, tar loss: 0.822315\n",
      "=====================\n",
      "Epoch 72/200, Batch 714/938\n",
      "D loss: 1.118936, acc: 94% // tar acc: 89% // adv loss: 0.435493, aux loss: 1.462134, tar loss: 0.858821\n",
      "=====================\n",
      "Epoch 72/200, Batch 764/938\n",
      "D loss: 0.991938, acc: 97% // tar acc: 95% // adv loss: 0.677604, aux loss: 1.477123, tar loss: 0.863001\n",
      "=====================\n",
      "Epoch 72/200, Batch 814/938\n",
      "D loss: 1.083025, acc: 99% // tar acc: 98% // adv loss: 0.380982, aux loss: 1.465213, tar loss: 0.886845\n",
      "=====================\n",
      "Epoch 72/200, Batch 864/938\n",
      "D loss: 1.041041, acc: 97% // tar acc: 96% // adv loss: 0.433691, aux loss: 1.461849, tar loss: 0.859119\n",
      "=====================\n",
      "Epoch 72/200, Batch 914/938\n",
      "D loss: 1.127786, acc: 98% // tar acc: 98% // adv loss: 0.497908, aux loss: 1.465286, tar loss: 0.884113\n",
      "=====================\n",
      "Epoch 73/200, Batch 26/938\n",
      "D loss: 1.054411, acc: 98% // tar acc: 98% // adv loss: 0.611639, aux loss: 1.462415, tar loss: 0.861381\n",
      "=====================\n",
      "Epoch 73/200, Batch 76/938\n",
      "D loss: 1.095363, acc: 99% // tar acc: 93% // adv loss: 0.473315, aux loss: 1.461275, tar loss: 0.840964\n",
      "=====================\n",
      "Epoch 73/200, Batch 126/938\n",
      "D loss: 1.138349, acc: 97% // tar acc: 93% // adv loss: 0.400212, aux loss: 1.475505, tar loss: 0.845732\n",
      "=====================\n",
      "Epoch 73/200, Batch 176/938\n",
      "D loss: 1.031789, acc: 99% // tar acc: 95% // adv loss: 0.398222, aux loss: 1.461440, tar loss: 0.861673\n",
      "=====================\n",
      "Epoch 73/200, Batch 226/938\n",
      "D loss: 1.063333, acc: 98% // tar acc: 95% // adv loss: 0.390131, aux loss: 1.462130, tar loss: 0.830770\n",
      "=====================\n",
      "Epoch 73/200, Batch 276/938\n",
      "D loss: 1.095983, acc: 99% // tar acc: 96% // adv loss: 0.459180, aux loss: 1.461663, tar loss: 0.848947\n",
      "=====================\n",
      "Epoch 73/200, Batch 326/938\n",
      "D loss: 1.055160, acc: 98% // tar acc: 96% // adv loss: 0.319018, aux loss: 1.479276, tar loss: 0.860850\n",
      "=====================\n",
      "Epoch 73/200, Batch 376/938\n",
      "D loss: 1.083744, acc: 99% // tar acc: 93% // adv loss: 0.380812, aux loss: 1.462650, tar loss: 0.872242\n",
      "=====================\n",
      "Epoch 73/200, Batch 426/938\n",
      "D loss: 1.028427, acc: 100% // tar acc: 95% // adv loss: 0.525473, aux loss: 1.464750, tar loss: 0.839410\n",
      "=====================\n",
      "Epoch 73/200, Batch 476/938\n",
      "D loss: 1.031240, acc: 98% // tar acc: 95% // adv loss: 0.748234, aux loss: 1.470766, tar loss: 0.825104\n",
      "=====================\n",
      "Epoch 73/200, Batch 526/938\n",
      "D loss: 1.037804, acc: 98% // tar acc: 96% // adv loss: 0.434725, aux loss: 1.465126, tar loss: 0.783787\n",
      "=====================\n",
      "Epoch 73/200, Batch 576/938\n",
      "D loss: 1.056971, acc: 95% // tar acc: 93% // adv loss: 0.484506, aux loss: 1.462283, tar loss: 0.864780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 73/200, Batch 626/938\n",
      "D loss: 1.105772, acc: 96% // tar acc: 96% // adv loss: 0.643120, aux loss: 1.461263, tar loss: 0.829036\n",
      "=====================\n",
      "Epoch 73/200, Batch 676/938\n",
      "D loss: 1.084350, acc: 98% // tar acc: 95% // adv loss: 0.651678, aux loss: 1.483841, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 73/200, Batch 726/938\n",
      "D loss: 1.057075, acc: 98% // tar acc: 95% // adv loss: 0.472001, aux loss: 1.468278, tar loss: 0.880270\n",
      "=====================\n",
      "Epoch 73/200, Batch 776/938\n",
      "D loss: 1.096303, acc: 96% // tar acc: 98% // adv loss: 0.475302, aux loss: 1.461185, tar loss: 0.892083\n",
      "=====================\n",
      "Epoch 73/200, Batch 826/938\n",
      "D loss: 1.002003, acc: 98% // tar acc: 98% // adv loss: 0.630511, aux loss: 1.471401, tar loss: 0.841767\n",
      "=====================\n",
      "Epoch 73/200, Batch 876/938\n",
      "D loss: 1.061979, acc: 98% // tar acc: 98% // adv loss: 0.556902, aux loss: 1.461166, tar loss: 0.892603\n",
      "=====================\n",
      "Epoch 73/200, Batch 926/938\n",
      "D loss: 1.071440, acc: 99% // tar acc: 100% // adv loss: 0.666904, aux loss: 1.462088, tar loss: 0.864502\n",
      "=====================\n",
      "Epoch 74/200, Batch 38/938\n",
      "D loss: 1.034550, acc: 97% // tar acc: 98% // adv loss: 0.294192, aux loss: 1.461321, tar loss: 0.880709\n",
      "=====================\n",
      "Epoch 74/200, Batch 88/938\n",
      "D loss: 1.016416, acc: 97% // tar acc: 98% // adv loss: 0.446048, aux loss: 1.464430, tar loss: 0.837405\n",
      "=====================\n",
      "Epoch 74/200, Batch 138/938\n",
      "D loss: 1.119782, acc: 98% // tar acc: 100% // adv loss: 0.567920, aux loss: 1.481295, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 74/200, Batch 188/938\n",
      "D loss: 1.023549, acc: 100% // tar acc: 92% // adv loss: 0.401907, aux loss: 1.470681, tar loss: 0.865275\n",
      "=====================\n",
      "Epoch 74/200, Batch 238/938\n",
      "D loss: 1.070704, acc: 100% // tar acc: 96% // adv loss: 0.446705, aux loss: 1.474710, tar loss: 0.852286\n",
      "=====================\n",
      "Epoch 74/200, Batch 288/938\n",
      "D loss: 1.111522, acc: 99% // tar acc: 95% // adv loss: 0.486070, aux loss: 1.468480, tar loss: 0.834801\n",
      "=====================\n",
      "Epoch 74/200, Batch 338/938\n",
      "D loss: 1.092724, acc: 98% // tar acc: 98% // adv loss: 0.385255, aux loss: 1.461934, tar loss: 0.849282\n",
      "=====================\n",
      "Epoch 74/200, Batch 388/938\n",
      "D loss: 1.045002, acc: 97% // tar acc: 90% // adv loss: 0.727477, aux loss: 1.476945, tar loss: 0.852976\n",
      "=====================\n",
      "Epoch 74/200, Batch 438/938\n",
      "D loss: 0.964544, acc: 99% // tar acc: 95% // adv loss: 0.548071, aux loss: 1.464276, tar loss: 0.852996\n",
      "=====================\n",
      "Epoch 74/200, Batch 488/938\n",
      "D loss: 1.025712, acc: 96% // tar acc: 98% // adv loss: 0.454678, aux loss: 1.481720, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 74/200, Batch 538/938\n",
      "D loss: 1.082934, acc: 98% // tar acc: 100% // adv loss: 0.513246, aux loss: 1.470843, tar loss: 0.880688\n",
      "=====================\n",
      "Epoch 74/200, Batch 588/938\n",
      "D loss: 1.007213, acc: 100% // tar acc: 96% // adv loss: 0.480561, aux loss: 1.461808, tar loss: 0.886504\n",
      "=====================\n",
      "Epoch 74/200, Batch 638/938\n",
      "D loss: 1.047896, acc: 96% // tar acc: 96% // adv loss: 0.397212, aux loss: 1.487525, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 74/200, Batch 688/938\n",
      "D loss: 1.088560, acc: 99% // tar acc: 92% // adv loss: 0.382013, aux loss: 1.467366, tar loss: 0.826316\n",
      "=====================\n",
      "Epoch 74/200, Batch 738/938\n",
      "D loss: 1.176123, acc: 95% // tar acc: 93% // adv loss: 0.488609, aux loss: 1.461910, tar loss: 0.852776\n",
      "=====================\n",
      "Epoch 74/200, Batch 788/938\n",
      "D loss: 1.055697, acc: 96% // tar acc: 93% // adv loss: 0.556787, aux loss: 1.480726, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 74/200, Batch 838/938\n",
      "D loss: 1.031958, acc: 98% // tar acc: 90% // adv loss: 0.669368, aux loss: 1.490342, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 74/200, Batch 888/938\n",
      "D loss: 1.096493, acc: 100% // tar acc: 95% // adv loss: 0.434969, aux loss: 1.462996, tar loss: 0.868275\n",
      "=====================\n",
      "Epoch 75/200, Batch 0/938\n",
      "D loss: 1.076991, acc: 99% // tar acc: 92% // adv loss: 0.420939, aux loss: 1.465985, tar loss: 0.856645\n",
      "=====================\n",
      "Epoch 75/200, Batch 50/938\n",
      "D loss: 1.053008, acc: 98% // tar acc: 93% // adv loss: 0.457178, aux loss: 1.461263, tar loss: 0.859351\n",
      "=====================\n",
      "Epoch 75/200, Batch 100/938\n",
      "D loss: 1.055130, acc: 100% // tar acc: 98% // adv loss: 0.518647, aux loss: 1.465878, tar loss: 0.897632\n",
      "=====================\n",
      "Epoch 75/200, Batch 150/938\n",
      "D loss: 1.024382, acc: 99% // tar acc: 96% // adv loss: 0.589845, aux loss: 1.461249, tar loss: 0.902189\n",
      "=====================\n",
      "Epoch 75/200, Batch 200/938\n",
      "D loss: 1.058215, acc: 98% // tar acc: 95% // adv loss: 0.356514, aux loss: 1.463850, tar loss: 0.857139\n",
      "=====================\n",
      "Epoch 75/200, Batch 250/938\n",
      "D loss: 1.028990, acc: 99% // tar acc: 95% // adv loss: 0.474565, aux loss: 1.469512, tar loss: 0.863867\n",
      "=====================\n",
      "Epoch 75/200, Batch 300/938\n",
      "D loss: 1.111293, acc: 96% // tar acc: 98% // adv loss: 0.420876, aux loss: 1.467937, tar loss: 0.839012\n",
      "=====================\n",
      "Epoch 75/200, Batch 350/938\n",
      "D loss: 1.065265, acc: 95% // tar acc: 96% // adv loss: 0.471411, aux loss: 1.461877, tar loss: 0.865363\n",
      "=====================\n",
      "Epoch 75/200, Batch 400/938\n",
      "D loss: 1.050397, acc: 100% // tar acc: 95% // adv loss: 0.517964, aux loss: 1.461190, tar loss: 0.857861\n",
      "=====================\n",
      "Epoch 75/200, Batch 450/938\n",
      "D loss: 1.028181, acc: 99% // tar acc: 93% // adv loss: 0.495141, aux loss: 1.461242, tar loss: 0.865769\n",
      "=====================\n",
      "Epoch 75/200, Batch 500/938\n",
      "D loss: 1.132817, acc: 97% // tar acc: 93% // adv loss: 0.560661, aux loss: 1.461352, tar loss: 0.855533\n",
      "=====================\n",
      "Epoch 75/200, Batch 550/938\n",
      "D loss: 0.994190, acc: 99% // tar acc: 98% // adv loss: 0.526656, aux loss: 1.463355, tar loss: 0.893817\n",
      "=====================\n",
      "Epoch 75/200, Batch 600/938\n",
      "D loss: 1.083450, acc: 96% // tar acc: 98% // adv loss: 0.389658, aux loss: 1.463946, tar loss: 0.876119\n",
      "=====================\n",
      "Epoch 75/200, Batch 650/938\n",
      "D loss: 1.157036, acc: 99% // tar acc: 96% // adv loss: 0.452288, aux loss: 1.461237, tar loss: 0.929700\n",
      "=====================\n",
      "Epoch 75/200, Batch 700/938\n",
      "D loss: 1.104475, acc: 97% // tar acc: 93% // adv loss: 0.354713, aux loss: 1.501553, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 75/200, Batch 750/938\n",
      "D loss: 1.035805, acc: 97% // tar acc: 96% // adv loss: 0.320146, aux loss: 1.464772, tar loss: 0.853017\n",
      "=====================\n",
      "Epoch 75/200, Batch 800/938\n",
      "D loss: 1.010763, acc: 97% // tar acc: 96% // adv loss: 0.369334, aux loss: 1.506063, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 75/200, Batch 850/938\n",
      "D loss: 1.016527, acc: 98% // tar acc: 93% // adv loss: 0.507468, aux loss: 1.461533, tar loss: 0.813657\n",
      "=====================\n",
      "Epoch 75/200, Batch 900/938\n",
      "D loss: 1.036737, acc: 97% // tar acc: 95% // adv loss: 0.347680, aux loss: 1.461437, tar loss: 0.864540\n",
      "=====================\n",
      "Epoch 76/200, Batch 12/938\n",
      "D loss: 1.109242, acc: 98% // tar acc: 96% // adv loss: 0.464035, aux loss: 1.476532, tar loss: 0.857856\n",
      "=====================\n",
      "Epoch 76/200, Batch 62/938\n",
      "D loss: 1.018601, acc: 99% // tar acc: 93% // adv loss: 0.592902, aux loss: 1.462161, tar loss: 0.874683\n",
      "=====================\n",
      "Epoch 76/200, Batch 112/938\n",
      "D loss: 1.090478, acc: 97% // tar acc: 96% // adv loss: 0.418742, aux loss: 1.466931, tar loss: 0.891193\n",
      "=====================\n",
      "Epoch 76/200, Batch 162/938\n",
      "D loss: 1.064057, acc: 96% // tar acc: 93% // adv loss: 0.514509, aux loss: 1.478055, tar loss: 0.850794\n",
      "=====================\n",
      "Epoch 76/200, Batch 212/938\n",
      "D loss: 1.064083, acc: 98% // tar acc: 95% // adv loss: 0.376007, aux loss: 1.476132, tar loss: 0.856302\n",
      "=====================\n",
      "Epoch 76/200, Batch 262/938\n",
      "D loss: 1.064314, acc: 97% // tar acc: 92% // adv loss: 0.502439, aux loss: 1.468435, tar loss: 0.851511\n",
      "=====================\n",
      "Epoch 76/200, Batch 312/938\n",
      "D loss: 1.028537, acc: 98% // tar acc: 98% // adv loss: 0.502643, aux loss: 1.462551, tar loss: 0.902168\n",
      "=====================\n",
      "Epoch 76/200, Batch 362/938\n",
      "D loss: 1.001125, acc: 96% // tar acc: 100% // adv loss: 0.390721, aux loss: 1.461286, tar loss: 0.879480\n",
      "=====================\n",
      "Epoch 76/200, Batch 412/938\n",
      "D loss: 1.071552, acc: 98% // tar acc: 95% // adv loss: 0.315755, aux loss: 1.462668, tar loss: 0.841505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 76/200, Batch 462/938\n",
      "D loss: 0.979696, acc: 94% // tar acc: 96% // adv loss: 0.741089, aux loss: 1.461617, tar loss: 0.809983\n",
      "=====================\n",
      "Epoch 76/200, Batch 512/938\n",
      "D loss: 0.997087, acc: 97% // tar acc: 98% // adv loss: 0.431465, aux loss: 1.471775, tar loss: 0.816900\n",
      "=====================\n",
      "Epoch 76/200, Batch 562/938\n",
      "D loss: 1.102230, acc: 98% // tar acc: 96% // adv loss: 0.241948, aux loss: 1.461541, tar loss: 0.852268\n",
      "=====================\n",
      "Epoch 76/200, Batch 612/938\n",
      "D loss: 1.030401, acc: 99% // tar acc: 98% // adv loss: 0.528051, aux loss: 1.461486, tar loss: 0.883410\n",
      "=====================\n",
      "Epoch 76/200, Batch 662/938\n",
      "D loss: 1.099219, acc: 97% // tar acc: 96% // adv loss: 0.498697, aux loss: 1.477470, tar loss: 0.833264\n",
      "=====================\n",
      "Epoch 76/200, Batch 712/938\n",
      "D loss: 1.109357, acc: 98% // tar acc: 98% // adv loss: 0.428636, aux loss: 1.481091, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 76/200, Batch 762/938\n",
      "D loss: 1.019937, acc: 98% // tar acc: 96% // adv loss: 0.350866, aux loss: 1.461819, tar loss: 0.875374\n",
      "=====================\n",
      "Epoch 76/200, Batch 812/938\n",
      "D loss: 1.027434, acc: 97% // tar acc: 93% // adv loss: 0.517707, aux loss: 1.464975, tar loss: 0.841312\n",
      "=====================\n",
      "Epoch 76/200, Batch 862/938\n",
      "D loss: 1.021591, acc: 99% // tar acc: 95% // adv loss: 0.399915, aux loss: 1.461767, tar loss: 0.814019\n",
      "=====================\n",
      "Epoch 76/200, Batch 912/938\n",
      "D loss: 1.020490, acc: 98% // tar acc: 96% // adv loss: 0.580608, aux loss: 1.464185, tar loss: 0.835228\n",
      "=====================\n",
      "Epoch 77/200, Batch 24/938\n",
      "D loss: 1.019419, acc: 96% // tar acc: 98% // adv loss: 0.289724, aux loss: 1.461776, tar loss: 0.890684\n",
      "=====================\n",
      "Epoch 77/200, Batch 74/938\n",
      "D loss: 1.117337, acc: 95% // tar acc: 93% // adv loss: 0.487081, aux loss: 1.468057, tar loss: 0.882255\n",
      "=====================\n",
      "Epoch 77/200, Batch 124/938\n",
      "D loss: 1.085785, acc: 97% // tar acc: 92% // adv loss: 0.279029, aux loss: 1.472544, tar loss: 0.833945\n",
      "=====================\n",
      "Epoch 77/200, Batch 174/938\n",
      "D loss: 1.078586, acc: 95% // tar acc: 96% // adv loss: 0.384523, aux loss: 1.476913, tar loss: 0.858851\n",
      "=====================\n",
      "Epoch 77/200, Batch 224/938\n",
      "D loss: 1.011283, acc: 100% // tar acc: 96% // adv loss: 0.441643, aux loss: 1.461480, tar loss: 0.863634\n",
      "=====================\n",
      "Epoch 77/200, Batch 274/938\n",
      "D loss: 1.037150, acc: 97% // tar acc: 96% // adv loss: 0.654775, aux loss: 1.483498, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 77/200, Batch 324/938\n",
      "D loss: 1.115770, acc: 97% // tar acc: 93% // adv loss: 0.525072, aux loss: 1.467088, tar loss: 0.885548\n",
      "=====================\n",
      "Epoch 77/200, Batch 374/938\n",
      "D loss: 1.140024, acc: 97% // tar acc: 96% // adv loss: 0.450552, aux loss: 1.461305, tar loss: 0.904099\n",
      "=====================\n",
      "Epoch 77/200, Batch 424/938\n",
      "D loss: 1.070130, acc: 100% // tar acc: 96% // adv loss: 0.674515, aux loss: 1.462304, tar loss: 0.864161\n",
      "=====================\n",
      "Epoch 77/200, Batch 474/938\n",
      "D loss: 1.076363, acc: 98% // tar acc: 92% // adv loss: 0.483546, aux loss: 1.470495, tar loss: 0.857821\n",
      "=====================\n",
      "Epoch 77/200, Batch 524/938\n",
      "D loss: 1.059983, acc: 100% // tar acc: 90% // adv loss: 0.514912, aux loss: 1.466441, tar loss: 0.809536\n",
      "=====================\n",
      "Epoch 77/200, Batch 574/938\n",
      "D loss: 1.060444, acc: 96% // tar acc: 98% // adv loss: 0.449484, aux loss: 1.467763, tar loss: 0.848458\n",
      "=====================\n",
      "Epoch 77/200, Batch 624/938\n",
      "D loss: 1.095394, acc: 98% // tar acc: 96% // adv loss: 0.561906, aux loss: 1.481234, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 77/200, Batch 674/938\n",
      "D loss: 1.088435, acc: 96% // tar acc: 96% // adv loss: 0.369734, aux loss: 1.463070, tar loss: 0.833589\n",
      "=====================\n",
      "Epoch 77/200, Batch 724/938\n",
      "D loss: 1.024943, acc: 96% // tar acc: 93% // adv loss: 0.635040, aux loss: 1.473092, tar loss: 0.839858\n",
      "=====================\n",
      "Epoch 77/200, Batch 774/938\n",
      "D loss: 0.973643, acc: 98% // tar acc: 96% // adv loss: 0.552336, aux loss: 1.461514, tar loss: 0.804907\n",
      "=====================\n",
      "Epoch 77/200, Batch 824/938\n",
      "D loss: 1.102240, acc: 98% // tar acc: 96% // adv loss: 0.513370, aux loss: 1.477101, tar loss: 0.829713\n",
      "=====================\n",
      "Epoch 77/200, Batch 874/938\n",
      "D loss: 1.083296, acc: 96% // tar acc: 93% // adv loss: 0.383775, aux loss: 1.462234, tar loss: 0.806620\n",
      "=====================\n",
      "Epoch 77/200, Batch 924/938\n",
      "D loss: 1.091795, acc: 98% // tar acc: 95% // adv loss: 0.616629, aux loss: 1.461785, tar loss: 0.854195\n",
      "=====================\n",
      "Epoch 78/200, Batch 36/938\n",
      "D loss: 1.103773, acc: 99% // tar acc: 98% // adv loss: 0.524579, aux loss: 1.463197, tar loss: 0.863300\n",
      "=====================\n",
      "Epoch 78/200, Batch 86/938\n",
      "D loss: 1.056725, acc: 97% // tar acc: 90% // adv loss: 0.536838, aux loss: 1.462041, tar loss: 0.871926\n",
      "=====================\n",
      "Epoch 78/200, Batch 136/938\n",
      "D loss: 1.035838, acc: 99% // tar acc: 96% // adv loss: 0.535327, aux loss: 1.462819, tar loss: 0.870992\n",
      "=====================\n",
      "Epoch 78/200, Batch 186/938\n",
      "D loss: 1.139638, acc: 98% // tar acc: 98% // adv loss: 0.426173, aux loss: 1.466286, tar loss: 0.892141\n",
      "=====================\n",
      "Epoch 78/200, Batch 236/938\n",
      "D loss: 1.028200, acc: 100% // tar acc: 95% // adv loss: 0.389335, aux loss: 1.464866, tar loss: 0.867607\n",
      "=====================\n",
      "Epoch 78/200, Batch 286/938\n",
      "D loss: 1.033286, acc: 95% // tar acc: 93% // adv loss: 0.677505, aux loss: 1.468805, tar loss: 0.837638\n",
      "=====================\n",
      "Epoch 78/200, Batch 336/938\n",
      "D loss: 1.007434, acc: 98% // tar acc: 95% // adv loss: 0.372987, aux loss: 1.476412, tar loss: 0.855920\n",
      "=====================\n",
      "Epoch 78/200, Batch 386/938\n",
      "D loss: 1.071504, acc: 94% // tar acc: 98% // adv loss: 0.446869, aux loss: 1.476413, tar loss: 0.833571\n",
      "=====================\n",
      "Epoch 78/200, Batch 436/938\n",
      "D loss: 1.108317, acc: 97% // tar acc: 96% // adv loss: 0.528594, aux loss: 1.461531, tar loss: 0.845655\n",
      "=====================\n",
      "Epoch 78/200, Batch 486/938\n",
      "D loss: 1.017901, acc: 96% // tar acc: 95% // adv loss: 0.484482, aux loss: 1.462516, tar loss: 0.869925\n",
      "=====================\n",
      "Epoch 78/200, Batch 536/938\n",
      "D loss: 1.060097, acc: 100% // tar acc: 100% // adv loss: 0.556151, aux loss: 1.474531, tar loss: 0.952140\n",
      "=====================\n",
      "Epoch 78/200, Batch 586/938\n",
      "D loss: 1.140731, acc: 99% // tar acc: 98% // adv loss: 0.664135, aux loss: 1.461309, tar loss: 0.916744\n",
      "=====================\n",
      "Epoch 78/200, Batch 636/938\n",
      "D loss: 1.073000, acc: 97% // tar acc: 96% // adv loss: 0.553988, aux loss: 1.461862, tar loss: 0.869750\n",
      "=====================\n",
      "Epoch 78/200, Batch 686/938\n",
      "D loss: 1.045133, acc: 98% // tar acc: 95% // adv loss: 0.743961, aux loss: 1.461880, tar loss: 0.854214\n",
      "=====================\n",
      "Epoch 78/200, Batch 736/938\n",
      "D loss: 1.010438, acc: 99% // tar acc: 98% // adv loss: 0.396204, aux loss: 1.462796, tar loss: 0.893331\n",
      "=====================\n",
      "Epoch 78/200, Batch 786/938\n",
      "D loss: 1.020440, acc: 96% // tar acc: 96% // adv loss: 0.453252, aux loss: 1.463368, tar loss: 0.895492\n",
      "=====================\n",
      "Epoch 78/200, Batch 836/938\n",
      "D loss: 1.009174, acc: 98% // tar acc: 100% // adv loss: 0.466006, aux loss: 1.461526, tar loss: 0.930168\n",
      "=====================\n",
      "Epoch 78/200, Batch 886/938\n",
      "D loss: 1.091776, acc: 98% // tar acc: 98% // adv loss: 0.283608, aux loss: 1.461168, tar loss: 0.921691\n",
      "=====================\n",
      "Epoch 78/200, Batch 936/938\n",
      "D loss: 1.063458, acc: 97% // tar acc: 92% // adv loss: 0.697909, aux loss: 1.464251, tar loss: 0.880453\n",
      "=====================\n",
      "Epoch 79/200, Batch 48/938\n",
      "D loss: 1.030928, acc: 99% // tar acc: 98% // adv loss: 0.341397, aux loss: 1.473045, tar loss: 0.860998\n",
      "=====================\n",
      "Epoch 79/200, Batch 98/938\n",
      "D loss: 0.956855, acc: 99% // tar acc: 95% // adv loss: 0.387371, aux loss: 1.461795, tar loss: 0.853736\n",
      "=====================\n",
      "Epoch 79/200, Batch 148/938\n",
      "D loss: 1.078264, acc: 98% // tar acc: 100% // adv loss: 0.420819, aux loss: 1.461209, tar loss: 0.864766\n",
      "=====================\n",
      "Epoch 79/200, Batch 198/938\n",
      "D loss: 1.012701, acc: 98% // tar acc: 95% // adv loss: 0.587594, aux loss: 1.475753, tar loss: 0.843059\n",
      "=====================\n",
      "Epoch 79/200, Batch 248/938\n",
      "D loss: 1.069754, acc: 97% // tar acc: 95% // adv loss: 0.523114, aux loss: 1.485514, tar loss: 1.332000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 79/200, Batch 298/938\n",
      "D loss: 1.078854, acc: 96% // tar acc: 92% // adv loss: 0.570318, aux loss: 1.487885, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 79/200, Batch 348/938\n",
      "D loss: 1.101578, acc: 100% // tar acc: 93% // adv loss: 0.477073, aux loss: 1.492220, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 79/200, Batch 398/938\n",
      "D loss: 1.073661, acc: 95% // tar acc: 98% // adv loss: 0.430842, aux loss: 1.461409, tar loss: 0.890865\n",
      "=====================\n",
      "Epoch 79/200, Batch 448/938\n",
      "D loss: 1.030052, acc: 98% // tar acc: 93% // adv loss: 0.369939, aux loss: 1.476516, tar loss: 0.864674\n",
      "=====================\n",
      "Epoch 79/200, Batch 498/938\n",
      "D loss: 1.089182, acc: 99% // tar acc: 96% // adv loss: 0.376920, aux loss: 1.472265, tar loss: 0.908652\n",
      "=====================\n",
      "Epoch 79/200, Batch 548/938\n",
      "D loss: 1.126999, acc: 96% // tar acc: 98% // adv loss: 0.385932, aux loss: 1.476615, tar loss: 0.936478\n",
      "=====================\n",
      "Epoch 79/200, Batch 598/938\n",
      "D loss: 1.004015, acc: 99% // tar acc: 96% // adv loss: 0.461770, aux loss: 1.473704, tar loss: 0.871160\n",
      "=====================\n",
      "Epoch 79/200, Batch 648/938\n",
      "D loss: 1.045888, acc: 98% // tar acc: 95% // adv loss: 0.568570, aux loss: 1.474755, tar loss: 0.921994\n",
      "=====================\n",
      "Epoch 79/200, Batch 698/938\n",
      "D loss: 1.045282, acc: 99% // tar acc: 98% // adv loss: 0.537700, aux loss: 1.462840, tar loss: 0.885165\n",
      "=====================\n",
      "Epoch 79/200, Batch 748/938\n",
      "D loss: 1.119505, acc: 98% // tar acc: 98% // adv loss: 0.487104, aux loss: 1.473253, tar loss: 0.918281\n",
      "=====================\n",
      "Epoch 79/200, Batch 798/938\n",
      "D loss: 1.077066, acc: 96% // tar acc: 98% // adv loss: 0.436029, aux loss: 1.461426, tar loss: 0.854584\n",
      "=====================\n",
      "Epoch 79/200, Batch 848/938\n",
      "D loss: 1.062016, acc: 97% // tar acc: 96% // adv loss: 0.426656, aux loss: 1.461213, tar loss: 0.885125\n",
      "=====================\n",
      "Epoch 79/200, Batch 898/938\n",
      "D loss: 1.097638, acc: 97% // tar acc: 100% // adv loss: 0.435396, aux loss: 1.461468, tar loss: 0.886734\n",
      "=====================\n",
      "Epoch 80/200, Batch 10/938\n",
      "D loss: 0.966203, acc: 99% // tar acc: 95% // adv loss: 0.595175, aux loss: 1.462627, tar loss: 0.901280\n",
      "=====================\n",
      "Epoch 80/200, Batch 60/938\n",
      "D loss: 1.069381, acc: 97% // tar acc: 96% // adv loss: 0.429114, aux loss: 1.471673, tar loss: 0.890137\n",
      "=====================\n",
      "Epoch 80/200, Batch 110/938\n",
      "D loss: 1.039115, acc: 97% // tar acc: 92% // adv loss: 0.355510, aux loss: 1.470617, tar loss: 0.865060\n",
      "=====================\n",
      "Epoch 80/200, Batch 160/938\n",
      "D loss: 1.032927, acc: 100% // tar acc: 96% // adv loss: 0.503520, aux loss: 1.463313, tar loss: 0.862413\n",
      "=====================\n",
      "Epoch 80/200, Batch 210/938\n",
      "D loss: 1.074359, acc: 98% // tar acc: 98% // adv loss: 0.445894, aux loss: 1.461732, tar loss: 0.859417\n",
      "=====================\n",
      "Epoch 80/200, Batch 260/938\n",
      "D loss: 1.071978, acc: 96% // tar acc: 96% // adv loss: 0.388369, aux loss: 1.476243, tar loss: 0.883617\n",
      "=====================\n",
      "Epoch 80/200, Batch 310/938\n",
      "D loss: 1.089769, acc: 98% // tar acc: 98% // adv loss: 0.414150, aux loss: 1.489084, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 80/200, Batch 360/938\n",
      "D loss: 1.048862, acc: 97% // tar acc: 95% // adv loss: 0.541162, aux loss: 1.461183, tar loss: 0.868391\n",
      "=====================\n",
      "Epoch 80/200, Batch 410/938\n",
      "D loss: 1.070120, acc: 99% // tar acc: 96% // adv loss: 0.280598, aux loss: 1.461789, tar loss: 0.891937\n",
      "=====================\n",
      "Epoch 80/200, Batch 460/938\n",
      "D loss: 1.119174, acc: 97% // tar acc: 93% // adv loss: 0.400728, aux loss: 1.471770, tar loss: 0.791472\n",
      "=====================\n",
      "Epoch 80/200, Batch 510/938\n",
      "D loss: 1.123850, acc: 97% // tar acc: 93% // adv loss: 0.339011, aux loss: 1.480873, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 80/200, Batch 560/938\n",
      "D loss: 1.096500, acc: 96% // tar acc: 95% // adv loss: 0.398436, aux loss: 1.484332, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 80/200, Batch 610/938\n",
      "D loss: 1.029861, acc: 98% // tar acc: 98% // adv loss: 0.386209, aux loss: 1.478029, tar loss: 0.878935\n",
      "=====================\n",
      "Epoch 80/200, Batch 660/938\n",
      "D loss: 1.094744, acc: 95% // tar acc: 95% // adv loss: 0.317181, aux loss: 1.464261, tar loss: 0.882406\n",
      "=====================\n",
      "Epoch 80/200, Batch 710/938\n",
      "D loss: 1.059962, acc: 98% // tar acc: 96% // adv loss: 0.602159, aux loss: 1.461721, tar loss: 0.942109\n",
      "=====================\n",
      "Epoch 80/200, Batch 760/938\n",
      "D loss: 1.088652, acc: 97% // tar acc: 90% // adv loss: 0.363219, aux loss: 1.476936, tar loss: 0.774877\n",
      "=====================\n",
      "Epoch 80/200, Batch 810/938\n",
      "D loss: 1.126653, acc: 99% // tar acc: 95% // adv loss: 0.434146, aux loss: 1.476863, tar loss: 0.846831\n",
      "=====================\n",
      "Epoch 80/200, Batch 860/938\n",
      "D loss: 1.016615, acc: 98% // tar acc: 98% // adv loss: 0.449078, aux loss: 1.470794, tar loss: 0.876457\n",
      "=====================\n",
      "Epoch 80/200, Batch 910/938\n",
      "D loss: 1.004263, acc: 97% // tar acc: 95% // adv loss: 0.629878, aux loss: 1.491389, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 81/200, Batch 22/938\n",
      "D loss: 1.012751, acc: 97% // tar acc: 96% // adv loss: 0.499595, aux loss: 1.468045, tar loss: 0.825054\n",
      "=====================\n",
      "Epoch 81/200, Batch 72/938\n",
      "D loss: 1.056387, acc: 99% // tar acc: 96% // adv loss: 0.609123, aux loss: 1.467724, tar loss: 0.867096\n",
      "=====================\n",
      "Epoch 81/200, Batch 122/938\n",
      "D loss: 1.020625, acc: 99% // tar acc: 100% // adv loss: 0.638199, aux loss: 1.473788, tar loss: 0.866035\n",
      "=====================\n",
      "Epoch 81/200, Batch 172/938\n",
      "D loss: 0.991096, acc: 98% // tar acc: 90% // adv loss: 0.434985, aux loss: 1.462486, tar loss: 0.847564\n",
      "=====================\n",
      "Epoch 81/200, Batch 222/938\n",
      "D loss: 1.063826, acc: 98% // tar acc: 96% // adv loss: 0.453937, aux loss: 1.499647, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 81/200, Batch 272/938\n",
      "D loss: 0.994115, acc: 98% // tar acc: 95% // adv loss: 0.555547, aux loss: 1.492705, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 81/200, Batch 322/938\n",
      "D loss: 1.108294, acc: 97% // tar acc: 95% // adv loss: 0.355912, aux loss: 1.469491, tar loss: 0.868819\n",
      "=====================\n",
      "Epoch 81/200, Batch 372/938\n",
      "D loss: 1.058954, acc: 99% // tar acc: 98% // adv loss: 0.639884, aux loss: 1.463165, tar loss: 0.873374\n",
      "=====================\n",
      "Epoch 81/200, Batch 422/938\n",
      "D loss: 1.109283, acc: 97% // tar acc: 98% // adv loss: 0.529867, aux loss: 1.471606, tar loss: 0.853462\n",
      "=====================\n",
      "Epoch 81/200, Batch 472/938\n",
      "D loss: 1.006570, acc: 98% // tar acc: 95% // adv loss: 0.468735, aux loss: 1.463811, tar loss: 0.863046\n",
      "=====================\n",
      "Epoch 81/200, Batch 522/938\n",
      "D loss: 1.170791, acc: 98% // tar acc: 98% // adv loss: 0.722268, aux loss: 1.463704, tar loss: 0.849093\n",
      "=====================\n",
      "Epoch 81/200, Batch 572/938\n",
      "D loss: 1.045676, acc: 100% // tar acc: 93% // adv loss: 0.378999, aux loss: 1.464814, tar loss: 0.871629\n",
      "=====================\n",
      "Epoch 81/200, Batch 622/938\n",
      "D loss: 1.027590, acc: 96% // tar acc: 98% // adv loss: 0.566596, aux loss: 1.497733, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 81/200, Batch 672/938\n",
      "D loss: 1.099195, acc: 98% // tar acc: 98% // adv loss: 0.426768, aux loss: 1.471432, tar loss: 0.827059\n",
      "=====================\n",
      "Epoch 81/200, Batch 722/938\n",
      "D loss: 1.069793, acc: 99% // tar acc: 98% // adv loss: 0.343604, aux loss: 1.476820, tar loss: 0.875325\n",
      "=====================\n",
      "Epoch 81/200, Batch 772/938\n",
      "D loss: 1.101154, acc: 97% // tar acc: 98% // adv loss: 0.624761, aux loss: 1.461454, tar loss: 0.852752\n",
      "=====================\n",
      "Epoch 81/200, Batch 822/938\n",
      "D loss: 1.018868, acc: 97% // tar acc: 96% // adv loss: 0.501141, aux loss: 1.464702, tar loss: 0.828408\n",
      "=====================\n",
      "Epoch 81/200, Batch 872/938\n",
      "D loss: 1.011779, acc: 100% // tar acc: 98% // adv loss: 0.517168, aux loss: 1.465621, tar loss: 0.865177\n",
      "=====================\n",
      "Epoch 81/200, Batch 922/938\n",
      "D loss: 1.079484, acc: 99% // tar acc: 96% // adv loss: 0.512469, aux loss: 1.504525, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 82/200, Batch 34/938\n",
      "D loss: 1.071194, acc: 98% // tar acc: 96% // adv loss: 0.424347, aux loss: 1.461785, tar loss: 0.856726\n",
      "=====================\n",
      "Epoch 82/200, Batch 84/938\n",
      "D loss: 1.093543, acc: 96% // tar acc: 98% // adv loss: 0.361724, aux loss: 1.481376, tar loss: 1.332000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 82/200, Batch 134/938\n",
      "D loss: 1.045821, acc: 97% // tar acc: 98% // adv loss: 0.383625, aux loss: 1.463483, tar loss: 0.875377\n",
      "=====================\n",
      "Epoch 82/200, Batch 184/938\n",
      "D loss: 1.068493, acc: 98% // tar acc: 96% // adv loss: 0.462888, aux loss: 1.483635, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 82/200, Batch 234/938\n",
      "D loss: 1.039213, acc: 99% // tar acc: 100% // adv loss: 0.532324, aux loss: 1.462180, tar loss: 0.911418\n",
      "=====================\n",
      "Epoch 82/200, Batch 284/938\n",
      "D loss: 1.054452, acc: 98% // tar acc: 98% // adv loss: 0.746078, aux loss: 1.461495, tar loss: 0.876442\n",
      "=====================\n",
      "Epoch 82/200, Batch 334/938\n",
      "D loss: 1.024646, acc: 95% // tar acc: 92% // adv loss: 0.351870, aux loss: 1.500110, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 82/200, Batch 384/938\n",
      "D loss: 0.989061, acc: 98% // tar acc: 98% // adv loss: 0.548823, aux loss: 1.470471, tar loss: 0.903579\n",
      "=====================\n",
      "Epoch 82/200, Batch 434/938\n",
      "D loss: 1.045841, acc: 99% // tar acc: 90% // adv loss: 0.404647, aux loss: 1.486503, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 82/200, Batch 484/938\n",
      "D loss: 1.031167, acc: 100% // tar acc: 95% // adv loss: 0.380357, aux loss: 1.471756, tar loss: 0.849235\n",
      "=====================\n",
      "Epoch 82/200, Batch 534/938\n",
      "D loss: 1.045431, acc: 99% // tar acc: 98% // adv loss: 0.436887, aux loss: 1.461393, tar loss: 0.909154\n",
      "=====================\n",
      "Epoch 82/200, Batch 584/938\n",
      "D loss: 1.074211, acc: 96% // tar acc: 95% // adv loss: 0.602329, aux loss: 1.475822, tar loss: 0.824872\n",
      "=====================\n",
      "Epoch 82/200, Batch 634/938\n",
      "D loss: 1.062829, acc: 97% // tar acc: 96% // adv loss: 0.345293, aux loss: 1.474637, tar loss: 0.879221\n",
      "=====================\n",
      "Epoch 82/200, Batch 684/938\n",
      "D loss: 1.129769, acc: 99% // tar acc: 93% // adv loss: 0.430505, aux loss: 1.479584, tar loss: 0.879835\n",
      "=====================\n",
      "Epoch 82/200, Batch 734/938\n",
      "D loss: 0.998548, acc: 99% // tar acc: 93% // adv loss: 0.503236, aux loss: 1.466202, tar loss: 0.835433\n",
      "=====================\n",
      "Epoch 82/200, Batch 784/938\n",
      "D loss: 1.082387, acc: 96% // tar acc: 96% // adv loss: 0.683583, aux loss: 1.467620, tar loss: 0.843212\n",
      "=====================\n",
      "Epoch 82/200, Batch 834/938\n",
      "D loss: 1.138549, acc: 96% // tar acc: 98% // adv loss: 0.471907, aux loss: 1.477059, tar loss: 0.861261\n",
      "=====================\n",
      "Epoch 82/200, Batch 884/938\n",
      "D loss: 1.050472, acc: 95% // tar acc: 100% // adv loss: 0.603753, aux loss: 1.461251, tar loss: 0.853779\n",
      "=====================\n",
      "Epoch 82/200, Batch 934/938\n",
      "D loss: 1.047853, acc: 96% // tar acc: 96% // adv loss: 0.427467, aux loss: 1.461627, tar loss: 0.823606\n",
      "=====================\n",
      "Epoch 83/200, Batch 46/938\n",
      "D loss: 1.100625, acc: 98% // tar acc: 96% // adv loss: 0.367608, aux loss: 1.463834, tar loss: 0.895532\n",
      "=====================\n",
      "Epoch 83/200, Batch 96/938\n",
      "D loss: 1.054014, acc: 96% // tar acc: 98% // adv loss: 0.307270, aux loss: 1.461216, tar loss: 0.899081\n",
      "=====================\n",
      "Epoch 83/200, Batch 146/938\n",
      "D loss: 0.997804, acc: 97% // tar acc: 95% // adv loss: 0.506408, aux loss: 1.472695, tar loss: 0.846332\n",
      "=====================\n",
      "Epoch 83/200, Batch 196/938\n",
      "D loss: 1.048767, acc: 93% // tar acc: 98% // adv loss: 0.409866, aux loss: 1.467335, tar loss: 0.842328\n",
      "=====================\n",
      "Epoch 83/200, Batch 246/938\n",
      "D loss: 1.135787, acc: 97% // tar acc: 92% // adv loss: 0.571297, aux loss: 1.476146, tar loss: 0.820175\n",
      "=====================\n",
      "Epoch 83/200, Batch 296/938\n",
      "D loss: 1.003229, acc: 99% // tar acc: 92% // adv loss: 0.444377, aux loss: 1.466489, tar loss: 0.886524\n",
      "=====================\n",
      "Epoch 83/200, Batch 346/938\n",
      "D loss: 1.058815, acc: 99% // tar acc: 98% // adv loss: 0.454584, aux loss: 1.463985, tar loss: 0.892806\n",
      "=====================\n",
      "Epoch 83/200, Batch 396/938\n",
      "D loss: 1.035303, acc: 97% // tar acc: 93% // adv loss: 0.529471, aux loss: 1.477473, tar loss: 0.870632\n",
      "=====================\n",
      "Epoch 83/200, Batch 446/938\n",
      "D loss: 1.034385, acc: 98% // tar acc: 96% // adv loss: 0.631944, aux loss: 1.463493, tar loss: 0.839710\n",
      "=====================\n",
      "Epoch 83/200, Batch 496/938\n",
      "D loss: 1.090460, acc: 98% // tar acc: 95% // adv loss: 0.423991, aux loss: 1.470555, tar loss: 0.855658\n",
      "=====================\n",
      "Epoch 83/200, Batch 546/938\n",
      "D loss: 1.119885, acc: 97% // tar acc: 98% // adv loss: 0.365509, aux loss: 1.462935, tar loss: 0.909998\n",
      "=====================\n",
      "Epoch 83/200, Batch 596/938\n",
      "D loss: 1.127123, acc: 99% // tar acc: 100% // adv loss: 0.310759, aux loss: 1.475891, tar loss: 0.818157\n",
      "=====================\n",
      "Epoch 83/200, Batch 646/938\n",
      "D loss: 1.072745, acc: 99% // tar acc: 93% // adv loss: 0.488473, aux loss: 1.469102, tar loss: 0.866086\n",
      "=====================\n",
      "Epoch 83/200, Batch 696/938\n",
      "D loss: 1.136615, acc: 97% // tar acc: 100% // adv loss: 0.386720, aux loss: 1.477432, tar loss: 0.944162\n",
      "=====================\n",
      "Epoch 83/200, Batch 746/938\n",
      "D loss: 0.987610, acc: 98% // tar acc: 98% // adv loss: 0.408907, aux loss: 1.461658, tar loss: 0.894050\n",
      "=====================\n",
      "Epoch 83/200, Batch 796/938\n",
      "D loss: 1.042516, acc: 97% // tar acc: 93% // adv loss: 0.256236, aux loss: 1.476521, tar loss: 0.846215\n",
      "=====================\n",
      "Epoch 83/200, Batch 846/938\n",
      "D loss: 1.015530, acc: 99% // tar acc: 98% // adv loss: 0.626003, aux loss: 1.472436, tar loss: 0.849344\n",
      "=====================\n",
      "Epoch 83/200, Batch 896/938\n",
      "D loss: 1.032895, acc: 98% // tar acc: 89% // adv loss: 0.479035, aux loss: 1.498212, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 84/200, Batch 8/938\n",
      "D loss: 1.160763, acc: 95% // tar acc: 98% // adv loss: 0.443027, aux loss: 1.461189, tar loss: 0.877768\n",
      "=====================\n",
      "Epoch 84/200, Batch 58/938\n",
      "D loss: 1.086160, acc: 97% // tar acc: 100% // adv loss: 0.328476, aux loss: 1.481705, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 84/200, Batch 108/938\n",
      "D loss: 1.056142, acc: 98% // tar acc: 95% // adv loss: 0.476888, aux loss: 1.476878, tar loss: 0.813983\n",
      "=====================\n",
      "Epoch 84/200, Batch 158/938\n",
      "D loss: 1.037383, acc: 100% // tar acc: 93% // adv loss: 0.476577, aux loss: 1.492638, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 84/200, Batch 208/938\n",
      "D loss: 1.023075, acc: 97% // tar acc: 98% // adv loss: 0.561111, aux loss: 1.462364, tar loss: 0.843398\n",
      "=====================\n",
      "Epoch 84/200, Batch 258/938\n",
      "D loss: 1.031329, acc: 99% // tar acc: 96% // adv loss: 0.606609, aux loss: 1.472949, tar loss: 0.888770\n",
      "=====================\n",
      "Epoch 84/200, Batch 308/938\n",
      "D loss: 0.986265, acc: 100% // tar acc: 93% // adv loss: 0.386311, aux loss: 1.478940, tar loss: 0.867034\n",
      "=====================\n",
      "Epoch 84/200, Batch 358/938\n",
      "D loss: 1.085648, acc: 98% // tar acc: 95% // adv loss: 0.672450, aux loss: 1.461287, tar loss: 0.907776\n",
      "=====================\n",
      "Epoch 84/200, Batch 408/938\n",
      "D loss: 1.001391, acc: 98% // tar acc: 98% // adv loss: 0.353984, aux loss: 1.461973, tar loss: 0.832645\n",
      "=====================\n",
      "Epoch 84/200, Batch 458/938\n",
      "D loss: 1.038374, acc: 98% // tar acc: 95% // adv loss: 0.554232, aux loss: 1.461873, tar loss: 0.827088\n",
      "=====================\n",
      "Epoch 84/200, Batch 508/938\n",
      "D loss: 1.077327, acc: 98% // tar acc: 98% // adv loss: 0.341230, aux loss: 1.465191, tar loss: 0.899234\n",
      "=====================\n",
      "Epoch 84/200, Batch 558/938\n",
      "D loss: 1.055406, acc: 99% // tar acc: 98% // adv loss: 0.764608, aux loss: 1.463454, tar loss: 0.816182\n",
      "=====================\n",
      "Epoch 84/200, Batch 608/938\n",
      "D loss: 1.080383, acc: 100% // tar acc: 96% // adv loss: 0.537524, aux loss: 1.480623, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 84/200, Batch 658/938\n",
      "D loss: 1.063883, acc: 96% // tar acc: 98% // adv loss: 0.448734, aux loss: 1.490987, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 84/200, Batch 708/938\n",
      "D loss: 1.031110, acc: 100% // tar acc: 96% // adv loss: 0.659679, aux loss: 1.476381, tar loss: 0.831037\n",
      "=====================\n",
      "Epoch 84/200, Batch 758/938\n",
      "D loss: 1.092002, acc: 97% // tar acc: 96% // adv loss: 0.526983, aux loss: 1.461160, tar loss: 0.862893\n",
      "=====================\n",
      "Epoch 84/200, Batch 808/938\n",
      "D loss: 1.077257, acc: 98% // tar acc: 93% // adv loss: 0.344414, aux loss: 1.465180, tar loss: 0.878889\n",
      "=====================\n",
      "Epoch 84/200, Batch 858/938\n",
      "D loss: 1.044468, acc: 98% // tar acc: 93% // adv loss: 0.409081, aux loss: 1.464984, tar loss: 0.848447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 84/200, Batch 908/938\n",
      "D loss: 1.022585, acc: 99% // tar acc: 93% // adv loss: 0.465222, aux loss: 1.464733, tar loss: 0.841346\n",
      "=====================\n",
      "Epoch 85/200, Batch 20/938\n",
      "D loss: 1.116567, acc: 97% // tar acc: 92% // adv loss: 0.455468, aux loss: 1.461788, tar loss: 0.781871\n",
      "=====================\n",
      "Epoch 85/200, Batch 70/938\n",
      "D loss: 1.024972, acc: 98% // tar acc: 96% // adv loss: 0.627084, aux loss: 1.470563, tar loss: 0.878289\n",
      "=====================\n",
      "Epoch 85/200, Batch 120/938\n",
      "D loss: 1.084152, acc: 96% // tar acc: 92% // adv loss: 0.454336, aux loss: 1.468868, tar loss: 0.849919\n",
      "=====================\n",
      "Epoch 85/200, Batch 170/938\n",
      "D loss: 1.030157, acc: 97% // tar acc: 98% // adv loss: 0.532312, aux loss: 1.461618, tar loss: 0.860659\n",
      "=====================\n",
      "Epoch 85/200, Batch 220/938\n",
      "D loss: 0.983642, acc: 99% // tar acc: 95% // adv loss: 0.598240, aux loss: 1.461239, tar loss: 0.866163\n",
      "=====================\n",
      "Epoch 85/200, Batch 270/938\n",
      "D loss: 1.078098, acc: 96% // tar acc: 93% // adv loss: 0.393485, aux loss: 1.461358, tar loss: 0.831536\n",
      "=====================\n",
      "Epoch 85/200, Batch 320/938\n",
      "D loss: 1.035869, acc: 98% // tar acc: 98% // adv loss: 0.637963, aux loss: 1.461518, tar loss: 0.818321\n",
      "=====================\n",
      "Epoch 85/200, Batch 370/938\n",
      "D loss: 0.977242, acc: 96% // tar acc: 95% // adv loss: 0.414383, aux loss: 1.468801, tar loss: 0.824490\n",
      "=====================\n",
      "Epoch 85/200, Batch 420/938\n",
      "D loss: 0.959434, acc: 97% // tar acc: 96% // adv loss: 0.565797, aux loss: 1.477012, tar loss: 0.864390\n",
      "=====================\n",
      "Epoch 85/200, Batch 470/938\n",
      "D loss: 1.012630, acc: 99% // tar acc: 96% // adv loss: 0.587534, aux loss: 1.463723, tar loss: 0.855092\n",
      "=====================\n",
      "Epoch 85/200, Batch 520/938\n",
      "D loss: 1.027758, acc: 97% // tar acc: 90% // adv loss: 0.238934, aux loss: 1.497515, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 85/200, Batch 570/938\n",
      "D loss: 1.089904, acc: 98% // tar acc: 100% // adv loss: 0.511954, aux loss: 1.468890, tar loss: 0.878208\n",
      "=====================\n",
      "Epoch 85/200, Batch 620/938\n",
      "D loss: 1.098045, acc: 99% // tar acc: 100% // adv loss: 0.813028, aux loss: 1.463053, tar loss: 0.902431\n",
      "=====================\n",
      "Epoch 85/200, Batch 670/938\n",
      "D loss: 1.111142, acc: 97% // tar acc: 98% // adv loss: 0.440386, aux loss: 1.461439, tar loss: 0.899618\n",
      "=====================\n",
      "Epoch 85/200, Batch 720/938\n",
      "D loss: 1.092637, acc: 98% // tar acc: 98% // adv loss: 0.524007, aux loss: 1.462092, tar loss: 0.867406\n",
      "=====================\n",
      "Epoch 85/200, Batch 770/938\n",
      "D loss: 1.008752, acc: 96% // tar acc: 93% // adv loss: 0.303885, aux loss: 1.466703, tar loss: 0.851584\n",
      "=====================\n",
      "Epoch 85/200, Batch 820/938\n",
      "D loss: 1.096752, acc: 98% // tar acc: 92% // adv loss: 0.529203, aux loss: 1.464644, tar loss: 0.871011\n",
      "=====================\n",
      "Epoch 85/200, Batch 870/938\n",
      "D loss: 1.031464, acc: 99% // tar acc: 90% // adv loss: 0.594783, aux loss: 1.481641, tar loss: 1.332000\n",
      "=====================\n",
      "Epoch 85/200, Batch 920/938\n",
      "D loss: 1.005761, acc: 98% // tar acc: 100% // adv loss: 0.449991, aux loss: 1.461185, tar loss: 0.848087\n",
      "=====================\n",
      "Epoch 86/200, Batch 32/938\n",
      "D loss: 1.019517, acc: 100% // tar acc: 98% // adv loss: 0.346092, aux loss: 1.461190, tar loss: 0.876019\n",
      "=====================\n",
      "Epoch 86/200, Batch 82/938\n",
      "D loss: 1.003309, acc: 96% // tar acc: 95% // adv loss: 0.491773, aux loss: 1.479276, tar loss: 0.871655\n",
      "=====================\n",
      "Epoch 86/200, Batch 132/938\n",
      "D loss: 1.022103, acc: 98% // tar acc: 96% // adv loss: 0.325655, aux loss: 1.495231, tar loss: 1.332000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-4a0e6491e5ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/AdversarialRobustness/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/AdversarialRobustness/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/AdversarialRobustness/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/AdversarialRobustness/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/AdversarialRobustness/venv/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/AdversarialRobustness/venv/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/AdversarialRobustness/venv/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/AdversarialRobustness/venv/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;31m# PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        \n",
    "        batch_size = imgs.shape[0]\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity, pred_label = discriminator(gen_imgs)\n",
    "        target_classifier_pred_label = target_classifier(gen_imgs)\n",
    "        \n",
    "        t_acc = np.mean(np.argmax(target_classifier_pred_label.data.cpu().numpy(), axis=1) == gen_labels.data.cpu().numpy())\n",
    "        \n",
    "        adv_loss = adv_loss_coeff * adversarial_loss(validity, valid)\n",
    "        aux_loss = aux_loss_coeff * auxiliary_loss(pred_label, gen_labels)\n",
    "        tar_loss = tar_loss_coeff * get_target_loss(adv_loss, aux_loss, target_classifier_pred_label, gen_labels)\n",
    "        g_loss = adv_loss + aux_loss + tar_loss\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_pred, real_aux = discriminator(real_imgs)\n",
    "        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "        # Loss for fake images\n",
    "        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = d_real_loss_coeff * d_real_loss + d_fake_loss_coeff * d_fake_loss\n",
    "\n",
    "        # Calculate discriminator accuracy\n",
    "        pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "        gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "        d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_image(n_row=10, batches_done=batches_done)\n",
    "            print(\n",
    "                \"=====================\\nEpoch %d/%d, Batch %d/%d\\nD loss: %f, acc: %d%% // tar acc: %d%% // adv loss: %f, aux loss: %f, tar loss: %f\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), 100 * d_acc, 100 * t_acc, adv_loss.item(), aux_loss.item(), tar_loss.item())\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves weights\n",
    "torch.save(generator.state_dict(), \"../models/Success1_G\")\n",
    "torch.save(discriminator.state_dict(), \"../models/Success1_D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
    "\n",
    "# Generate a batch of images\n",
    "gen_imgs = generator(z, gen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Gen Label: 3; LeNet Label: 2')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots a sample\n",
    "sample_idx = 26\n",
    "print(gen_labels[sample_idx])\n",
    "plt.imshow(gen_imgs[sample_idx][0].cpu().detach().numpy(), cmap='gray', interpolation='none')\n",
    "plt.title(\"Gen Label: \" + str(gen_labels.cpu().detach().numpy()[sample_idx]) + \"; LeNet Label: \" + str(np.argmax(pred_labels.data.cpu().numpy()[sample_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds target classification for sample\n",
    "pred_labels = target_classifier(gen_imgs)\n",
    "np.argmax(pred_labels.data.cpu().numpy()[sample_idx])\n",
    "#pred_labels.data.cpu().numpy()[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([23, 24, 26, 43, 45, 46]),)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds indexes that have adversarial examples\n",
    "t_acc = np.argmax(pred_labels.data.cpu().numpy(), axis=1) == gen_labels.data.cpu().numpy()\n",
    "np.where(t_acc == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAUJUlEQVR4nO3df/BVdZ3H8ecLBFRQFClEQjC1zdoxUjRaiaHJynXXqBl1tR+LlqGWlZM1sbaN1lLr9ENzt9EWRwf7YdZWrNpmxbIpmq2J5c/8kRAu4FeIiJCyBHnvH+fgXr5+7+d8v/e338/rMXOHe8/7/Pjcw319z7nnc885igjMbPgb0e0GmFlnOOxmmXDYzTLhsJtlwmE3y4TDbpYJh70LJC2RtKjT0xpICkmHdXraXjDswi7pNEl3SvqDpI3l8/dJUhuWdYuks1o933aQdM1QP6yNfrjL6e6XNKJm2CJJSwY5fXK9SppeLmOPobatUyRNkXSDpM2S1kk6p9ttGlZhl3QBcDnwOeBAYBJwDnAcMLqLTesqSbOBQzu82IOA0zq8zF7yNeDXFJ/BvwE+I+n13WzQsAm7pPHAp4D3RcS3I+KpKPwiIt4REX8uxxsj6fOS/lfSBklflrRXWZtb/hW+oNwr6JN0ZoPt+XdJT0r6vaQVkl7Zb5SJkpZJekrSrZKm1Uz78rK2WdIjkk5tcLVQbv3+FfjAALUrJF3R4HzfLekhSb+T9MPa9pc+C3yy3tZX0ixJd0jaIuleSXPL4Z8GXgd8SdI2SV8aYruOlfTTcr59kr4kqf8f+hMlrZa0SdLn+u2BVL2vwbRhHDAX+HREbI+Ie4FvA+8e6rxaKiKGxQM4AdgB7FEx3mXAjcAEYB/gJuCfy9rcch6fAkYBJwJ/BPavM69bgLPq1N5dzn8M8EXgnpraEuApYE5Zvxy4vayNBdYCZwJ7AK8GNgGvqJl2Uc28tgCzE+/3o8Dl5fMADhvCOh1wfGAe8BhwRNnGfwTu6Dfd4cDdu9YPsAhYUj6fAvy2XL8jgDeWr19UtV7L+vRyGc/7vwaOBmaV7ZoOPASc369tPy7//w8GHq1p42De12Hl87cD99Vp3z7luC+uGXYV8IuuZqSbC2/pG4F3Ak/2G3ZHGYany2AJ+ANwaM04rwV+XT6fW467R019IzCrzjKTH8qa8fYr//PHl6+XANfX1McBzwJTgb8Dbus3/b8BF9VMu6hqmeW4U8sP767ltirsNwPvqXk9guKP4rTa6cowP07xFao27B8Dvtpvnj8E5g9mvabCPsC45wNL+72nE2pevw9YPpT3Nch1dzvFHtWewFHAZuCRbmZk2OzGU2wZJtbuNkbEX0XEfmVtBPAiYG/g7nI3bwvwg3L4c/OJiB01r/9IEcZBkzRS0iWSVknaCqwpSxNrRltb085tFB+Gg4BpwGt2ta9s4zsojkEM1ReBT0XE7xuYNmUacHlN+zZT/CGdUjtSRHwfWAecPcD0p/R7j7OByc02TNLLJH2v/Aq1FfgMu693qFn3FH+MDhrK+xqkdwCHlMu6kuI7/LoG5tMywynsPwX+TLErVs8mii33KyNiv/IxPiKGFOZBeHvZjuOB8RRbIig+OLtM3fWk/I43AXiC4sNxa0379ouIcRFxbgPteAPwufKD/2Q57KeS3t7AvGqtBc7u18a9IuKOAcb9OHAhxR/Z2um/2m/6sRFxSVlv5lTMK4GHgcMjYt9y2f17YqbWPD+YYr0P9X0lRcTjEfG3EfGiiHgNxR+cnw353bTQsAl7RGwBPglcIelkSftIGiFpBsX3YCJiJ8V3p8skvRie6yJ5cxOL3kPSnjWPURTf2f5MsUexN8XWpb8TJc0uDx79E/A/EbEW+B7wMknvkjSqfBwj6YgG2vYy4FXAjPIBcBKwFJ7rs19SMY/R/d7fSODLwD/sOugoabykUwaaOCJuAR4A5tcM/hpwkqQ3l3tBe5YHR19S1jcALx3E+xvTr20jKNb9VmCbpJcDA/2R/Kik/SVNBT4EfLMcPuj3VUXSEeVncLSkdwJvAi5tZF4t083vEO14UOw+/Yxi9/s3wJ3AAmB0Wd+TInyrKT4UDwEfLGtzgXX95rcGOL7Osm6h2ArVPr5Gsdt/A8VBuMeBv2f3gztLKD5Yy4BtwArgkJr5/gXwn2X7fwv8NzCjZtraA3TbgNcNct3s9p0TWA68t2L8/o9dB7PeBdxfrsO1wDWJ5bymHLak37BbKXaVf1O+34PL2mspDpz9DviXAdo1vU7bjqc4NvNwuV5uozjYenu/tn2w/P//LfAFYGRNfVDvi+Jz9mBi3Z1fvq8/UHx/n9ntbKhsmGWm3KO4FzgyIrZ3uz3Wfg67WSaGzXd2M0tz2M0y4bCbZaKjZw1J8gECszaLiAHP8Gxqyy7phPJEjcckLWxmXmbWXg0fjS9/XPEoxUkM64C7gNMj4peJabxlN2uzdmzZjwUei4jVEfEMcD3pn6qaWRc1E/Yp7H5CwToGOGFA0gJJKyWtbGJZZtakth+gi4jFwGLwbrxZNzWzZV/P7mcPvaQcZmY9qJmw3wUcLumQ8nfWp1FcAcbMelDDu/ERsUPSeRRXGBlJcXbQgy1rmZm1VEdPhPF3drP2a8uPaszshcNhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmGr5ls1m7jRo1Klnfvn17h1oyPDQVdklrgKeAZ4EdETGzFY0ys9ZrxZb99RGxqQXzMbM28nd2s0w0G/YAfiTpbkkLBhpB0gJJKyWtbHJZZtYERUTjE0tTImK9pBcDy4APRMSKxPiNL8yy4wN0jYkIDTS8qS17RKwv/90ILAWObWZ+ZtY+DYdd0lhJ++x6DrwJeKBVDTOz1mrmaPwkYKmkXfO5LiJ+0JJWWc/Yc889k/XRo0cn6x/+8Ifr1hYuXJicduTIkcn6ddddl6yfeeaZdWs7d+5MTjscNRz2iFgNvKqFbTGzNnLXm1kmHHazTDjsZplw2M0y4bCbZcKnuGbuuOOOS9aXLl2arO+7777JeupXblXddlWmTZuWrI8ZM6Zu7emnn25q2c1KdStW/aq10W5Db9nNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0y4n30YSJ2G+pa3vCU57aJFi5L1Aw44IFnfsmVLsj5u3Li6tar+5Kp5f+QjH0nWu92XnvLss892fJnesptlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXA/+wtA1eWcV6yoexMejjnmmOS0VX3dq1atStZvvvnmZP2ss86qW6u648vmzZuT9a1btybrtjtv2c0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTLifvQdMmDAhWb/pppuS9aOOOqpureq86U984hPJ+qWXXpqs77XXXsn6ueeeW7dW3u67rp/85CfJ+qOPPpqs2+4qt+ySrpG0UdIDNcMmSFom6Vflv/u3t5lm1qzB7MYvAU7oN2whsDwiDgeWl6/NrIdVhj0iVgD9f7c4D7i2fH4t8NYWt8vMWqzR7+yTIqKvfP4kMKneiJIWAAsaXI6ZtUjTB+giIiTVPZsiIhYDiwFS45lZezXa9bZB0mSA8t+NrWuSmbVDo2G/EZhfPp8P3NCa5phZu6jqfGZJ3wDmAhOBDcBFwH8A3wIOBh4HTo2I9MnH5LsbX9WfvHLlymQ91Y8O6eurn3zyyclply9fnqxX2WOP9DfBP/3pTw3Pe86cOcn6HXfc0fC8h7OIGPADV/mdPSJOr1N6Q1MtMrOO8s9lzTLhsJtlwmE3y4TDbpYJh90sEz7FtQOuu+66ZL2qa63qNNXZs2fXrT344IPJaZt12GGHJeup2ybvvffeyWmruiRtaLxlN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4X72FjjllFOS9VNPPTVZrzrNOHXbY2h/X3rK9u3bk/XU7aZTp+YCPPPMMw21aTCqTjseMSK9HayafseOHUNuU7t5y26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcL97C1w3nnnJetVfbZVfbKbN6ev0p2a/+jRo5PTVp0rf+ihhybrV199dbKeatvYsWOT0x599NHJ+urVq5P1efPm1a0deeSRyWnvuuuuZL3qdtJVvz/o6+tL1tvBW3azTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBPuZ2+Bbdu2tXX+S5cuTdar+vFTdu7c2fC0zS676jcAVdeNr7oOQOp8+DvvvDM57fXXX5+sP/HEE8n6C/J8dknXSNoo6YGaYRdLWi/pnvJxYnubaWbNGsyf5SXACQMMvywiZpSP77e2WWbWapVhj4gVQPr3mmbW85o5QHeepPvK3fz9640kaYGklZJ84y6zLmo07FcChwIzgD7gC/VGjIjFETEzImY2uCwza4GGwh4RGyLi2YjYCVwFHNvaZplZqzUUdkmTa16+DXig3rhm1htU1Vcp6RvAXGAisAG4qHw9AwhgDXB2RFSeoCspvbAXqAkTJiTrt956a7JedZ/y6dOnJ+vN9HW3W+rzVbVeUvd2h+q+8EceeaRu7eGHH05Ou3Xr1mS9KjfdFBEDXtS+8kc1EXH6AIPTVywws57Tu5sEM2sph90sEw67WSYcdrNMOOxmmajsemvpwoZp11uzqk71rDoNNfV/OH78+OS0VZc83rBhQ7I+ZsyYZD3VPXbGGWckp61qmw2sXtebt+xmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSbcz565ZcuWJetz5sxJ1qtOr02dvut+9PZwP7tZ5hx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgnfsnmYqzpX/sADD2xq+qpLMrsvvXd4y26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZaKyn13SVOArwCSKWzQvjojLJU0AvglMp7ht86kR8bv2NdUacdJJJyXrRxxxRLJedc36iy66aMhtsu4YzJZ9B3BBRLwCmAW8X9IrgIXA8og4HFhevjazHlUZ9ojoi4ifl8+fAh4CpgDzgGvL0a4F3tquRppZ84b0nV3SdODVwJ3ApIjoK0tPUuzmm1mPGvRv4yWNA74DnB8RW6X/v8xVRES968tJWgAsaLahZtacQW3ZJY2iCPrXI+K75eANkiaX9cnAxoGmjYjFETEzIma2osFm1pjKsKvYhF8NPBQRl9aUbgTml8/nAze0vnlm1iqVl5KWNBu4Dbgf2NUPcyHF9/ZvAQcDj1N0vW2umJcvJd0Gqcs5r1+/Pjlt1Smua9asSdaPP/74ZH3VqlXJurVevUtJV35nj4jbgQEnBt7QTKPMrHP8CzqzTDjsZplw2M0y4bCbZcJhN8uEw26WCV9KehhI3VZ54sSJyWmrTmHdtGlTsl7Vj2+9w1t2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTleezt3RhPp+9Lfr6+urWqs5Xr7ql8ubNyUsUcNBBByXrVf341nr1zmf3lt0sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TPZ38BmDVrVrI+aVLjt9mr6me/4oorknX3o79weMtulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2ViMPdnnwp8BZgEBLA4Ii6XdDHwXuA35agXRsT3K+bl89nb4Kqrrqpbqzrf/JxzzknW165d21CbrHsavj87sAO4ICJ+Lmkf4G5Jy8raZRHx+VY10szapzLsEdEH9JXPn5L0EDCl3Q0zs9Ya0nd2SdOBVwN3loPOk3SfpGsk7V9nmgWSVkpa2VRLzawpgw67pHHAd4DzI2IrcCVwKDCDYsv/hYGmi4jFETEzIma2oL1m1qBBhV3SKIqgfz0ivgsQERsi4tmI2AlcBRzbvmaaWbMqwy5JwNXAQxFxac3wyTWjvQ14oPXNM7NWGUzX22zgNuB+YNf5jBcCp1PswgewBji7PJiXmpe73szarF7Xm68bbzbM+LrxZplz2M0y4bCbZcJhN8uEw26WCYfdLBO+lLTZC0zxO7eBpbrSvWU3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLR6X72TcDjNa8nlsN6Ua+2rVfbBW5bo4bUtorT0qfVK3T0fPbnLVxa2avXpuvVtvVqu8Bta1Sn2ubdeLNMOOxmmeh22Bd3efkpvdq2Xm0XuG2N6kjbuvqd3cw6p9tbdjPrEIfdLBNdCbukEyQ9IukxSQu70YZ6JK2RdL+ke7p9f7ryHnobJT1QM2yCpGWSflX+O+A99rrUtoslrS/X3T2STuxS26ZK+rGkX0p6UNKHyuFdXXeJdnVkvXX8O7ukkcCjwBuBdcBdwOkR8cuONqQOSWuAmRHR9R9gSJoDbAO+EhF/WQ77LLA5Ii4p/1DuHxEf65G2XQxs6/ZtvMu7FU2uvc048FbgDLq47hLtOpUOrLdubNmPBR6LiNUR8QxwPTCvC+3oeRGxAtjcb/A84Nry+bUUH5aOq9O2nhARfRHx8/L5U8Cu24x3dd0l2tUR3Qj7FGBtzet19Nb93gP4kaS7JS3odmMGMKnmNltPApO62ZgBVN7Gu5P63Wa8Z9ZdI7c/b5YP0D3f7Ig4Cvhr4P3l7mpPiuI7WC/1nQ7qNt6dMsBtxp/TzXXX6O3Pm9WNsK8Hpta8fkk5rCdExPry343AUnrvVtQbdt1Bt/x3Y5fb85xeuo33QLcZpwfWXTdvf96NsN8FHC7pEEmjgdOAG7vQjueRNLY8cIKkscCb6L1bUd8IzC+fzwdu6GJbdtMrt/Gud5txurzuun7784jo+AM4keKI/Crg491oQ512vRS4t3w82O22Ad+g2K3bTnFs4z3AAcBy4FfAfwETeqhtX6W4tfd9FMGa3KW2zabYRb8PuKd8nNjtdZdoV0fWm38ua5YJH6Azy4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLxf8nzBaBKDgZfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen = gen_imgs.cpu().detach().numpy()\n",
    "preds = pred_labels.data.cpu().numpy()\n",
    "true = gen_labels.data.cpu().numpy()\n",
    "for i in range(len(true)):\n",
    "    if np.argmax(preds[i]) != true[i]:\n",
    "        plt.title(\"Gen Label: \" + str(true[i]) + \"; LeNet Label: \" + str(np.argmax(preds[i])))\n",
    "        plt.imshow(gen[i][0], cmap='gray')\n",
    "        plt.savefig(\"../images/FirstRun/AdversarialExamples/\" + str(i) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra saving\n",
    "torch.save(gen_imgs, \"../models/Success1_GenSample\")\n",
    "torch.save(gen_labels, \"../models/Success1_GenLabels\")\n",
    "torch.save(pred_labels, \"../models/Success1_LeNetLabels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
