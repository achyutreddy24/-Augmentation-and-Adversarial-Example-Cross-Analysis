{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code:\n",
    "# https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/acgan/acgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# Models\n",
    "from torchvision.models import vgg19\n",
    "from sys import path\n",
    "path.append(\"../utils\")\n",
    "from models import LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "cuda = True\n",
    "\n",
    "n_epochs=200\n",
    "batch_size=64\n",
    "lr=0.0002\n",
    "b1=0.5\n",
    "b2=0.999\n",
    "latent_dim=100\n",
    "n_classes=10\n",
    "img_size=28\n",
    "channels=1\n",
    "sample_interval=500\n",
    "\n",
    "d_real_loss_coeff = 0.6\n",
    "d_fake_loss_coeff = 0.4\n",
    "\n",
    "adv_loss_coeff = 1\n",
    "aux_loss_coeff = 1\n",
    "tar_loss_coeff = .05\n",
    "\n",
    "tar_loss_default = 22.2  # This is equal to the max possible tar_loss value\n",
    "\n",
    "# target classifier conditional constants\n",
    "adv_loss_threshold = 0.9\n",
    "aux_loss_threshold = 1.48\n",
    "\n",
    "lenet5_state_path = \"../utils/models/trained_lenet5.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions for fixed target classifier\n",
    "\n",
    "def load_pytorch_model():\n",
    "    # Load model from torchvision.models (NOTE: VVG19 IS NOT FOR MNIST, DON'T USE)\n",
    "    model = vgg19(pretrained=True)\n",
    "    model.eval()\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    \n",
    "    for p in model.features.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.classifier.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_LeNet5():\n",
    "    net = LeNet5()\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    # remove map location = cpu if using cuda\n",
    "    net.load_state_dict(torch.load(lenet5_state_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    # set model to eval mode so nothing is changed\n",
    "    net.eval()\n",
    "    \n",
    "    return net\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(n_classes, latent_dim)\n",
    "\n",
    "        self.init_size = img_size // 4  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = 2#img_size // 2 ** 4\n",
    "\n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, n_classes), nn.Softmax())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "\n",
    "        return validity, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "adversarial_loss = nn.BCELoss()\n",
    "auxiliary_loss = nn.CrossEntropyLoss()\n",
    "target_classifier_loss = nn.CrossEntropyLoss() # negate target classifier output when passing to this loss function\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Load target classifier\n",
    "\n",
    "target_classifier = load_LeNet5()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()\n",
    "    target_classifier = target_classifier.cuda()\n",
    "    target_classifier_loss = target_classifier_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs(\"../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, \"../images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def shouldCalculateTargetLoss(validity, pred_label, label):\n",
    "    # Only works if batch size is 1, so just uses the first value\n",
    "    return (validity > threshold_valid and  # validity is above threshold\n",
    "        pred_label[label] == np.amax(pred_label) and  # classification is correct\n",
    "        pred_label[label] > threshold_classification)  # classification confidence is above threshold\n",
    "\n",
    "def get_target_loss(validity, pred_labels, label, target_classification):\n",
    "    # Apply conditional loss to each output individually\n",
    "    for i in range(target_classification.size(0)):\n",
    "        if shouldCalculateTargetLoss(validity[i], pred_labels[i], label[i]):\n",
    "            target_classification[i] *= -1\n",
    "            \n",
    "            \n",
    "    loss = target_classifier_loss(target_classification, gen_labels)\n",
    "    \n",
    "    print(\"Count of loss including target loss:\", torch.sum(loss_mult))\n",
    "    \n",
    "    return loss * loss_mult\n",
    "'''\n",
    "\n",
    "\n",
    "def get_target_loss(adv_loss, aux_loss, target_classification, true_classification):\n",
    "    #print(target_classification[0])\n",
    "    if (adv_loss < adv_loss_threshold and aux_loss < aux_loss_threshold):\n",
    "        return target_classifier_loss(target_classification * -1, true_classification)\n",
    "    return Variable(FloatTensor([tar_loss_default]))\n",
    "\n",
    "def test_attack(size):\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (size, latent_dim))))\n",
    "    gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, size)))\n",
    "    imgs = generator(z, gen_labels)\n",
    "    validity, pred_label = discriminator(imgs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMPORARY SECTION: testing target classifier output\n",
    "\n",
    "z = Variable(FloatTensor(np.random.normal(0, 1, (1, latent_dim))))\n",
    "gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, 1)))\n",
    "imgs = generator(z, gen_labels)\n",
    "validity, pred_label = discriminator(imgs)\n",
    "\n",
    "#print(validity)\n",
    "\n",
    "\n",
    "#bceloss_test = nn.BCELoss()\n",
    "celoss_test = nn.CrossEntropyLoss()\n",
    "\n",
    "inp = Variable(FloatTensor([[10,10,10,10,10,10,10,10,10,-10]]))\n",
    "true = Variable(LongTensor([9]))\n",
    "\n",
    "print(celoss_test(inp, true))\n",
    "\n",
    "#print(bceloss_test(Variable(FloatTensor([[0.65]])), Variable(FloatTensor([[1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achyut/Projects/AdversarialRobustness/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 0/200, Batch 0/938\n",
      "D loss: 1.496471, acc: 10% // tar acc: 6% // adv loss: 0.680685, aux loss: 2.302219, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 0/200, Batch 500/938\n",
      "D loss: 1.244165, acc: 60% // tar acc: 32% // adv loss: 0.522499, aux loss: 2.058553, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 1/200, Batch 62/938\n",
      "D loss: 1.076492, acc: 89% // tar acc: 65% // adv loss: 0.716126, aux loss: 1.581210, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 1/200, Batch 562/938\n",
      "D loss: 1.086960, acc: 89% // tar acc: 82% // adv loss: 0.625577, aux loss: 1.530647, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 2/200, Batch 124/938\n",
      "D loss: 1.076780, acc: 95% // tar acc: 79% // adv loss: 0.657504, aux loss: 1.520543, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 2/200, Batch 624/938\n",
      "D loss: 1.050300, acc: 94% // tar acc: 79% // adv loss: 0.788152, aux loss: 1.482747, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 3/200, Batch 186/938\n",
      "D loss: 1.049540, acc: 93% // tar acc: 89% // adv loss: 0.694938, aux loss: 1.511545, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 3/200, Batch 686/938\n",
      "D loss: 1.107228, acc: 95% // tar acc: 81% // adv loss: 0.575741, aux loss: 1.522167, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 4/200, Batch 248/938\n",
      "D loss: 1.077177, acc: 92% // tar acc: 87% // adv loss: 0.877700, aux loss: 1.480148, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 4/200, Batch 748/938\n",
      "D loss: 1.077381, acc: 96% // tar acc: 84% // adv loss: 0.731820, aux loss: 1.475413, tar loss: 0.631776\n",
      "=====================\n",
      "Epoch 5/200, Batch 310/938\n",
      "D loss: 1.091099, acc: 94% // tar acc: 90% // adv loss: 0.705624, aux loss: 1.489221, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 5/200, Batch 810/938\n",
      "D loss: 1.054059, acc: 96% // tar acc: 96% // adv loss: 0.657830, aux loss: 1.498201, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 6/200, Batch 372/938\n",
      "D loss: 1.065108, acc: 97% // tar acc: 81% // adv loss: 0.619772, aux loss: 1.488894, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 6/200, Batch 872/938\n",
      "D loss: 1.046116, acc: 92% // tar acc: 95% // adv loss: 0.576890, aux loss: 1.471911, tar loss: 0.687215\n",
      "=====================\n",
      "Epoch 7/200, Batch 434/938\n",
      "D loss: 1.100193, acc: 96% // tar acc: 87% // adv loss: 0.564733, aux loss: 1.484742, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 7/200, Batch 934/938\n",
      "D loss: 1.115708, acc: 94% // tar acc: 90% // adv loss: 0.763577, aux loss: 1.471668, tar loss: 0.703887\n",
      "=====================\n",
      "Epoch 8/200, Batch 496/938\n",
      "D loss: 1.071389, acc: 96% // tar acc: 90% // adv loss: 0.644155, aux loss: 1.506686, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 9/200, Batch 58/938\n",
      "D loss: 1.047816, acc: 95% // tar acc: 92% // adv loss: 0.698019, aux loss: 1.472104, tar loss: 0.712666\n",
      "=====================\n",
      "Epoch 9/200, Batch 558/938\n",
      "D loss: 1.072197, acc: 98% // tar acc: 96% // adv loss: 0.555955, aux loss: 1.481147, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 10/200, Batch 120/938\n",
      "D loss: 1.059708, acc: 96% // tar acc: 100% // adv loss: 0.708227, aux loss: 1.466481, tar loss: 0.713185\n",
      "=====================\n",
      "Epoch 10/200, Batch 620/938\n",
      "D loss: 1.062706, acc: 97% // tar acc: 89% // adv loss: 0.546574, aux loss: 1.470683, tar loss: 0.707634\n",
      "=====================\n",
      "Epoch 11/200, Batch 182/938\n",
      "D loss: 1.073844, acc: 95% // tar acc: 95% // adv loss: 0.577916, aux loss: 1.478496, tar loss: 0.745079\n",
      "=====================\n",
      "Epoch 11/200, Batch 682/938\n",
      "D loss: 1.070634, acc: 97% // tar acc: 92% // adv loss: 0.447794, aux loss: 1.490030, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 12/200, Batch 244/938\n",
      "D loss: 1.108126, acc: 96% // tar acc: 89% // adv loss: 0.593638, aux loss: 1.494784, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 12/200, Batch 744/938\n",
      "D loss: 1.133347, acc: 96% // tar acc: 98% // adv loss: 0.668035, aux loss: 1.480025, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 13/200, Batch 306/938\n",
      "D loss: 1.083925, acc: 97% // tar acc: 95% // adv loss: 0.550354, aux loss: 1.494114, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 13/200, Batch 806/938\n",
      "D loss: 1.067971, acc: 98% // tar acc: 96% // adv loss: 0.703006, aux loss: 1.464829, tar loss: 0.733931\n",
      "=====================\n",
      "Epoch 14/200, Batch 368/938\n",
      "D loss: 1.137075, acc: 96% // tar acc: 98% // adv loss: 0.561353, aux loss: 1.475496, tar loss: 0.714497\n",
      "=====================\n",
      "Epoch 14/200, Batch 868/938\n",
      "D loss: 1.058251, acc: 98% // tar acc: 100% // adv loss: 0.436470, aux loss: 1.481476, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 15/200, Batch 430/938\n",
      "D loss: 1.120564, acc: 93% // tar acc: 98% // adv loss: 0.627464, aux loss: 1.488225, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 15/200, Batch 930/938\n",
      "D loss: 1.081524, acc: 94% // tar acc: 95% // adv loss: 0.463114, aux loss: 1.479085, tar loss: 0.718739\n",
      "=====================\n",
      "Epoch 16/200, Batch 492/938\n",
      "D loss: 1.080056, acc: 97% // tar acc: 100% // adv loss: 0.477365, aux loss: 1.464145, tar loss: 0.716385\n",
      "=====================\n",
      "Epoch 17/200, Batch 54/938\n",
      "D loss: 1.052179, acc: 95% // tar acc: 93% // adv loss: 0.493423, aux loss: 1.481734, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 17/200, Batch 554/938\n",
      "D loss: 1.052294, acc: 97% // tar acc: 93% // adv loss: 0.582270, aux loss: 1.462560, tar loss: 0.685903\n",
      "=====================\n",
      "Epoch 18/200, Batch 116/938\n",
      "D loss: 1.079087, acc: 96% // tar acc: 95% // adv loss: 0.583333, aux loss: 1.475754, tar loss: 0.718085\n",
      "=====================\n",
      "Epoch 18/200, Batch 616/938\n",
      "D loss: 1.096837, acc: 96% // tar acc: 93% // adv loss: 0.459141, aux loss: 1.463785, tar loss: 0.721622\n",
      "=====================\n",
      "Epoch 19/200, Batch 178/938\n",
      "D loss: 1.086999, acc: 96% // tar acc: 98% // adv loss: 0.486276, aux loss: 1.463708, tar loss: 0.741392\n",
      "=====================\n",
      "Epoch 19/200, Batch 678/938\n",
      "D loss: 1.104443, acc: 92% // tar acc: 96% // adv loss: 0.669745, aux loss: 1.462677, tar loss: 0.658290\n",
      "=====================\n",
      "Epoch 20/200, Batch 240/938\n",
      "D loss: 1.098897, acc: 97% // tar acc: 100% // adv loss: 0.482320, aux loss: 1.468840, tar loss: 0.686529\n",
      "=====================\n",
      "Epoch 20/200, Batch 740/938\n",
      "D loss: 1.051933, acc: 100% // tar acc: 96% // adv loss: 0.625249, aux loss: 1.487333, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 21/200, Batch 302/938\n",
      "D loss: 1.066481, acc: 96% // tar acc: 95% // adv loss: 0.703785, aux loss: 1.486516, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 21/200, Batch 802/938\n",
      "D loss: 1.110650, acc: 97% // tar acc: 96% // adv loss: 0.655384, aux loss: 1.467166, tar loss: 0.706448\n",
      "=====================\n",
      "Epoch 22/200, Batch 364/938\n",
      "D loss: 1.097746, acc: 96% // tar acc: 93% // adv loss: 0.565168, aux loss: 1.464322, tar loss: 0.696889\n",
      "=====================\n",
      "Epoch 22/200, Batch 864/938\n",
      "D loss: 1.089288, acc: 96% // tar acc: 92% // adv loss: 0.459077, aux loss: 1.507419, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 23/200, Batch 426/938\n",
      "D loss: 1.048517, acc: 96% // tar acc: 90% // adv loss: 0.601541, aux loss: 1.485859, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 23/200, Batch 926/938\n",
      "D loss: 1.106194, acc: 96% // tar acc: 89% // adv loss: 0.490078, aux loss: 1.476831, tar loss: 0.653777\n",
      "=====================\n",
      "Epoch 24/200, Batch 488/938\n",
      "D loss: 1.059771, acc: 98% // tar acc: 95% // adv loss: 0.839408, aux loss: 1.462032, tar loss: 0.699982\n",
      "=====================\n",
      "Epoch 25/200, Batch 50/938\n",
      "D loss: 1.055753, acc: 97% // tar acc: 90% // adv loss: 0.521906, aux loss: 1.462565, tar loss: 0.678337\n",
      "=====================\n",
      "Epoch 25/200, Batch 550/938\n",
      "D loss: 1.077668, acc: 96% // tar acc: 92% // adv loss: 0.807550, aux loss: 1.487752, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 26/200, Batch 112/938\n",
      "D loss: 1.027313, acc: 96% // tar acc: 95% // adv loss: 0.522815, aux loss: 1.501587, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 26/200, Batch 612/938\n",
      "D loss: 1.050415, acc: 98% // tar acc: 95% // adv loss: 0.600502, aux loss: 1.461941, tar loss: 0.671161\n",
      "=====================\n",
      "Epoch 27/200, Batch 174/938\n",
      "D loss: 1.152574, acc: 98% // tar acc: 98% // adv loss: 0.537674, aux loss: 1.477127, tar loss: 0.725610\n",
      "=====================\n",
      "Epoch 27/200, Batch 674/938\n",
      "D loss: 1.066067, acc: 99% // tar acc: 96% // adv loss: 0.684419, aux loss: 1.478985, tar loss: 0.639687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 28/200, Batch 236/938\n",
      "D loss: 1.058966, acc: 96% // tar acc: 92% // adv loss: 0.555245, aux loss: 1.484763, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 28/200, Batch 736/938\n",
      "D loss: 1.094902, acc: 97% // tar acc: 90% // adv loss: 0.513282, aux loss: 1.475943, tar loss: 0.666770\n",
      "=====================\n",
      "Epoch 29/200, Batch 298/938\n",
      "D loss: 1.100030, acc: 97% // tar acc: 98% // adv loss: 0.472176, aux loss: 1.474909, tar loss: 0.716309\n",
      "=====================\n",
      "Epoch 29/200, Batch 798/938\n",
      "D loss: 1.052245, acc: 98% // tar acc: 95% // adv loss: 0.767529, aux loss: 1.467054, tar loss: 0.667246\n",
      "=====================\n",
      "Epoch 30/200, Batch 360/938\n",
      "D loss: 1.070722, acc: 95% // tar acc: 90% // adv loss: 0.605066, aux loss: 1.479037, tar loss: 0.675978\n",
      "=====================\n",
      "Epoch 30/200, Batch 860/938\n",
      "D loss: 1.113433, acc: 96% // tar acc: 93% // adv loss: 0.701868, aux loss: 1.476196, tar loss: 0.720308\n",
      "=====================\n",
      "Epoch 31/200, Batch 422/938\n",
      "D loss: 1.088548, acc: 98% // tar acc: 93% // adv loss: 0.700036, aux loss: 1.471754, tar loss: 0.674842\n",
      "=====================\n",
      "Epoch 31/200, Batch 922/938\n",
      "D loss: 1.111251, acc: 97% // tar acc: 93% // adv loss: 0.606274, aux loss: 1.472641, tar loss: 0.689475\n",
      "=====================\n",
      "Epoch 32/200, Batch 484/938\n",
      "D loss: 1.139297, acc: 96% // tar acc: 96% // adv loss: 0.693114, aux loss: 1.479170, tar loss: 0.743585\n",
      "=====================\n",
      "Epoch 33/200, Batch 46/938\n",
      "D loss: 1.042842, acc: 96% // tar acc: 95% // adv loss: 0.525819, aux loss: 1.463261, tar loss: 0.716919\n",
      "=====================\n",
      "Epoch 33/200, Batch 546/938\n",
      "D loss: 1.017660, acc: 96% // tar acc: 98% // adv loss: 0.560419, aux loss: 1.461342, tar loss: 0.739526\n",
      "=====================\n",
      "Epoch 34/200, Batch 108/938\n",
      "D loss: 1.106311, acc: 98% // tar acc: 96% // adv loss: 0.406096, aux loss: 1.464470, tar loss: 0.715425\n",
      "=====================\n",
      "Epoch 34/200, Batch 608/938\n",
      "D loss: 1.049257, acc: 96% // tar acc: 100% // adv loss: 0.432743, aux loss: 1.465871, tar loss: 0.745291\n",
      "=====================\n",
      "Epoch 35/200, Batch 170/938\n",
      "D loss: 1.035651, acc: 98% // tar acc: 95% // adv loss: 0.740855, aux loss: 1.489376, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 35/200, Batch 670/938\n",
      "D loss: 1.083135, acc: 97% // tar acc: 98% // adv loss: 0.509045, aux loss: 1.461249, tar loss: 0.743457\n",
      "=====================\n",
      "Epoch 36/200, Batch 232/938\n",
      "D loss: 1.052132, acc: 99% // tar acc: 93% // adv loss: 0.419159, aux loss: 1.461706, tar loss: 0.717797\n",
      "=====================\n",
      "Epoch 36/200, Batch 732/938\n",
      "D loss: 1.036775, acc: 96% // tar acc: 96% // adv loss: 0.509142, aux loss: 1.478836, tar loss: 0.669379\n",
      "=====================\n",
      "Epoch 37/200, Batch 294/938\n",
      "D loss: 1.046807, acc: 97% // tar acc: 93% // adv loss: 0.531792, aux loss: 1.490463, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 37/200, Batch 794/938\n",
      "D loss: 1.087341, acc: 96% // tar acc: 96% // adv loss: 0.695133, aux loss: 1.490578, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 38/200, Batch 356/938\n",
      "D loss: 1.108280, acc: 98% // tar acc: 98% // adv loss: 0.586070, aux loss: 1.461897, tar loss: 0.763380\n",
      "=====================\n",
      "Epoch 38/200, Batch 856/938\n",
      "D loss: 1.080064, acc: 99% // tar acc: 98% // adv loss: 0.484882, aux loss: 1.493109, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 39/200, Batch 418/938\n",
      "D loss: 1.024710, acc: 97% // tar acc: 95% // adv loss: 0.331878, aux loss: 1.461567, tar loss: 0.714454\n",
      "=====================\n",
      "Epoch 39/200, Batch 918/938\n",
      "D loss: 1.001126, acc: 96% // tar acc: 98% // adv loss: 0.589631, aux loss: 1.477793, tar loss: 0.737367\n",
      "=====================\n",
      "Epoch 40/200, Batch 480/938\n",
      "D loss: 1.059770, acc: 96% // tar acc: 96% // adv loss: 0.424483, aux loss: 1.471256, tar loss: 0.732509\n",
      "=====================\n",
      "Epoch 41/200, Batch 42/938\n",
      "D loss: 1.022184, acc: 96% // tar acc: 96% // adv loss: 0.506926, aux loss: 1.461825, tar loss: 0.739525\n",
      "=====================\n",
      "Epoch 41/200, Batch 542/938\n",
      "D loss: 1.104489, acc: 96% // tar acc: 96% // adv loss: 0.589523, aux loss: 1.490680, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 42/200, Batch 104/938\n",
      "D loss: 1.059777, acc: 100% // tar acc: 100% // adv loss: 0.580160, aux loss: 1.461448, tar loss: 0.733700\n",
      "=====================\n",
      "Epoch 42/200, Batch 604/938\n",
      "D loss: 1.079088, acc: 99% // tar acc: 98% // adv loss: 0.874930, aux loss: 1.475583, tar loss: 0.689961\n",
      "=====================\n",
      "Epoch 43/200, Batch 166/938\n",
      "D loss: 1.023540, acc: 99% // tar acc: 93% // adv loss: 0.604918, aux loss: 1.465404, tar loss: 0.730042\n",
      "=====================\n",
      "Epoch 43/200, Batch 666/938\n",
      "D loss: 1.061491, acc: 99% // tar acc: 98% // adv loss: 0.538511, aux loss: 1.461870, tar loss: 0.722282\n",
      "=====================\n",
      "Epoch 44/200, Batch 228/938\n",
      "D loss: 1.079621, acc: 96% // tar acc: 98% // adv loss: 0.632799, aux loss: 1.464740, tar loss: 0.788069\n",
      "=====================\n",
      "Epoch 44/200, Batch 728/938\n",
      "D loss: 1.006669, acc: 96% // tar acc: 93% // adv loss: 0.764324, aux loss: 1.475001, tar loss: 0.690545\n",
      "=====================\n",
      "Epoch 45/200, Batch 290/938\n",
      "D loss: 1.094594, acc: 96% // tar acc: 95% // adv loss: 0.516104, aux loss: 1.495422, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 45/200, Batch 790/938\n",
      "D loss: 1.049734, acc: 98% // tar acc: 93% // adv loss: 0.596501, aux loss: 1.465103, tar loss: 0.732165\n",
      "=====================\n",
      "Epoch 46/200, Batch 352/938\n",
      "D loss: 1.061190, acc: 98% // tar acc: 93% // adv loss: 0.496740, aux loss: 1.463091, tar loss: 0.736976\n",
      "=====================\n",
      "Epoch 46/200, Batch 852/938\n",
      "D loss: 1.047239, acc: 97% // tar acc: 96% // adv loss: 0.536659, aux loss: 1.473631, tar loss: 0.701932\n",
      "=====================\n",
      "Epoch 47/200, Batch 414/938\n",
      "D loss: 1.061819, acc: 98% // tar acc: 98% // adv loss: 0.470392, aux loss: 1.472648, tar loss: 0.760568\n",
      "=====================\n",
      "Epoch 47/200, Batch 914/938\n",
      "D loss: 1.043879, acc: 98% // tar acc: 96% // adv loss: 0.733797, aux loss: 1.462017, tar loss: 0.696450\n",
      "=====================\n",
      "Epoch 48/200, Batch 476/938\n",
      "D loss: 1.039093, acc: 99% // tar acc: 98% // adv loss: 0.468535, aux loss: 1.461480, tar loss: 0.706680\n",
      "=====================\n",
      "Epoch 49/200, Batch 38/938\n",
      "D loss: 1.036323, acc: 99% // tar acc: 96% // adv loss: 0.617166, aux loss: 1.463121, tar loss: 0.762257\n",
      "=====================\n",
      "Epoch 49/200, Batch 538/938\n",
      "D loss: 1.081297, acc: 96% // tar acc: 98% // adv loss: 0.710754, aux loss: 1.461627, tar loss: 0.772043\n",
      "=====================\n",
      "Epoch 50/200, Batch 100/938\n",
      "D loss: 1.042269, acc: 98% // tar acc: 100% // adv loss: 0.478870, aux loss: 1.464742, tar loss: 0.728843\n",
      "=====================\n",
      "Epoch 50/200, Batch 600/938\n",
      "D loss: 1.101960, acc: 98% // tar acc: 93% // adv loss: 0.511603, aux loss: 1.468165, tar loss: 0.715506\n",
      "=====================\n",
      "Epoch 51/200, Batch 162/938\n",
      "D loss: 1.127373, acc: 96% // tar acc: 100% // adv loss: 0.470360, aux loss: 1.471349, tar loss: 0.744458\n",
      "=====================\n",
      "Epoch 51/200, Batch 662/938\n",
      "D loss: 1.027504, acc: 96% // tar acc: 96% // adv loss: 0.898053, aux loss: 1.461359, tar loss: 0.745590\n",
      "=====================\n",
      "Epoch 52/200, Batch 224/938\n",
      "D loss: 1.087776, acc: 98% // tar acc: 98% // adv loss: 0.681912, aux loss: 1.470662, tar loss: 0.729745\n",
      "=====================\n",
      "Epoch 52/200, Batch 724/938\n",
      "D loss: 1.063921, acc: 97% // tar acc: 98% // adv loss: 0.736638, aux loss: 1.463562, tar loss: 0.683056\n",
      "=====================\n",
      "Epoch 53/200, Batch 286/938\n",
      "D loss: 1.080345, acc: 98% // tar acc: 96% // adv loss: 0.510422, aux loss: 1.483782, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 53/200, Batch 786/938\n",
      "D loss: 1.020655, acc: 97% // tar acc: 100% // adv loss: 0.851904, aux loss: 1.463653, tar loss: 0.724717\n",
      "=====================\n",
      "Epoch 54/200, Batch 348/938\n",
      "D loss: 1.061663, acc: 100% // tar acc: 98% // adv loss: 0.590375, aux loss: 1.462284, tar loss: 0.721721\n",
      "=====================\n",
      "Epoch 54/200, Batch 848/938\n",
      "D loss: 1.095840, acc: 96% // tar acc: 96% // adv loss: 0.397875, aux loss: 1.464347, tar loss: 0.724487\n",
      "=====================\n",
      "Epoch 55/200, Batch 410/938\n",
      "D loss: 1.154916, acc: 96% // tar acc: 93% // adv loss: 0.479367, aux loss: 1.479377, tar loss: 0.707174\n",
      "=====================\n",
      "Epoch 55/200, Batch 910/938\n",
      "D loss: 1.087436, acc: 99% // tar acc: 95% // adv loss: 0.754907, aux loss: 1.480938, tar loss: 1.110000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 56/200, Batch 472/938\n",
      "D loss: 1.097376, acc: 97% // tar acc: 98% // adv loss: 0.586164, aux loss: 1.470191, tar loss: 0.727943\n",
      "=====================\n",
      "Epoch 57/200, Batch 34/938\n",
      "D loss: 1.072906, acc: 100% // tar acc: 98% // adv loss: 0.343300, aux loss: 1.470239, tar loss: 0.761386\n",
      "=====================\n",
      "Epoch 57/200, Batch 534/938\n",
      "D loss: 1.116461, acc: 99% // tar acc: 98% // adv loss: 0.456984, aux loss: 1.475827, tar loss: 0.732722\n",
      "=====================\n",
      "Epoch 58/200, Batch 96/938\n",
      "D loss: 1.039398, acc: 99% // tar acc: 98% // adv loss: 0.580406, aux loss: 1.461217, tar loss: 0.764191\n",
      "=====================\n",
      "Epoch 58/200, Batch 596/938\n",
      "D loss: 1.137089, acc: 98% // tar acc: 96% // adv loss: 0.393357, aux loss: 1.508871, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 59/200, Batch 158/938\n",
      "D loss: 1.034417, acc: 97% // tar acc: 96% // adv loss: 0.445418, aux loss: 1.477736, tar loss: 0.726244\n",
      "=====================\n",
      "Epoch 59/200, Batch 658/938\n",
      "D loss: 1.070597, acc: 98% // tar acc: 96% // adv loss: 0.538131, aux loss: 1.477825, tar loss: 0.732716\n",
      "=====================\n",
      "Epoch 60/200, Batch 220/938\n",
      "D loss: 1.083262, acc: 98% // tar acc: 100% // adv loss: 0.454038, aux loss: 1.473166, tar loss: 0.770441\n",
      "=====================\n",
      "Epoch 60/200, Batch 720/938\n",
      "D loss: 1.076534, acc: 95% // tar acc: 96% // adv loss: 0.595754, aux loss: 1.478717, tar loss: 0.708544\n",
      "=====================\n",
      "Epoch 61/200, Batch 282/938\n",
      "D loss: 1.009477, acc: 96% // tar acc: 98% // adv loss: 0.586684, aux loss: 1.462600, tar loss: 0.686252\n",
      "=====================\n",
      "Epoch 61/200, Batch 782/938\n",
      "D loss: 1.055063, acc: 97% // tar acc: 98% // adv loss: 0.520634, aux loss: 1.462269, tar loss: 0.760357\n",
      "=====================\n",
      "Epoch 62/200, Batch 344/938\n",
      "D loss: 1.054212, acc: 96% // tar acc: 98% // adv loss: 0.591908, aux loss: 1.463979, tar loss: 0.713062\n",
      "=====================\n",
      "Epoch 62/200, Batch 844/938\n",
      "D loss: 1.135780, acc: 96% // tar acc: 95% // adv loss: 0.510417, aux loss: 1.479119, tar loss: 0.738646\n",
      "=====================\n",
      "Epoch 63/200, Batch 406/938\n",
      "D loss: 1.027536, acc: 99% // tar acc: 100% // adv loss: 0.714017, aux loss: 1.469676, tar loss: 0.719592\n",
      "=====================\n",
      "Epoch 63/200, Batch 906/938\n",
      "D loss: 1.040154, acc: 99% // tar acc: 100% // adv loss: 0.382717, aux loss: 1.461303, tar loss: 0.729240\n",
      "=====================\n",
      "Epoch 64/200, Batch 468/938\n",
      "D loss: 1.060076, acc: 98% // tar acc: 98% // adv loss: 0.474562, aux loss: 1.488164, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 65/200, Batch 30/938\n",
      "D loss: 1.061099, acc: 99% // tar acc: 96% // adv loss: 0.506515, aux loss: 1.476833, tar loss: 0.722782\n",
      "=====================\n",
      "Epoch 65/200, Batch 530/938\n",
      "D loss: 1.041896, acc: 98% // tar acc: 98% // adv loss: 0.721130, aux loss: 1.462500, tar loss: 0.698070\n",
      "=====================\n",
      "Epoch 66/200, Batch 92/938\n",
      "D loss: 1.071443, acc: 100% // tar acc: 98% // adv loss: 0.653701, aux loss: 1.463455, tar loss: 0.718632\n",
      "=====================\n",
      "Epoch 66/200, Batch 592/938\n",
      "D loss: 1.113958, acc: 98% // tar acc: 98% // adv loss: 0.345972, aux loss: 1.461370, tar loss: 0.715889\n",
      "=====================\n",
      "Epoch 67/200, Batch 154/938\n",
      "D loss: 1.042124, acc: 98% // tar acc: 96% // adv loss: 0.519411, aux loss: 1.478228, tar loss: 0.698378\n",
      "=====================\n",
      "Epoch 67/200, Batch 654/938\n",
      "D loss: 1.092624, acc: 98% // tar acc: 100% // adv loss: 0.609265, aux loss: 1.461338, tar loss: 0.746960\n",
      "=====================\n",
      "Epoch 68/200, Batch 216/938\n",
      "D loss: 1.030169, acc: 99% // tar acc: 96% // adv loss: 0.428459, aux loss: 1.474106, tar loss: 0.708555\n",
      "=====================\n",
      "Epoch 68/200, Batch 716/938\n",
      "D loss: 1.056630, acc: 96% // tar acc: 98% // adv loss: 0.380755, aux loss: 1.485728, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 69/200, Batch 278/938\n",
      "D loss: 1.050871, acc: 96% // tar acc: 95% // adv loss: 0.635461, aux loss: 1.478469, tar loss: 0.673574\n",
      "=====================\n",
      "Epoch 69/200, Batch 778/938\n",
      "D loss: 1.067751, acc: 95% // tar acc: 95% // adv loss: 0.597406, aux loss: 1.472727, tar loss: 0.725271\n",
      "=====================\n",
      "Epoch 70/200, Batch 340/938\n",
      "D loss: 1.081542, acc: 97% // tar acc: 95% // adv loss: 0.610637, aux loss: 1.476696, tar loss: 0.729423\n",
      "=====================\n",
      "Epoch 70/200, Batch 840/938\n",
      "D loss: 1.098281, acc: 96% // tar acc: 96% // adv loss: 0.542700, aux loss: 1.470917, tar loss: 0.725538\n",
      "=====================\n",
      "Epoch 71/200, Batch 402/938\n",
      "D loss: 1.090563, acc: 97% // tar acc: 98% // adv loss: 0.551092, aux loss: 1.461366, tar loss: 0.738867\n",
      "=====================\n",
      "Epoch 71/200, Batch 902/938\n",
      "D loss: 1.150677, acc: 97% // tar acc: 100% // adv loss: 0.485749, aux loss: 1.463368, tar loss: 0.750552\n",
      "=====================\n",
      "Epoch 72/200, Batch 464/938\n",
      "D loss: 1.056907, acc: 99% // tar acc: 96% // adv loss: 0.425737, aux loss: 1.470156, tar loss: 0.752607\n",
      "=====================\n",
      "Epoch 73/200, Batch 26/938\n",
      "D loss: 1.011435, acc: 100% // tar acc: 95% // adv loss: 0.548382, aux loss: 1.461286, tar loss: 0.707342\n",
      "=====================\n",
      "Epoch 73/200, Batch 526/938\n",
      "D loss: 1.084314, acc: 99% // tar acc: 98% // adv loss: 0.481348, aux loss: 1.479783, tar loss: 0.719408\n",
      "=====================\n",
      "Epoch 74/200, Batch 88/938\n",
      "D loss: 1.088856, acc: 98% // tar acc: 96% // adv loss: 0.587474, aux loss: 1.481487, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 74/200, Batch 588/938\n",
      "D loss: 1.019980, acc: 96% // tar acc: 95% // adv loss: 0.541361, aux loss: 1.461862, tar loss: 0.740979\n",
      "=====================\n",
      "Epoch 75/200, Batch 150/938\n",
      "D loss: 0.999880, acc: 99% // tar acc: 96% // adv loss: 0.351773, aux loss: 1.479156, tar loss: 0.698121\n",
      "=====================\n",
      "Epoch 75/200, Batch 650/938\n",
      "D loss: 1.085378, acc: 97% // tar acc: 96% // adv loss: 0.637084, aux loss: 1.463412, tar loss: 0.705019\n",
      "=====================\n",
      "Epoch 76/200, Batch 212/938\n",
      "D loss: 1.027254, acc: 96% // tar acc: 96% // adv loss: 0.673973, aux loss: 1.492581, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 76/200, Batch 712/938\n",
      "D loss: 1.044570, acc: 97% // tar acc: 100% // adv loss: 0.610272, aux loss: 1.465786, tar loss: 0.724014\n",
      "=====================\n",
      "Epoch 77/200, Batch 274/938\n",
      "D loss: 1.114075, acc: 99% // tar acc: 96% // adv loss: 0.494791, aux loss: 1.461673, tar loss: 0.725321\n",
      "=====================\n",
      "Epoch 77/200, Batch 774/938\n",
      "D loss: 1.072156, acc: 97% // tar acc: 96% // adv loss: 0.720262, aux loss: 1.477839, tar loss: 0.740046\n",
      "=====================\n",
      "Epoch 78/200, Batch 336/938\n",
      "D loss: 1.092465, acc: 97% // tar acc: 100% // adv loss: 0.678137, aux loss: 1.464505, tar loss: 0.713282\n",
      "=====================\n",
      "Epoch 78/200, Batch 836/938\n",
      "D loss: 1.021209, acc: 97% // tar acc: 100% // adv loss: 0.545433, aux loss: 1.496902, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 79/200, Batch 398/938\n",
      "D loss: 1.076433, acc: 98% // tar acc: 98% // adv loss: 0.678719, aux loss: 1.469288, tar loss: 0.717937\n",
      "=====================\n",
      "Epoch 79/200, Batch 898/938\n",
      "D loss: 1.056882, acc: 96% // tar acc: 90% // adv loss: 0.616233, aux loss: 1.466034, tar loss: 0.666907\n",
      "=====================\n",
      "Epoch 80/200, Batch 460/938\n",
      "D loss: 1.035565, acc: 99% // tar acc: 98% // adv loss: 0.442492, aux loss: 1.461510, tar loss: 0.745144\n",
      "=====================\n",
      "Epoch 81/200, Batch 22/938\n",
      "D loss: 1.114095, acc: 98% // tar acc: 100% // adv loss: 0.425663, aux loss: 1.476450, tar loss: 0.725714\n",
      "=====================\n",
      "Epoch 81/200, Batch 522/938\n",
      "D loss: 1.088995, acc: 98% // tar acc: 96% // adv loss: 0.559348, aux loss: 1.461228, tar loss: 0.732273\n",
      "=====================\n",
      "Epoch 82/200, Batch 84/938\n",
      "D loss: 1.056357, acc: 97% // tar acc: 93% // adv loss: 0.551734, aux loss: 1.467690, tar loss: 0.719936\n",
      "=====================\n",
      "Epoch 82/200, Batch 584/938\n",
      "D loss: 1.140354, acc: 99% // tar acc: 98% // adv loss: 0.630986, aux loss: 1.471804, tar loss: 0.748068\n",
      "=====================\n",
      "Epoch 83/200, Batch 146/938\n",
      "D loss: 1.057671, acc: 98% // tar acc: 98% // adv loss: 0.592283, aux loss: 1.462303, tar loss: 0.718400\n",
      "=====================\n",
      "Epoch 83/200, Batch 646/938\n",
      "D loss: 1.121902, acc: 97% // tar acc: 98% // adv loss: 0.741289, aux loss: 1.461461, tar loss: 0.691486\n",
      "=====================\n",
      "Epoch 84/200, Batch 208/938\n",
      "D loss: 1.074562, acc: 97% // tar acc: 96% // adv loss: 0.467029, aux loss: 1.465186, tar loss: 0.733303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 84/200, Batch 708/938\n",
      "D loss: 1.036918, acc: 99% // tar acc: 100% // adv loss: 0.490782, aux loss: 1.461796, tar loss: 0.725036\n",
      "=====================\n",
      "Epoch 85/200, Batch 270/938\n",
      "D loss: 1.087543, acc: 100% // tar acc: 98% // adv loss: 0.574082, aux loss: 1.465254, tar loss: 0.750062\n",
      "=====================\n",
      "Epoch 85/200, Batch 770/938\n",
      "D loss: 1.069425, acc: 98% // tar acc: 96% // adv loss: 0.541370, aux loss: 1.472052, tar loss: 0.733162\n",
      "=====================\n",
      "Epoch 86/200, Batch 332/938\n",
      "D loss: 1.028328, acc: 94% // tar acc: 100% // adv loss: 0.552051, aux loss: 1.461738, tar loss: 0.767940\n",
      "=====================\n",
      "Epoch 86/200, Batch 832/938\n",
      "D loss: 1.102319, acc: 98% // tar acc: 98% // adv loss: 0.574909, aux loss: 1.467092, tar loss: 0.748562\n",
      "=====================\n",
      "Epoch 87/200, Batch 394/938\n",
      "D loss: 1.122140, acc: 97% // tar acc: 95% // adv loss: 0.492706, aux loss: 1.489296, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 87/200, Batch 894/938\n",
      "D loss: 1.027699, acc: 99% // tar acc: 100% // adv loss: 0.380148, aux loss: 1.462241, tar loss: 0.716818\n",
      "=====================\n",
      "Epoch 88/200, Batch 456/938\n",
      "D loss: 1.075253, acc: 98% // tar acc: 98% // adv loss: 0.621135, aux loss: 1.472823, tar loss: 0.747359\n",
      "=====================\n",
      "Epoch 89/200, Batch 18/938\n",
      "D loss: 1.012205, acc: 96% // tar acc: 95% // adv loss: 0.609847, aux loss: 1.473853, tar loss: 0.687293\n",
      "=====================\n",
      "Epoch 89/200, Batch 518/938\n",
      "D loss: 1.139053, acc: 96% // tar acc: 100% // adv loss: 0.521059, aux loss: 1.461223, tar loss: 0.738969\n",
      "=====================\n",
      "Epoch 90/200, Batch 80/938\n",
      "D loss: 1.021828, acc: 96% // tar acc: 98% // adv loss: 0.453386, aux loss: 1.474669, tar loss: 0.701506\n",
      "=====================\n",
      "Epoch 90/200, Batch 580/938\n",
      "D loss: 1.095450, acc: 98% // tar acc: 100% // adv loss: 0.565578, aux loss: 1.476751, tar loss: 0.730589\n",
      "=====================\n",
      "Epoch 91/200, Batch 142/938\n",
      "D loss: 1.092420, acc: 99% // tar acc: 100% // adv loss: 0.605352, aux loss: 1.461292, tar loss: 0.734761\n",
      "=====================\n",
      "Epoch 91/200, Batch 642/938\n",
      "D loss: 1.136419, acc: 94% // tar acc: 95% // adv loss: 0.324548, aux loss: 1.461987, tar loss: 0.726298\n",
      "=====================\n",
      "Epoch 92/200, Batch 204/938\n",
      "D loss: 1.066942, acc: 97% // tar acc: 98% // adv loss: 0.670146, aux loss: 1.477991, tar loss: 0.712918\n",
      "=====================\n",
      "Epoch 92/200, Batch 704/938\n",
      "D loss: 1.041158, acc: 98% // tar acc: 100% // adv loss: 0.561980, aux loss: 1.462373, tar loss: 0.737471\n",
      "=====================\n",
      "Epoch 93/200, Batch 266/938\n",
      "D loss: 1.139011, acc: 97% // tar acc: 98% // adv loss: 0.737673, aux loss: 1.462618, tar loss: 0.706867\n",
      "=====================\n",
      "Epoch 93/200, Batch 766/938\n",
      "D loss: 1.081338, acc: 100% // tar acc: 96% // adv loss: 0.539619, aux loss: 1.476550, tar loss: 0.705722\n",
      "=====================\n",
      "Epoch 94/200, Batch 328/938\n",
      "D loss: 1.138518, acc: 99% // tar acc: 96% // adv loss: 0.490497, aux loss: 1.477186, tar loss: 0.700895\n",
      "=====================\n",
      "Epoch 94/200, Batch 828/938\n",
      "D loss: 1.053082, acc: 99% // tar acc: 98% // adv loss: 0.427521, aux loss: 1.472204, tar loss: 0.714848\n",
      "=====================\n",
      "Epoch 95/200, Batch 390/938\n",
      "D loss: 1.131792, acc: 99% // tar acc: 100% // adv loss: 0.512221, aux loss: 1.461781, tar loss: 0.765439\n",
      "=====================\n",
      "Epoch 95/200, Batch 890/938\n",
      "D loss: 1.107575, acc: 99% // tar acc: 96% // adv loss: 0.515271, aux loss: 1.465984, tar loss: 0.681578\n",
      "=====================\n",
      "Epoch 96/200, Batch 452/938\n",
      "D loss: 1.070654, acc: 100% // tar acc: 93% // adv loss: 0.501236, aux loss: 1.461371, tar loss: 0.698340\n",
      "=====================\n",
      "Epoch 97/200, Batch 14/938\n",
      "D loss: 1.051911, acc: 96% // tar acc: 98% // adv loss: 0.844305, aux loss: 1.466570, tar loss: 0.698339\n",
      "=====================\n",
      "Epoch 97/200, Batch 514/938\n",
      "D loss: 1.088166, acc: 97% // tar acc: 96% // adv loss: 0.579633, aux loss: 1.488453, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 98/200, Batch 76/938\n",
      "D loss: 1.046945, acc: 99% // tar acc: 90% // adv loss: 0.712741, aux loss: 1.462407, tar loss: 0.678643\n",
      "=====================\n",
      "Epoch 98/200, Batch 576/938\n",
      "D loss: 1.053534, acc: 100% // tar acc: 98% // adv loss: 0.629779, aux loss: 1.474181, tar loss: 0.782488\n",
      "=====================\n",
      "Epoch 99/200, Batch 138/938\n",
      "D loss: 1.084488, acc: 96% // tar acc: 98% // adv loss: 0.491002, aux loss: 1.461743, tar loss: 0.732731\n",
      "=====================\n",
      "Epoch 99/200, Batch 638/938\n",
      "D loss: 1.053114, acc: 97% // tar acc: 98% // adv loss: 0.700217, aux loss: 1.461177, tar loss: 0.721820\n",
      "=====================\n",
      "Epoch 100/200, Batch 200/938\n",
      "D loss: 1.112145, acc: 97% // tar acc: 96% // adv loss: 0.539285, aux loss: 1.473662, tar loss: 0.694920\n",
      "=====================\n",
      "Epoch 100/200, Batch 700/938\n",
      "D loss: 1.012594, acc: 96% // tar acc: 96% // adv loss: 0.446854, aux loss: 1.463726, tar loss: 0.719080\n",
      "=====================\n",
      "Epoch 101/200, Batch 262/938\n",
      "D loss: 1.117314, acc: 97% // tar acc: 98% // adv loss: 0.388728, aux loss: 1.461152, tar loss: 0.742605\n",
      "=====================\n",
      "Epoch 101/200, Batch 762/938\n",
      "D loss: 1.070942, acc: 99% // tar acc: 95% // adv loss: 0.492229, aux loss: 1.463399, tar loss: 0.742699\n",
      "=====================\n",
      "Epoch 102/200, Batch 324/938\n",
      "D loss: 1.027265, acc: 99% // tar acc: 93% // adv loss: 0.633024, aux loss: 1.478052, tar loss: 0.718525\n",
      "=====================\n",
      "Epoch 102/200, Batch 824/938\n",
      "D loss: 1.093740, acc: 100% // tar acc: 98% // adv loss: 0.653080, aux loss: 1.462489, tar loss: 0.737409\n",
      "=====================\n",
      "Epoch 103/200, Batch 386/938\n",
      "D loss: 1.110854, acc: 99% // tar acc: 98% // adv loss: 0.532715, aux loss: 1.461617, tar loss: 0.749705\n",
      "=====================\n",
      "Epoch 103/200, Batch 886/938\n",
      "D loss: 0.985728, acc: 100% // tar acc: 96% // adv loss: 0.509944, aux loss: 1.462638, tar loss: 0.690342\n",
      "=====================\n",
      "Epoch 104/200, Batch 448/938\n",
      "D loss: 1.071023, acc: 97% // tar acc: 98% // adv loss: 0.666550, aux loss: 1.462831, tar loss: 0.730144\n",
      "=====================\n",
      "Epoch 105/200, Batch 10/938\n",
      "D loss: 1.101282, acc: 97% // tar acc: 96% // adv loss: 0.497668, aux loss: 1.461189, tar loss: 0.704852\n",
      "=====================\n",
      "Epoch 105/200, Batch 510/938\n",
      "D loss: 1.056466, acc: 98% // tar acc: 100% // adv loss: 0.653982, aux loss: 1.472865, tar loss: 0.718022\n",
      "=====================\n",
      "Epoch 106/200, Batch 72/938\n",
      "D loss: 1.065399, acc: 97% // tar acc: 100% // adv loss: 0.549999, aux loss: 1.475179, tar loss: 0.752698\n",
      "=====================\n",
      "Epoch 106/200, Batch 572/938\n",
      "D loss: 1.039524, acc: 99% // tar acc: 100% // adv loss: 0.555636, aux loss: 1.473291, tar loss: 0.737333\n",
      "=====================\n",
      "Epoch 107/200, Batch 134/938\n",
      "D loss: 1.107394, acc: 97% // tar acc: 100% // adv loss: 0.467574, aux loss: 1.462440, tar loss: 0.769001\n",
      "=====================\n",
      "Epoch 107/200, Batch 634/938\n",
      "D loss: 1.093647, acc: 98% // tar acc: 93% // adv loss: 0.710331, aux loss: 1.470112, tar loss: 0.720018\n",
      "=====================\n",
      "Epoch 108/200, Batch 196/938\n",
      "D loss: 1.009730, acc: 99% // tar acc: 96% // adv loss: 0.615587, aux loss: 1.462232, tar loss: 0.706877\n",
      "=====================\n",
      "Epoch 108/200, Batch 696/938\n",
      "D loss: 1.047771, acc: 100% // tar acc: 98% // adv loss: 0.537286, aux loss: 1.461173, tar loss: 0.710621\n",
      "=====================\n",
      "Epoch 109/200, Batch 258/938\n",
      "D loss: 1.107980, acc: 97% // tar acc: 98% // adv loss: 0.575231, aux loss: 1.462064, tar loss: 0.716525\n",
      "=====================\n",
      "Epoch 109/200, Batch 758/938\n",
      "D loss: 1.002998, acc: 99% // tar acc: 100% // adv loss: 0.821627, aux loss: 1.472787, tar loss: 0.732944\n",
      "=====================\n",
      "Epoch 110/200, Batch 320/938\n",
      "D loss: 1.015500, acc: 99% // tar acc: 96% // adv loss: 0.633123, aux loss: 1.461400, tar loss: 0.717762\n",
      "=====================\n",
      "Epoch 110/200, Batch 820/938\n",
      "D loss: 1.060492, acc: 95% // tar acc: 95% // adv loss: 0.664207, aux loss: 1.461283, tar loss: 0.695845\n",
      "=====================\n",
      "Epoch 111/200, Batch 382/938\n",
      "D loss: 1.115873, acc: 99% // tar acc: 96% // adv loss: 0.578683, aux loss: 1.484938, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 111/200, Batch 882/938\n",
      "D loss: 1.085198, acc: 97% // tar acc: 98% // adv loss: 0.762107, aux loss: 1.472704, tar loss: 0.745585\n",
      "=====================\n",
      "Epoch 112/200, Batch 444/938\n",
      "D loss: 1.080525, acc: 97% // tar acc: 98% // adv loss: 0.637169, aux loss: 1.473818, tar loss: 0.731400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 113/200, Batch 6/938\n",
      "D loss: 1.103758, acc: 96% // tar acc: 98% // adv loss: 0.652833, aux loss: 1.479757, tar loss: 0.724463\n",
      "=====================\n",
      "Epoch 113/200, Batch 506/938\n",
      "D loss: 1.054078, acc: 99% // tar acc: 93% // adv loss: 0.568651, aux loss: 1.475664, tar loss: 0.701824\n",
      "=====================\n",
      "Epoch 114/200, Batch 68/938\n",
      "D loss: 1.028264, acc: 100% // tar acc: 95% // adv loss: 0.541973, aux loss: 1.468399, tar loss: 0.735935\n",
      "=====================\n",
      "Epoch 114/200, Batch 568/938\n",
      "D loss: 1.077362, acc: 98% // tar acc: 92% // adv loss: 0.530133, aux loss: 1.461492, tar loss: 0.731546\n",
      "=====================\n",
      "Epoch 115/200, Batch 130/938\n",
      "D loss: 1.098559, acc: 98% // tar acc: 93% // adv loss: 0.604282, aux loss: 1.461169, tar loss: 0.768441\n",
      "=====================\n",
      "Epoch 115/200, Batch 630/938\n",
      "D loss: 1.020148, acc: 97% // tar acc: 93% // adv loss: 0.599339, aux loss: 1.478329, tar loss: 0.706356\n",
      "=====================\n",
      "Epoch 116/200, Batch 192/938\n",
      "D loss: 1.064544, acc: 99% // tar acc: 98% // adv loss: 0.403904, aux loss: 1.463603, tar loss: 0.710789\n",
      "=====================\n",
      "Epoch 116/200, Batch 692/938\n",
      "D loss: 1.068958, acc: 96% // tar acc: 92% // adv loss: 0.579947, aux loss: 1.462708, tar loss: 0.729539\n",
      "=====================\n",
      "Epoch 117/200, Batch 254/938\n",
      "D loss: 1.088189, acc: 99% // tar acc: 98% // adv loss: 0.503668, aux loss: 1.477412, tar loss: 0.732384\n",
      "=====================\n",
      "Epoch 117/200, Batch 754/938\n",
      "D loss: 1.071795, acc: 99% // tar acc: 98% // adv loss: 0.437814, aux loss: 1.466118, tar loss: 0.727534\n",
      "=====================\n",
      "Epoch 118/200, Batch 316/938\n",
      "D loss: 1.081076, acc: 99% // tar acc: 95% // adv loss: 0.737020, aux loss: 1.477133, tar loss: 0.693722\n",
      "=====================\n",
      "Epoch 118/200, Batch 816/938\n",
      "D loss: 1.057035, acc: 97% // tar acc: 100% // adv loss: 0.503208, aux loss: 1.467749, tar loss: 0.749461\n",
      "=====================\n",
      "Epoch 119/200, Batch 378/938\n",
      "D loss: 1.033956, acc: 100% // tar acc: 95% // adv loss: 0.643175, aux loss: 1.470715, tar loss: 0.701062\n",
      "=====================\n",
      "Epoch 119/200, Batch 878/938\n",
      "D loss: 1.077797, acc: 98% // tar acc: 100% // adv loss: 0.503999, aux loss: 1.464334, tar loss: 0.682404\n",
      "=====================\n",
      "Epoch 120/200, Batch 440/938\n",
      "D loss: 1.012164, acc: 96% // tar acc: 92% // adv loss: 0.734163, aux loss: 1.462625, tar loss: 0.680307\n",
      "=====================\n",
      "Epoch 121/200, Batch 2/938\n",
      "D loss: 1.054077, acc: 99% // tar acc: 98% // adv loss: 0.577767, aux loss: 1.462739, tar loss: 0.670477\n",
      "=====================\n",
      "Epoch 121/200, Batch 502/938\n",
      "D loss: 1.027604, acc: 100% // tar acc: 96% // adv loss: 0.568214, aux loss: 1.492426, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 122/200, Batch 64/938\n",
      "D loss: 1.056075, acc: 100% // tar acc: 98% // adv loss: 0.473295, aux loss: 1.475751, tar loss: 0.730613\n",
      "=====================\n",
      "Epoch 122/200, Batch 564/938\n",
      "D loss: 1.070533, acc: 99% // tar acc: 98% // adv loss: 0.583201, aux loss: 1.461278, tar loss: 0.756128\n",
      "=====================\n",
      "Epoch 123/200, Batch 126/938\n",
      "D loss: 1.062234, acc: 98% // tar acc: 98% // adv loss: 0.547594, aux loss: 1.462862, tar loss: 0.750093\n",
      "=====================\n",
      "Epoch 123/200, Batch 626/938\n",
      "D loss: 1.047092, acc: 98% // tar acc: 96% // adv loss: 0.558465, aux loss: 1.463919, tar loss: 0.730402\n",
      "=====================\n",
      "Epoch 124/200, Batch 188/938\n",
      "D loss: 1.074601, acc: 98% // tar acc: 96% // adv loss: 0.550557, aux loss: 1.461152, tar loss: 0.779090\n",
      "=====================\n",
      "Epoch 124/200, Batch 688/938\n",
      "D loss: 1.084082, acc: 97% // tar acc: 92% // adv loss: 0.709246, aux loss: 1.481696, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 125/200, Batch 250/938\n",
      "D loss: 1.038622, acc: 98% // tar acc: 100% // adv loss: 0.530526, aux loss: 1.463227, tar loss: 0.765809\n",
      "=====================\n",
      "Epoch 125/200, Batch 750/938\n",
      "D loss: 1.058397, acc: 98% // tar acc: 96% // adv loss: 0.424617, aux loss: 1.476018, tar loss: 0.704457\n",
      "=====================\n",
      "Epoch 126/200, Batch 312/938\n",
      "D loss: 1.010612, acc: 97% // tar acc: 98% // adv loss: 0.529584, aux loss: 1.464753, tar loss: 0.707783\n",
      "=====================\n",
      "Epoch 126/200, Batch 812/938\n",
      "D loss: 1.068326, acc: 99% // tar acc: 100% // adv loss: 0.520882, aux loss: 1.461376, tar loss: 0.746096\n",
      "=====================\n",
      "Epoch 127/200, Batch 374/938\n",
      "D loss: 1.109249, acc: 98% // tar acc: 98% // adv loss: 0.620223, aux loss: 1.461154, tar loss: 0.735329\n",
      "=====================\n",
      "Epoch 127/200, Batch 874/938\n",
      "D loss: 1.067629, acc: 96% // tar acc: 95% // adv loss: 0.495103, aux loss: 1.461823, tar loss: 0.738162\n",
      "=====================\n",
      "Epoch 128/200, Batch 436/938\n",
      "D loss: 1.055443, acc: 98% // tar acc: 95% // adv loss: 0.467079, aux loss: 1.461233, tar loss: 0.738238\n",
      "=====================\n",
      "Epoch 128/200, Batch 936/938\n",
      "D loss: 1.048229, acc: 96% // tar acc: 96% // adv loss: 0.427097, aux loss: 1.461797, tar loss: 0.720652\n",
      "=====================\n",
      "Epoch 129/200, Batch 498/938\n",
      "D loss: 1.068603, acc: 100% // tar acc: 96% // adv loss: 0.513986, aux loss: 1.462993, tar loss: 0.758603\n",
      "=====================\n",
      "Epoch 130/200, Batch 60/938\n",
      "D loss: 1.086369, acc: 99% // tar acc: 100% // adv loss: 0.568468, aux loss: 1.462901, tar loss: 0.713957\n",
      "=====================\n",
      "Epoch 130/200, Batch 560/938\n",
      "D loss: 1.119179, acc: 97% // tar acc: 96% // adv loss: 0.484446, aux loss: 1.486238, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 131/200, Batch 122/938\n",
      "D loss: 1.071908, acc: 97% // tar acc: 98% // adv loss: 0.599099, aux loss: 1.461347, tar loss: 0.699963\n",
      "=====================\n",
      "Epoch 131/200, Batch 622/938\n",
      "D loss: 1.065014, acc: 99% // tar acc: 98% // adv loss: 0.712201, aux loss: 1.461567, tar loss: 0.696537\n",
      "=====================\n",
      "Epoch 132/200, Batch 184/938\n",
      "D loss: 1.041276, acc: 97% // tar acc: 100% // adv loss: 0.528575, aux loss: 1.461910, tar loss: 0.725178\n",
      "=====================\n",
      "Epoch 132/200, Batch 684/938\n",
      "D loss: 1.083020, acc: 98% // tar acc: 98% // adv loss: 0.508321, aux loss: 1.482621, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 133/200, Batch 246/938\n",
      "D loss: 1.126070, acc: 99% // tar acc: 98% // adv loss: 0.612910, aux loss: 1.479502, tar loss: 0.727118\n",
      "=====================\n",
      "Epoch 133/200, Batch 746/938\n",
      "D loss: 1.077715, acc: 96% // tar acc: 98% // adv loss: 0.591637, aux loss: 1.476777, tar loss: 0.753856\n",
      "=====================\n",
      "Epoch 134/200, Batch 308/938\n",
      "D loss: 1.089678, acc: 98% // tar acc: 100% // adv loss: 0.437324, aux loss: 1.477452, tar loss: 0.721617\n",
      "=====================\n",
      "Epoch 134/200, Batch 808/938\n",
      "D loss: 1.071873, acc: 99% // tar acc: 98% // adv loss: 0.501563, aux loss: 1.461322, tar loss: 0.709076\n",
      "=====================\n",
      "Epoch 135/200, Batch 370/938\n",
      "D loss: 1.087519, acc: 96% // tar acc: 96% // adv loss: 0.606152, aux loss: 1.477745, tar loss: 0.702374\n",
      "=====================\n",
      "Epoch 135/200, Batch 870/938\n",
      "D loss: 1.078004, acc: 96% // tar acc: 95% // adv loss: 0.472786, aux loss: 1.461213, tar loss: 0.730058\n",
      "=====================\n",
      "Epoch 136/200, Batch 432/938\n",
      "D loss: 1.093017, acc: 99% // tar acc: 100% // adv loss: 0.562183, aux loss: 1.461173, tar loss: 0.723781\n",
      "=====================\n",
      "Epoch 136/200, Batch 932/938\n",
      "D loss: 1.048115, acc: 97% // tar acc: 95% // adv loss: 0.728738, aux loss: 1.461815, tar loss: 0.732935\n",
      "=====================\n",
      "Epoch 137/200, Batch 494/938\n",
      "D loss: 1.079461, acc: 100% // tar acc: 95% // adv loss: 0.461230, aux loss: 1.461156, tar loss: 0.730507\n",
      "=====================\n",
      "Epoch 138/200, Batch 56/938\n",
      "D loss: 1.043906, acc: 99% // tar acc: 100% // adv loss: 0.644712, aux loss: 1.461211, tar loss: 0.726777\n",
      "=====================\n",
      "Epoch 138/200, Batch 556/938\n",
      "D loss: 1.104459, acc: 98% // tar acc: 98% // adv loss: 0.480885, aux loss: 1.461522, tar loss: 0.748450\n",
      "=====================\n",
      "Epoch 139/200, Batch 118/938\n",
      "D loss: 1.084892, acc: 97% // tar acc: 98% // adv loss: 0.581921, aux loss: 1.490280, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 139/200, Batch 618/938\n",
      "D loss: 1.017149, acc: 97% // tar acc: 100% // adv loss: 0.514794, aux loss: 1.472464, tar loss: 0.728100\n",
      "=====================\n",
      "Epoch 140/200, Batch 180/938\n",
      "D loss: 1.223346, acc: 96% // tar acc: 100% // adv loss: 0.608131, aux loss: 1.461180, tar loss: 0.750654\n",
      "=====================\n",
      "Epoch 140/200, Batch 680/938\n",
      "D loss: 1.090308, acc: 98% // tar acc: 96% // adv loss: 0.540526, aux loss: 1.461745, tar loss: 0.732094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 141/200, Batch 242/938\n",
      "D loss: 1.041348, acc: 100% // tar acc: 96% // adv loss: 0.601877, aux loss: 1.461156, tar loss: 0.728066\n",
      "=====================\n",
      "Epoch 141/200, Batch 742/938\n",
      "D loss: 0.981212, acc: 98% // tar acc: 98% // adv loss: 0.453854, aux loss: 1.476413, tar loss: 0.740815\n",
      "=====================\n",
      "Epoch 142/200, Batch 304/938\n",
      "D loss: 1.031346, acc: 99% // tar acc: 96% // adv loss: 0.597584, aux loss: 1.461367, tar loss: 0.737435\n",
      "=====================\n",
      "Epoch 142/200, Batch 804/938\n",
      "D loss: 1.046610, acc: 96% // tar acc: 96% // adv loss: 0.464298, aux loss: 1.466286, tar loss: 0.727960\n",
      "=====================\n",
      "Epoch 143/200, Batch 366/938\n",
      "D loss: 1.082028, acc: 99% // tar acc: 96% // adv loss: 0.458199, aux loss: 1.461507, tar loss: 0.752208\n",
      "=====================\n",
      "Epoch 143/200, Batch 866/938\n",
      "D loss: 1.009441, acc: 100% // tar acc: 98% // adv loss: 0.515554, aux loss: 1.462531, tar loss: 0.728422\n",
      "=====================\n",
      "Epoch 144/200, Batch 428/938\n",
      "D loss: 1.071695, acc: 100% // tar acc: 93% // adv loss: 0.352840, aux loss: 1.462008, tar loss: 0.745982\n",
      "=====================\n",
      "Epoch 144/200, Batch 928/938\n",
      "D loss: 1.089451, acc: 98% // tar acc: 100% // adv loss: 0.574898, aux loss: 1.463191, tar loss: 0.706876\n",
      "=====================\n",
      "Epoch 145/200, Batch 490/938\n",
      "D loss: 1.075166, acc: 98% // tar acc: 98% // adv loss: 0.569247, aux loss: 1.461713, tar loss: 0.741454\n",
      "=====================\n",
      "Epoch 146/200, Batch 52/938\n",
      "D loss: 1.123827, acc: 99% // tar acc: 96% // adv loss: 0.347674, aux loss: 1.476444, tar loss: 0.734738\n",
      "=====================\n",
      "Epoch 146/200, Batch 552/938\n",
      "D loss: 1.060051, acc: 97% // tar acc: 96% // adv loss: 0.582986, aux loss: 1.461159, tar loss: 0.708578\n",
      "=====================\n",
      "Epoch 147/200, Batch 114/938\n",
      "D loss: 1.033841, acc: 99% // tar acc: 95% // adv loss: 0.506320, aux loss: 1.462499, tar loss: 0.694930\n",
      "=====================\n",
      "Epoch 147/200, Batch 614/938\n",
      "D loss: 1.088505, acc: 98% // tar acc: 95% // adv loss: 0.531524, aux loss: 1.461199, tar loss: 0.676738\n",
      "=====================\n",
      "Epoch 148/200, Batch 176/938\n",
      "D loss: 1.071406, acc: 98% // tar acc: 98% // adv loss: 0.556715, aux loss: 1.461609, tar loss: 0.752558\n",
      "=====================\n",
      "Epoch 148/200, Batch 676/938\n",
      "D loss: 1.112900, acc: 97% // tar acc: 92% // adv loss: 0.491888, aux loss: 1.462648, tar loss: 0.705688\n",
      "=====================\n",
      "Epoch 149/200, Batch 238/938\n",
      "D loss: 1.081188, acc: 97% // tar acc: 98% // adv loss: 0.410044, aux loss: 1.478405, tar loss: 0.726248\n",
      "=====================\n",
      "Epoch 149/200, Batch 738/938\n",
      "D loss: 1.064528, acc: 97% // tar acc: 95% // adv loss: 0.669234, aux loss: 1.461732, tar loss: 0.736040\n",
      "=====================\n",
      "Epoch 150/200, Batch 300/938\n",
      "D loss: 1.153935, acc: 97% // tar acc: 93% // adv loss: 0.373661, aux loss: 1.474512, tar loss: 0.692771\n",
      "=====================\n",
      "Epoch 150/200, Batch 800/938\n",
      "D loss: 1.109648, acc: 98% // tar acc: 95% // adv loss: 0.584404, aux loss: 1.482259, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 151/200, Batch 362/938\n",
      "D loss: 1.021407, acc: 99% // tar acc: 95% // adv loss: 0.626005, aux loss: 1.461267, tar loss: 0.714831\n",
      "=====================\n",
      "Epoch 151/200, Batch 862/938\n",
      "D loss: 1.061623, acc: 96% // tar acc: 95% // adv loss: 0.626294, aux loss: 1.477489, tar loss: 0.710966\n",
      "=====================\n",
      "Epoch 152/200, Batch 424/938\n",
      "D loss: 1.117101, acc: 97% // tar acc: 100% // adv loss: 0.424369, aux loss: 1.480133, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 152/200, Batch 924/938\n",
      "D loss: 1.056849, acc: 99% // tar acc: 98% // adv loss: 0.762476, aux loss: 1.461173, tar loss: 0.755183\n",
      "=====================\n",
      "Epoch 153/200, Batch 486/938\n",
      "D loss: 1.093654, acc: 98% // tar acc: 98% // adv loss: 0.575381, aux loss: 1.464041, tar loss: 0.730675\n",
      "=====================\n",
      "Epoch 154/200, Batch 48/938\n",
      "D loss: 1.070709, acc: 100% // tar acc: 98% // adv loss: 0.465203, aux loss: 1.476742, tar loss: 0.721666\n",
      "=====================\n",
      "Epoch 154/200, Batch 548/938\n",
      "D loss: 1.112900, acc: 99% // tar acc: 100% // adv loss: 0.366914, aux loss: 1.461181, tar loss: 0.740002\n",
      "=====================\n",
      "Epoch 155/200, Batch 110/938\n",
      "D loss: 1.096444, acc: 97% // tar acc: 95% // adv loss: 0.503019, aux loss: 1.475415, tar loss: 0.705870\n",
      "=====================\n",
      "Epoch 155/200, Batch 610/938\n",
      "D loss: 1.013457, acc: 96% // tar acc: 93% // adv loss: 0.516516, aux loss: 1.475469, tar loss: 0.713988\n",
      "=====================\n",
      "Epoch 156/200, Batch 172/938\n",
      "D loss: 1.063692, acc: 98% // tar acc: 100% // adv loss: 0.538238, aux loss: 1.467735, tar loss: 0.713816\n",
      "=====================\n",
      "Epoch 156/200, Batch 672/938\n",
      "D loss: 1.104073, acc: 98% // tar acc: 93% // adv loss: 0.475762, aux loss: 1.461571, tar loss: 0.707071\n",
      "=====================\n",
      "Epoch 157/200, Batch 234/938\n",
      "D loss: 1.047342, acc: 98% // tar acc: 98% // adv loss: 0.661972, aux loss: 1.461417, tar loss: 0.733254\n",
      "=====================\n",
      "Epoch 157/200, Batch 734/938\n",
      "D loss: 1.036624, acc: 100% // tar acc: 98% // adv loss: 0.718632, aux loss: 1.461214, tar loss: 0.723441\n",
      "=====================\n",
      "Epoch 158/200, Batch 296/938\n",
      "D loss: 1.008487, acc: 97% // tar acc: 96% // adv loss: 0.600635, aux loss: 1.462492, tar loss: 0.717126\n",
      "=====================\n",
      "Epoch 158/200, Batch 796/938\n",
      "D loss: 1.024726, acc: 97% // tar acc: 98% // adv loss: 0.594725, aux loss: 1.476733, tar loss: 0.700450\n",
      "=====================\n",
      "Epoch 159/200, Batch 358/938\n",
      "D loss: 1.062797, acc: 97% // tar acc: 95% // adv loss: 0.617474, aux loss: 1.462350, tar loss: 0.731882\n",
      "=====================\n",
      "Epoch 159/200, Batch 858/938\n",
      "D loss: 1.043717, acc: 98% // tar acc: 100% // adv loss: 0.479636, aux loss: 1.461889, tar loss: 0.713667\n",
      "=====================\n",
      "Epoch 160/200, Batch 420/938\n",
      "D loss: 1.140245, acc: 99% // tar acc: 100% // adv loss: 0.369711, aux loss: 1.461541, tar loss: 0.754538\n",
      "=====================\n",
      "Epoch 160/200, Batch 920/938\n",
      "D loss: 1.061592, acc: 96% // tar acc: 95% // adv loss: 0.495587, aux loss: 1.480073, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 161/200, Batch 482/938\n",
      "D loss: 1.037772, acc: 99% // tar acc: 93% // adv loss: 0.509046, aux loss: 1.461192, tar loss: 0.717405\n",
      "=====================\n",
      "Epoch 162/200, Batch 44/938\n",
      "D loss: 1.114591, acc: 100% // tar acc: 98% // adv loss: 0.497018, aux loss: 1.461156, tar loss: 0.737286\n",
      "=====================\n",
      "Epoch 162/200, Batch 544/938\n",
      "D loss: 1.042597, acc: 98% // tar acc: 98% // adv loss: 0.391196, aux loss: 1.462122, tar loss: 0.735324\n",
      "=====================\n",
      "Epoch 163/200, Batch 106/938\n",
      "D loss: 1.056287, acc: 96% // tar acc: 93% // adv loss: 0.634153, aux loss: 1.462409, tar loss: 0.730884\n",
      "=====================\n",
      "Epoch 163/200, Batch 606/938\n",
      "D loss: 1.096351, acc: 98% // tar acc: 96% // adv loss: 0.681243, aux loss: 1.461549, tar loss: 0.716051\n",
      "=====================\n",
      "Epoch 164/200, Batch 168/938\n",
      "D loss: 1.103051, acc: 97% // tar acc: 100% // adv loss: 0.685803, aux loss: 1.463467, tar loss: 0.729987\n",
      "=====================\n",
      "Epoch 164/200, Batch 668/938\n",
      "D loss: 1.111960, acc: 96% // tar acc: 98% // adv loss: 0.389385, aux loss: 1.461160, tar loss: 0.720100\n",
      "=====================\n",
      "Epoch 165/200, Batch 230/938\n",
      "D loss: 1.067764, acc: 96% // tar acc: 93% // adv loss: 0.541101, aux loss: 1.461291, tar loss: 0.730160\n",
      "=====================\n",
      "Epoch 165/200, Batch 730/938\n",
      "D loss: 1.093924, acc: 98% // tar acc: 98% // adv loss: 0.510439, aux loss: 1.461228, tar loss: 0.720491\n",
      "=====================\n",
      "Epoch 166/200, Batch 292/938\n",
      "D loss: 1.043264, acc: 97% // tar acc: 98% // adv loss: 0.488525, aux loss: 1.461275, tar loss: 0.742485\n",
      "=====================\n",
      "Epoch 166/200, Batch 792/938\n",
      "D loss: 1.065591, acc: 97% // tar acc: 96% // adv loss: 0.628244, aux loss: 1.476585, tar loss: 0.709908\n",
      "=====================\n",
      "Epoch 167/200, Batch 354/938\n",
      "D loss: 1.103997, acc: 99% // tar acc: 95% // adv loss: 0.508816, aux loss: 1.461975, tar loss: 0.720080\n",
      "=====================\n",
      "Epoch 167/200, Batch 854/938\n",
      "D loss: 1.093888, acc: 96% // tar acc: 93% // adv loss: 0.429545, aux loss: 1.472597, tar loss: 0.734078\n",
      "=====================\n",
      "Epoch 168/200, Batch 416/938\n",
      "D loss: 1.059063, acc: 100% // tar acc: 100% // adv loss: 0.652955, aux loss: 1.461190, tar loss: 0.695198\n",
      "=====================\n",
      "Epoch 168/200, Batch 916/938\n",
      "D loss: 1.094236, acc: 98% // tar acc: 96% // adv loss: 0.593958, aux loss: 1.464178, tar loss: 0.715942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 169/200, Batch 478/938\n",
      "D loss: 1.097578, acc: 96% // tar acc: 95% // adv loss: 0.630628, aux loss: 1.490201, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 170/200, Batch 40/938\n",
      "D loss: 1.085714, acc: 100% // tar acc: 96% // adv loss: 0.467572, aux loss: 1.462662, tar loss: 0.741092\n",
      "=====================\n",
      "Epoch 170/200, Batch 540/938\n",
      "D loss: 1.081771, acc: 98% // tar acc: 96% // adv loss: 0.778774, aux loss: 1.476852, tar loss: 0.722795\n",
      "=====================\n",
      "Epoch 171/200, Batch 102/938\n",
      "D loss: 1.049416, acc: 98% // tar acc: 90% // adv loss: 0.572177, aux loss: 1.476108, tar loss: 0.721377\n",
      "=====================\n",
      "Epoch 171/200, Batch 602/938\n",
      "D loss: 1.066619, acc: 96% // tar acc: 98% // adv loss: 0.512080, aux loss: 1.471441, tar loss: 0.736313\n",
      "=====================\n",
      "Epoch 172/200, Batch 164/938\n",
      "D loss: 1.006714, acc: 98% // tar acc: 100% // adv loss: 0.579929, aux loss: 1.461933, tar loss: 0.765095\n",
      "=====================\n",
      "Epoch 172/200, Batch 664/938\n",
      "D loss: 1.055970, acc: 99% // tar acc: 95% // adv loss: 0.352413, aux loss: 1.461166, tar loss: 0.732516\n",
      "=====================\n",
      "Epoch 173/200, Batch 226/938\n",
      "D loss: 1.085546, acc: 97% // tar acc: 98% // adv loss: 0.455581, aux loss: 1.463374, tar loss: 0.709247\n",
      "=====================\n",
      "Epoch 173/200, Batch 726/938\n",
      "D loss: 1.042917, acc: 99% // tar acc: 95% // adv loss: 0.480332, aux loss: 1.499921, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 174/200, Batch 288/938\n",
      "D loss: 1.111910, acc: 99% // tar acc: 100% // adv loss: 0.680715, aux loss: 1.461223, tar loss: 0.773177\n",
      "=====================\n",
      "Epoch 174/200, Batch 788/938\n",
      "D loss: 1.066078, acc: 98% // tar acc: 95% // adv loss: 0.500114, aux loss: 1.481006, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 175/200, Batch 350/938\n",
      "D loss: 1.021943, acc: 100% // tar acc: 95% // adv loss: 0.554046, aux loss: 1.461168, tar loss: 0.679625\n",
      "=====================\n",
      "Epoch 175/200, Batch 850/938\n",
      "D loss: 1.091917, acc: 97% // tar acc: 100% // adv loss: 0.459354, aux loss: 1.470302, tar loss: 0.747784\n",
      "=====================\n",
      "Epoch 176/200, Batch 412/938\n",
      "D loss: 1.124583, acc: 96% // tar acc: 98% // adv loss: 0.540080, aux loss: 1.461235, tar loss: 0.733605\n",
      "=====================\n",
      "Epoch 176/200, Batch 912/938\n",
      "D loss: 1.070360, acc: 98% // tar acc: 96% // adv loss: 0.458175, aux loss: 1.476222, tar loss: 0.729247\n",
      "=====================\n",
      "Epoch 177/200, Batch 474/938\n",
      "D loss: 1.007844, acc: 100% // tar acc: 90% // adv loss: 0.555540, aux loss: 1.475886, tar loss: 0.693719\n",
      "=====================\n",
      "Epoch 178/200, Batch 36/938\n",
      "D loss: 1.117716, acc: 100% // tar acc: 98% // adv loss: 0.479826, aux loss: 1.461163, tar loss: 0.704947\n",
      "=====================\n",
      "Epoch 178/200, Batch 536/938\n",
      "D loss: 1.070714, acc: 99% // tar acc: 98% // adv loss: 0.625394, aux loss: 1.473717, tar loss: 0.721846\n",
      "=====================\n",
      "Epoch 179/200, Batch 98/938\n",
      "D loss: 1.075622, acc: 98% // tar acc: 100% // adv loss: 0.530398, aux loss: 1.470612, tar loss: 0.727946\n",
      "=====================\n",
      "Epoch 179/200, Batch 598/938\n",
      "D loss: 1.125446, acc: 96% // tar acc: 95% // adv loss: 0.431534, aux loss: 1.461255, tar loss: 0.720729\n",
      "=====================\n",
      "Epoch 180/200, Batch 160/938\n",
      "D loss: 1.039481, acc: 96% // tar acc: 95% // adv loss: 0.468186, aux loss: 1.462595, tar loss: 0.730058\n",
      "=====================\n",
      "Epoch 180/200, Batch 660/938\n",
      "D loss: 1.023955, acc: 98% // tar acc: 96% // adv loss: 0.631413, aux loss: 1.461189, tar loss: 0.697790\n",
      "=====================\n",
      "Epoch 181/200, Batch 222/938\n",
      "D loss: 1.081970, acc: 98% // tar acc: 100% // adv loss: 0.580306, aux loss: 1.462651, tar loss: 0.710961\n",
      "=====================\n",
      "Epoch 181/200, Batch 722/938\n",
      "D loss: 1.044387, acc: 99% // tar acc: 96% // adv loss: 0.661269, aux loss: 1.462059, tar loss: 0.714670\n",
      "=====================\n",
      "Epoch 182/200, Batch 284/938\n",
      "D loss: 1.052520, acc: 97% // tar acc: 100% // adv loss: 0.512901, aux loss: 1.461475, tar loss: 0.733433\n",
      "=====================\n",
      "Epoch 182/200, Batch 784/938\n",
      "D loss: 1.098046, acc: 98% // tar acc: 98% // adv loss: 0.501779, aux loss: 1.463985, tar loss: 0.732559\n",
      "=====================\n",
      "Epoch 183/200, Batch 346/938\n",
      "D loss: 1.106218, acc: 100% // tar acc: 98% // adv loss: 0.604960, aux loss: 1.461203, tar loss: 0.731958\n",
      "=====================\n",
      "Epoch 183/200, Batch 846/938\n",
      "D loss: 1.042602, acc: 99% // tar acc: 98% // adv loss: 0.392923, aux loss: 1.463717, tar loss: 0.724076\n",
      "=====================\n",
      "Epoch 184/200, Batch 408/938\n",
      "D loss: 1.073920, acc: 98% // tar acc: 92% // adv loss: 0.632643, aux loss: 1.461264, tar loss: 0.714635\n",
      "=====================\n",
      "Epoch 184/200, Batch 908/938\n",
      "D loss: 1.142921, acc: 96% // tar acc: 96% // adv loss: 0.586076, aux loss: 1.491865, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 185/200, Batch 470/938\n",
      "D loss: 1.054886, acc: 98% // tar acc: 95% // adv loss: 0.450455, aux loss: 1.461250, tar loss: 0.707982\n",
      "=====================\n",
      "Epoch 186/200, Batch 32/938\n",
      "D loss: 1.051166, acc: 98% // tar acc: 98% // adv loss: 0.517008, aux loss: 1.466855, tar loss: 0.733800\n",
      "=====================\n",
      "Epoch 186/200, Batch 532/938\n",
      "D loss: 1.132021, acc: 99% // tar acc: 95% // adv loss: 0.522301, aux loss: 1.476726, tar loss: 0.698570\n",
      "=====================\n",
      "Epoch 187/200, Batch 94/938\n",
      "D loss: 1.022754, acc: 99% // tar acc: 100% // adv loss: 0.827767, aux loss: 1.461153, tar loss: 0.693518\n",
      "=====================\n",
      "Epoch 187/200, Batch 594/938\n",
      "D loss: 1.104858, acc: 99% // tar acc: 98% // adv loss: 0.460006, aux loss: 1.476898, tar loss: 0.749963\n",
      "=====================\n",
      "Epoch 188/200, Batch 156/938\n",
      "D loss: 1.053865, acc: 97% // tar acc: 98% // adv loss: 0.534951, aux loss: 1.476911, tar loss: 0.746308\n",
      "=====================\n",
      "Epoch 188/200, Batch 656/938\n",
      "D loss: 1.024903, acc: 99% // tar acc: 92% // adv loss: 0.732211, aux loss: 1.461195, tar loss: 0.736720\n",
      "=====================\n",
      "Epoch 189/200, Batch 218/938\n",
      "D loss: 1.066721, acc: 100% // tar acc: 96% // adv loss: 0.702010, aux loss: 1.462000, tar loss: 0.743487\n",
      "=====================\n",
      "Epoch 189/200, Batch 718/938\n",
      "D loss: 1.073018, acc: 100% // tar acc: 98% // adv loss: 0.471777, aux loss: 1.466929, tar loss: 0.747931\n",
      "=====================\n",
      "Epoch 190/200, Batch 280/938\n",
      "D loss: 1.122339, acc: 97% // tar acc: 100% // adv loss: 0.497702, aux loss: 1.461163, tar loss: 0.756455\n",
      "=====================\n",
      "Epoch 190/200, Batch 780/938\n",
      "D loss: 1.036879, acc: 96% // tar acc: 96% // adv loss: 0.575264, aux loss: 1.461193, tar loss: 0.719702\n",
      "=====================\n",
      "Epoch 191/200, Batch 342/938\n",
      "D loss: 1.063896, acc: 99% // tar acc: 100% // adv loss: 0.539095, aux loss: 1.461174, tar loss: 0.754995\n",
      "=====================\n",
      "Epoch 191/200, Batch 842/938\n",
      "D loss: 1.066643, acc: 98% // tar acc: 98% // adv loss: 0.519588, aux loss: 1.461191, tar loss: 0.735953\n",
      "=====================\n",
      "Epoch 192/200, Batch 404/938\n",
      "D loss: 1.082036, acc: 99% // tar acc: 98% // adv loss: 0.473494, aux loss: 1.461194, tar loss: 0.696273\n",
      "=====================\n",
      "Epoch 192/200, Batch 904/938\n",
      "D loss: 1.101370, acc: 97% // tar acc: 98% // adv loss: 0.652353, aux loss: 1.461206, tar loss: 0.714455\n",
      "=====================\n",
      "Epoch 193/200, Batch 466/938\n",
      "D loss: 1.081990, acc: 98% // tar acc: 95% // adv loss: 0.427848, aux loss: 1.461152, tar loss: 0.763811\n",
      "=====================\n",
      "Epoch 194/200, Batch 28/938\n",
      "D loss: 1.066514, acc: 96% // tar acc: 98% // adv loss: 0.522527, aux loss: 1.461422, tar loss: 0.716923\n",
      "=====================\n",
      "Epoch 194/200, Batch 528/938\n",
      "D loss: 1.097660, acc: 97% // tar acc: 98% // adv loss: 0.460128, aux loss: 1.462683, tar loss: 0.700370\n",
      "=====================\n",
      "Epoch 195/200, Batch 90/938\n",
      "D loss: 1.144025, acc: 99% // tar acc: 96% // adv loss: 0.479096, aux loss: 1.461911, tar loss: 0.732187\n",
      "=====================\n",
      "Epoch 195/200, Batch 590/938\n",
      "D loss: 1.093062, acc: 99% // tar acc: 95% // adv loss: 0.347398, aux loss: 1.462563, tar loss: 0.714863\n",
      "=====================\n",
      "Epoch 196/200, Batch 152/938\n",
      "D loss: 1.038452, acc: 99% // tar acc: 93% // adv loss: 0.544614, aux loss: 1.461296, tar loss: 0.684699\n",
      "=====================\n",
      "Epoch 196/200, Batch 652/938\n",
      "D loss: 1.095401, acc: 95% // tar acc: 100% // adv loss: 0.463991, aux loss: 1.465091, tar loss: 0.759075\n",
      "=====================\n",
      "Epoch 197/200, Batch 214/938\n",
      "D loss: 1.098987, acc: 96% // tar acc: 96% // adv loss: 0.513795, aux loss: 1.476811, tar loss: 0.684788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 197/200, Batch 714/938\n",
      "D loss: 1.090556, acc: 100% // tar acc: 96% // adv loss: 0.521840, aux loss: 1.462583, tar loss: 0.715464\n",
      "=====================\n",
      "Epoch 198/200, Batch 276/938\n",
      "D loss: 1.110210, acc: 99% // tar acc: 95% // adv loss: 0.413220, aux loss: 1.475931, tar loss: 0.735903\n",
      "=====================\n",
      "Epoch 198/200, Batch 776/938\n",
      "D loss: 1.025543, acc: 97% // tar acc: 95% // adv loss: 0.633249, aux loss: 1.491300, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 199/200, Batch 338/938\n",
      "D loss: 1.039352, acc: 99% // tar acc: 100% // adv loss: 0.474189, aux loss: 1.461372, tar loss: 0.739939\n",
      "=====================\n",
      "Epoch 199/200, Batch 838/938\n",
      "D loss: 1.096465, acc: 96% // tar acc: 96% // adv loss: 0.363089, aux loss: 1.461183, tar loss: 0.750407\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import csv\n",
    "f = open('../models/Run2_log.csv', 'a')\n",
    "log_writer = csv.writer(f, delimiter=',')\n",
    "log_writer.writerow(['Epoch', 'Batch', 'DLoss', 'DAcc', 'TarAcc', 'AdvLoss', 'AuxLoss', 'TarLoss', 'GLoss'])\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        \n",
    "        batch_size = imgs.shape[0]\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity, pred_label = discriminator(gen_imgs)\n",
    "        target_classifier_pred_label = target_classifier(gen_imgs)\n",
    "        \n",
    "        t_acc = np.mean(np.argmax(target_classifier_pred_label.data.cpu().numpy(), axis=1) == gen_labels.data.cpu().numpy())\n",
    "        \n",
    "        adv_loss = adv_loss_coeff * adversarial_loss(validity, valid)\n",
    "        aux_loss = aux_loss_coeff * auxiliary_loss(pred_label, gen_labels)\n",
    "        tar_loss = tar_loss_coeff * get_target_loss(adv_loss, aux_loss, target_classifier_pred_label, gen_labels)\n",
    "        g_loss = adv_loss + aux_loss + tar_loss\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_pred, real_aux = discriminator(real_imgs)\n",
    "        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "        # Loss for fake images\n",
    "        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = d_real_loss_coeff * d_real_loss + d_fake_loss_coeff * d_fake_loss\n",
    "\n",
    "        # Calculate discriminator accuracy\n",
    "        pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "        gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "        d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        log_writer.writerow([epoch, i, d_loss.item(), 100*d_acc, 100*t_acc, adv_loss.item(), aux_loss.item(), tar_loss.item(), g_loss.item()])\n",
    "        \n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_image(n_row=10, batches_done=batches_done)\n",
    "            # Saves weights\n",
    "            torch.save(generator.state_dict(), \"../models/Success2_G\")\n",
    "            torch.save(discriminator.state_dict(), \"../models/Success2_D\")\n",
    "            print(\n",
    "                \"=====================\\nEpoch %d/%d, Batch %d/%d\\nD loss: %f, acc: %d%% // tar acc: %d%% // adv loss: %f, aux loss: %f, tar loss: %f\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), 100 * d_acc, 100 * t_acc, adv_loss.item(), aux_loss.item(), tar_loss.item())\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves weights\n",
    "torch.save(generator.state_dict(), \"../models/Success2_G\")\n",
    "torch.save(discriminator.state_dict(), \"../models/Success2_D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "batch_size = 1000\n",
    "z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
    "\n",
    "# Generate a batch of images\n",
    "gen_imgs = generator(z, gen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Gen Label: 9; LeNet Label: 3')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots a sample\n",
    "import matplotlib.pyplot as plt\n",
    "sample_idx = 28\n",
    "print(gen_labels[sample_idx])\n",
    "plt.imshow(gen_imgs[sample_idx][0].cpu().detach().numpy(), cmap='gray', interpolation='none')\n",
    "plt.title(\"Gen Label: \" + str(gen_labels.cpu().detach().numpy()[sample_idx]) + \"; LeNet Label: \" + str(np.argmax(pred_labels.data.cpu().numpy()[sample_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds target classification for sample\n",
    "pred_labels = target_classifier(gen_imgs)\n",
    "np.argmax(pred_labels.data.cpu().numpy()[sample_idx])\n",
    "#pred_labels.data.cpu().numpy()[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 28, 120, 163, 301, 308, 338, 357, 381, 411, 473, 481, 630, 671,\n",
       "        733, 763, 859, 939, 950, 952, 954, 961, 970]),)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds indexes that have adversarial examples\n",
    "t_acc = np.argmax(pred_labels.data.cpu().numpy(), axis=1) == gen_labels.data.cpu().numpy()\n",
    "np.where(t_acc == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen = gen_imgs.cpu().detach().numpy()\n",
    "preds = pred_labels.data.cpu().numpy()\n",
    "true = gen_labels.data.cpu().numpy()\n",
    "for i in range(len(true)):\n",
    "    if np.argmax(preds[i]) != true[i]:\n",
    "        plt.title(\"Gen Label: \" + str(true[i]) + \"; LeNet Label: \" + str(np.argmax(preds[i])))\n",
    "        plt.imshow(gen[i][0], cmap='gray')\n",
    "        plt.savefig(\"../images/SecondRun/AdversarialExamples/\" + str(1000+i) + \".png\")\n",
    "        \n",
    "#for i in range(len(true)):\n",
    "#    plt.title(\"Gen Label: \" + str(true[i]) + \"; LeNet Label: \" + str(np.argmax(preds[i])))\n",
    "#    plt.imshow(gen[i][0], cmap='gray')\n",
    "#    plt.savefig(\"../images/SecondRun/AllSamples/\" + str(i) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra saving\n",
    "torch.save(gen_imgs, \"../models/Success2_GenSample\")\n",
    "torch.save(gen_labels, \"../models/Success2_GenLabels\")\n",
    "torch.save(pred_labels, \"../models/Success2_LeNetLabels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
