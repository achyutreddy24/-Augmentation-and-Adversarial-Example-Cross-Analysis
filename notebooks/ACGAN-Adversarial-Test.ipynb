{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code:\n",
    "# https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/acgan/acgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# Models\n",
    "from torchvision.models import vgg19\n",
    "from sys import path\n",
    "path.append(\"../utils\")\n",
    "from models import LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "cuda = True\n",
    "\n",
    "n_epochs=200\n",
    "batch_size=64\n",
    "lr=0.0002\n",
    "b1=0.5\n",
    "b2=0.999\n",
    "latent_dim=100\n",
    "n_classes=10\n",
    "img_size=28\n",
    "channels=1\n",
    "sample_interval=500\n",
    "\n",
    "d_real_loss_coeff = 0.6\n",
    "d_fake_loss_coeff = 0.4\n",
    "\n",
    "adv_loss_coeff = 1\n",
    "aux_loss_coeff = 1\n",
    "tar_loss_coeff = .05\n",
    "\n",
    "tar_loss_default = 22.2  # This is equal to the max possible tar_loss value\n",
    "\n",
    "# target classifier conditional constants\n",
    "adv_loss_threshold = 0.9\n",
    "aux_loss_threshold = 1.48\n",
    "\n",
    "lenet5_state_path = \"../utils/models/trained_lenet5.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions for fixed target classifier\n",
    "\n",
    "def load_pytorch_model():\n",
    "    # Load model from torchvision.models (NOTE: VVG19 IS NOT FOR MNIST, DON'T USE)\n",
    "    model = vgg19(pretrained=True)\n",
    "    model.eval()\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    \n",
    "    for p in model.features.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.classifier.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_LeNet5():\n",
    "    net = LeNet5()\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    # remove map location = cpu if using cuda\n",
    "    net.load_state_dict(torch.load(lenet5_state_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    # set model to eval mode so nothing is changed\n",
    "    net.eval()\n",
    "    \n",
    "    return net\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(n_classes, latent_dim)\n",
    "\n",
    "        self.init_size = img_size // 4  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = 2#img_size // 2 ** 4\n",
    "\n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, n_classes), nn.Softmax())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "\n",
    "        return validity, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "adversarial_loss = nn.BCELoss()\n",
    "auxiliary_loss = nn.CrossEntropyLoss()\n",
    "target_classifier_loss = nn.CrossEntropyLoss() # negate target classifier output when passing to this loss function\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Load target classifier\n",
    "\n",
    "target_classifier = load_LeNet5()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()\n",
    "    target_classifier = target_classifier.cuda()\n",
    "    target_classifier_loss = target_classifier_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs(\"../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, \"../images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def shouldCalculateTargetLoss(validity, pred_label, label):\n",
    "    # Only works if batch size is 1, so just uses the first value\n",
    "    return (validity > threshold_valid and  # validity is above threshold\n",
    "        pred_label[label] == np.amax(pred_label) and  # classification is correct\n",
    "        pred_label[label] > threshold_classification)  # classification confidence is above threshold\n",
    "\n",
    "def get_target_loss(validity, pred_labels, label, target_classification):\n",
    "    # Apply conditional loss to each output individually\n",
    "    for i in range(target_classification.size(0)):\n",
    "        if shouldCalculateTargetLoss(validity[i], pred_labels[i], label[i]):\n",
    "            target_classification[i] *= -1\n",
    "            \n",
    "            \n",
    "    loss = target_classifier_loss(target_classification, gen_labels)\n",
    "    \n",
    "    print(\"Count of loss including target loss:\", torch.sum(loss_mult))\n",
    "    \n",
    "    return loss * loss_mult\n",
    "'''\n",
    "\n",
    "\n",
    "def get_target_loss(adv_loss, aux_loss, target_classification, true_classification):\n",
    "    #print(target_classification[0])\n",
    "    if (adv_loss < adv_loss_threshold and aux_loss < aux_loss_threshold):\n",
    "        return target_classifier_loss(target_classification * -1, true_classification)\n",
    "    return Variable(FloatTensor([tar_loss_default]))\n",
    "\n",
    "def test_attack(size):\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (size, latent_dim))))\n",
    "    gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, size)))\n",
    "    imgs = generator(z, gen_labels)\n",
    "    validity, pred_label = discriminator(imgs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMPORARY SECTION: testing target classifier output\n",
    "\n",
    "z = Variable(FloatTensor(np.random.normal(0, 1, (1, latent_dim))))\n",
    "gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, 1)))\n",
    "imgs = generator(z, gen_labels)\n",
    "validity, pred_label = discriminator(imgs)\n",
    "\n",
    "#print(validity)\n",
    "\n",
    "\n",
    "#bceloss_test = nn.BCELoss()\n",
    "celoss_test = nn.CrossEntropyLoss()\n",
    "\n",
    "inp = Variable(FloatTensor([[10,10,10,10,10,10,10,10,10,-10]]))\n",
    "true = Variable(LongTensor([9]))\n",
    "\n",
    "print(celoss_test(inp, true))\n",
    "\n",
    "#print(bceloss_test(Variable(FloatTensor([[0.65]])), Variable(FloatTensor([[1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achyut/Projects/AdversarialRobustness/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 0/200, Batch 0/938\n",
      "D loss: 1.496471, acc: 10% // tar acc: 6% // adv loss: 0.680685, aux loss: 2.302219, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 0/200, Batch 500/938\n",
      "D loss: 1.244165, acc: 60% // tar acc: 32% // adv loss: 0.522499, aux loss: 2.058553, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 1/200, Batch 62/938\n",
      "D loss: 1.076492, acc: 89% // tar acc: 65% // adv loss: 0.716126, aux loss: 1.581210, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 1/200, Batch 562/938\n",
      "D loss: 1.086960, acc: 89% // tar acc: 82% // adv loss: 0.625577, aux loss: 1.530647, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 2/200, Batch 124/938\n",
      "D loss: 1.076780, acc: 95% // tar acc: 79% // adv loss: 0.657504, aux loss: 1.520543, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 2/200, Batch 624/938\n",
      "D loss: 1.050300, acc: 94% // tar acc: 79% // adv loss: 0.788152, aux loss: 1.482747, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 3/200, Batch 186/938\n",
      "D loss: 1.049540, acc: 93% // tar acc: 89% // adv loss: 0.694938, aux loss: 1.511545, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 3/200, Batch 686/938\n",
      "D loss: 1.107228, acc: 95% // tar acc: 81% // adv loss: 0.575741, aux loss: 1.522167, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 4/200, Batch 248/938\n",
      "D loss: 1.077177, acc: 92% // tar acc: 87% // adv loss: 0.877700, aux loss: 1.480148, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 4/200, Batch 748/938\n",
      "D loss: 1.077381, acc: 96% // tar acc: 84% // adv loss: 0.731820, aux loss: 1.475413, tar loss: 0.631776\n",
      "=====================\n",
      "Epoch 5/200, Batch 310/938\n",
      "D loss: 1.091099, acc: 94% // tar acc: 90% // adv loss: 0.705624, aux loss: 1.489221, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 5/200, Batch 810/938\n",
      "D loss: 1.054059, acc: 96% // tar acc: 96% // adv loss: 0.657830, aux loss: 1.498201, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 6/200, Batch 372/938\n",
      "D loss: 1.065108, acc: 97% // tar acc: 81% // adv loss: 0.619772, aux loss: 1.488894, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 6/200, Batch 872/938\n",
      "D loss: 1.046116, acc: 92% // tar acc: 95% // adv loss: 0.576890, aux loss: 1.471911, tar loss: 0.687215\n",
      "=====================\n",
      "Epoch 7/200, Batch 434/938\n",
      "D loss: 1.100193, acc: 96% // tar acc: 87% // adv loss: 0.564733, aux loss: 1.484742, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 7/200, Batch 934/938\n",
      "D loss: 1.115708, acc: 94% // tar acc: 90% // adv loss: 0.763577, aux loss: 1.471668, tar loss: 0.703887\n",
      "=====================\n",
      "Epoch 8/200, Batch 496/938\n",
      "D loss: 1.071389, acc: 96% // tar acc: 90% // adv loss: 0.644155, aux loss: 1.506686, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 9/200, Batch 58/938\n",
      "D loss: 1.047816, acc: 95% // tar acc: 92% // adv loss: 0.698019, aux loss: 1.472104, tar loss: 0.712666\n",
      "=====================\n",
      "Epoch 9/200, Batch 558/938\n",
      "D loss: 1.072197, acc: 98% // tar acc: 96% // adv loss: 0.555955, aux loss: 1.481147, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 10/200, Batch 120/938\n",
      "D loss: 1.059708, acc: 96% // tar acc: 100% // adv loss: 0.708227, aux loss: 1.466481, tar loss: 0.713185\n",
      "=====================\n",
      "Epoch 10/200, Batch 620/938\n",
      "D loss: 1.062706, acc: 97% // tar acc: 89% // adv loss: 0.546574, aux loss: 1.470683, tar loss: 0.707634\n",
      "=====================\n",
      "Epoch 11/200, Batch 182/938\n",
      "D loss: 1.073844, acc: 95% // tar acc: 95% // adv loss: 0.577916, aux loss: 1.478496, tar loss: 0.745079\n",
      "=====================\n",
      "Epoch 11/200, Batch 682/938\n",
      "D loss: 1.070634, acc: 97% // tar acc: 92% // adv loss: 0.447794, aux loss: 1.490030, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 12/200, Batch 244/938\n",
      "D loss: 1.108126, acc: 96% // tar acc: 89% // adv loss: 0.593638, aux loss: 1.494784, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 12/200, Batch 744/938\n",
      "D loss: 1.133347, acc: 96% // tar acc: 98% // adv loss: 0.668035, aux loss: 1.480025, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 13/200, Batch 306/938\n",
      "D loss: 1.083925, acc: 97% // tar acc: 95% // adv loss: 0.550354, aux loss: 1.494114, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 13/200, Batch 806/938\n",
      "D loss: 1.067971, acc: 98% // tar acc: 96% // adv loss: 0.703006, aux loss: 1.464829, tar loss: 0.733931\n",
      "=====================\n",
      "Epoch 14/200, Batch 368/938\n",
      "D loss: 1.137075, acc: 96% // tar acc: 98% // adv loss: 0.561353, aux loss: 1.475496, tar loss: 0.714497\n",
      "=====================\n",
      "Epoch 14/200, Batch 868/938\n",
      "D loss: 1.058251, acc: 98% // tar acc: 100% // adv loss: 0.436470, aux loss: 1.481476, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 15/200, Batch 430/938\n",
      "D loss: 1.120564, acc: 93% // tar acc: 98% // adv loss: 0.627464, aux loss: 1.488225, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 15/200, Batch 930/938\n",
      "D loss: 1.081524, acc: 94% // tar acc: 95% // adv loss: 0.463114, aux loss: 1.479085, tar loss: 0.718739\n",
      "=====================\n",
      "Epoch 16/200, Batch 492/938\n",
      "D loss: 1.080056, acc: 97% // tar acc: 100% // adv loss: 0.477365, aux loss: 1.464145, tar loss: 0.716385\n",
      "=====================\n",
      "Epoch 17/200, Batch 54/938\n",
      "D loss: 1.052179, acc: 95% // tar acc: 93% // adv loss: 0.493423, aux loss: 1.481734, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 17/200, Batch 554/938\n",
      "D loss: 1.052294, acc: 97% // tar acc: 93% // adv loss: 0.582270, aux loss: 1.462560, tar loss: 0.685903\n",
      "=====================\n",
      "Epoch 18/200, Batch 116/938\n",
      "D loss: 1.079087, acc: 96% // tar acc: 95% // adv loss: 0.583333, aux loss: 1.475754, tar loss: 0.718085\n",
      "=====================\n",
      "Epoch 18/200, Batch 616/938\n",
      "D loss: 1.096837, acc: 96% // tar acc: 93% // adv loss: 0.459141, aux loss: 1.463785, tar loss: 0.721622\n",
      "=====================\n",
      "Epoch 19/200, Batch 178/938\n",
      "D loss: 1.086999, acc: 96% // tar acc: 98% // adv loss: 0.486276, aux loss: 1.463708, tar loss: 0.741392\n",
      "=====================\n",
      "Epoch 19/200, Batch 678/938\n",
      "D loss: 1.104443, acc: 92% // tar acc: 96% // adv loss: 0.669745, aux loss: 1.462677, tar loss: 0.658290\n",
      "=====================\n",
      "Epoch 20/200, Batch 240/938\n",
      "D loss: 1.098897, acc: 97% // tar acc: 100% // adv loss: 0.482320, aux loss: 1.468840, tar loss: 0.686529\n",
      "=====================\n",
      "Epoch 20/200, Batch 740/938\n",
      "D loss: 1.051933, acc: 100% // tar acc: 96% // adv loss: 0.625249, aux loss: 1.487333, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 21/200, Batch 302/938\n",
      "D loss: 1.066481, acc: 96% // tar acc: 95% // adv loss: 0.703785, aux loss: 1.486516, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 21/200, Batch 802/938\n",
      "D loss: 1.110650, acc: 97% // tar acc: 96% // adv loss: 0.655384, aux loss: 1.467166, tar loss: 0.706448\n",
      "=====================\n",
      "Epoch 22/200, Batch 364/938\n",
      "D loss: 1.097746, acc: 96% // tar acc: 93% // adv loss: 0.565168, aux loss: 1.464322, tar loss: 0.696889\n",
      "=====================\n",
      "Epoch 22/200, Batch 864/938\n",
      "D loss: 1.089288, acc: 96% // tar acc: 92% // adv loss: 0.459077, aux loss: 1.507419, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 23/200, Batch 426/938\n",
      "D loss: 1.048517, acc: 96% // tar acc: 90% // adv loss: 0.601541, aux loss: 1.485859, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 23/200, Batch 926/938\n",
      "D loss: 1.106194, acc: 96% // tar acc: 89% // adv loss: 0.490078, aux loss: 1.476831, tar loss: 0.653777\n",
      "=====================\n",
      "Epoch 24/200, Batch 488/938\n",
      "D loss: 1.059771, acc: 98% // tar acc: 95% // adv loss: 0.839408, aux loss: 1.462032, tar loss: 0.699982\n",
      "=====================\n",
      "Epoch 25/200, Batch 50/938\n",
      "D loss: 1.055753, acc: 97% // tar acc: 90% // adv loss: 0.521906, aux loss: 1.462565, tar loss: 0.678337\n",
      "=====================\n",
      "Epoch 25/200, Batch 550/938\n",
      "D loss: 1.077668, acc: 96% // tar acc: 92% // adv loss: 0.807550, aux loss: 1.487752, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 26/200, Batch 112/938\n",
      "D loss: 1.027313, acc: 96% // tar acc: 95% // adv loss: 0.522815, aux loss: 1.501587, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 26/200, Batch 612/938\n",
      "D loss: 1.050415, acc: 98% // tar acc: 95% // adv loss: 0.600502, aux loss: 1.461941, tar loss: 0.671161\n",
      "=====================\n",
      "Epoch 27/200, Batch 174/938\n",
      "D loss: 1.152574, acc: 98% // tar acc: 98% // adv loss: 0.537674, aux loss: 1.477127, tar loss: 0.725610\n",
      "=====================\n",
      "Epoch 27/200, Batch 674/938\n",
      "D loss: 1.066067, acc: 99% // tar acc: 96% // adv loss: 0.684419, aux loss: 1.478985, tar loss: 0.639687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 28/200, Batch 236/938\n",
      "D loss: 1.058966, acc: 96% // tar acc: 92% // adv loss: 0.555245, aux loss: 1.484763, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 28/200, Batch 736/938\n",
      "D loss: 1.094902, acc: 97% // tar acc: 90% // adv loss: 0.513282, aux loss: 1.475943, tar loss: 0.666770\n",
      "=====================\n",
      "Epoch 29/200, Batch 298/938\n",
      "D loss: 1.100030, acc: 97% // tar acc: 98% // adv loss: 0.472176, aux loss: 1.474909, tar loss: 0.716309\n",
      "=====================\n",
      "Epoch 29/200, Batch 798/938\n",
      "D loss: 1.052245, acc: 98% // tar acc: 95% // adv loss: 0.767529, aux loss: 1.467054, tar loss: 0.667246\n",
      "=====================\n",
      "Epoch 30/200, Batch 360/938\n",
      "D loss: 1.070722, acc: 95% // tar acc: 90% // adv loss: 0.605066, aux loss: 1.479037, tar loss: 0.675978\n",
      "=====================\n",
      "Epoch 30/200, Batch 860/938\n",
      "D loss: 1.113433, acc: 96% // tar acc: 93% // adv loss: 0.701868, aux loss: 1.476196, tar loss: 0.720308\n",
      "=====================\n",
      "Epoch 31/200, Batch 422/938\n",
      "D loss: 1.088548, acc: 98% // tar acc: 93% // adv loss: 0.700036, aux loss: 1.471754, tar loss: 0.674842\n",
      "=====================\n",
      "Epoch 31/200, Batch 922/938\n",
      "D loss: 1.111251, acc: 97% // tar acc: 93% // adv loss: 0.606274, aux loss: 1.472641, tar loss: 0.689475\n",
      "=====================\n",
      "Epoch 32/200, Batch 484/938\n",
      "D loss: 1.139297, acc: 96% // tar acc: 96% // adv loss: 0.693114, aux loss: 1.479170, tar loss: 0.743585\n",
      "=====================\n",
      "Epoch 33/200, Batch 46/938\n",
      "D loss: 1.042842, acc: 96% // tar acc: 95% // adv loss: 0.525819, aux loss: 1.463261, tar loss: 0.716919\n",
      "=====================\n",
      "Epoch 33/200, Batch 546/938\n",
      "D loss: 1.017660, acc: 96% // tar acc: 98% // adv loss: 0.560419, aux loss: 1.461342, tar loss: 0.739526\n",
      "=====================\n",
      "Epoch 34/200, Batch 108/938\n",
      "D loss: 1.106311, acc: 98% // tar acc: 96% // adv loss: 0.406096, aux loss: 1.464470, tar loss: 0.715425\n",
      "=====================\n",
      "Epoch 34/200, Batch 608/938\n",
      "D loss: 1.049257, acc: 96% // tar acc: 100% // adv loss: 0.432743, aux loss: 1.465871, tar loss: 0.745291\n",
      "=====================\n",
      "Epoch 35/200, Batch 170/938\n",
      "D loss: 1.035651, acc: 98% // tar acc: 95% // adv loss: 0.740855, aux loss: 1.489376, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 35/200, Batch 670/938\n",
      "D loss: 1.083135, acc: 97% // tar acc: 98% // adv loss: 0.509045, aux loss: 1.461249, tar loss: 0.743457\n",
      "=====================\n",
      "Epoch 36/200, Batch 232/938\n",
      "D loss: 1.052132, acc: 99% // tar acc: 93% // adv loss: 0.419159, aux loss: 1.461706, tar loss: 0.717797\n",
      "=====================\n",
      "Epoch 36/200, Batch 732/938\n",
      "D loss: 1.036775, acc: 96% // tar acc: 96% // adv loss: 0.509142, aux loss: 1.478836, tar loss: 0.669379\n",
      "=====================\n",
      "Epoch 37/200, Batch 294/938\n",
      "D loss: 1.046807, acc: 97% // tar acc: 93% // adv loss: 0.531792, aux loss: 1.490463, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 37/200, Batch 794/938\n",
      "D loss: 1.087341, acc: 96% // tar acc: 96% // adv loss: 0.695133, aux loss: 1.490578, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 38/200, Batch 356/938\n",
      "D loss: 1.108280, acc: 98% // tar acc: 98% // adv loss: 0.586070, aux loss: 1.461897, tar loss: 0.763380\n",
      "=====================\n",
      "Epoch 38/200, Batch 856/938\n",
      "D loss: 1.080064, acc: 99% // tar acc: 98% // adv loss: 0.484882, aux loss: 1.493109, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 39/200, Batch 418/938\n",
      "D loss: 1.024710, acc: 97% // tar acc: 95% // adv loss: 0.331878, aux loss: 1.461567, tar loss: 0.714454\n",
      "=====================\n",
      "Epoch 39/200, Batch 918/938\n",
      "D loss: 1.001126, acc: 96% // tar acc: 98% // adv loss: 0.589631, aux loss: 1.477793, tar loss: 0.737367\n",
      "=====================\n",
      "Epoch 40/200, Batch 480/938\n",
      "D loss: 1.059770, acc: 96% // tar acc: 96% // adv loss: 0.424483, aux loss: 1.471256, tar loss: 0.732509\n",
      "=====================\n",
      "Epoch 41/200, Batch 42/938\n",
      "D loss: 1.022184, acc: 96% // tar acc: 96% // adv loss: 0.506926, aux loss: 1.461825, tar loss: 0.739525\n",
      "=====================\n",
      "Epoch 41/200, Batch 542/938\n",
      "D loss: 1.104489, acc: 96% // tar acc: 96% // adv loss: 0.589523, aux loss: 1.490680, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 42/200, Batch 104/938\n",
      "D loss: 1.059777, acc: 100% // tar acc: 100% // adv loss: 0.580160, aux loss: 1.461448, tar loss: 0.733700\n",
      "=====================\n",
      "Epoch 42/200, Batch 604/938\n",
      "D loss: 1.079088, acc: 99% // tar acc: 98% // adv loss: 0.874930, aux loss: 1.475583, tar loss: 0.689961\n",
      "=====================\n",
      "Epoch 43/200, Batch 166/938\n",
      "D loss: 1.023540, acc: 99% // tar acc: 93% // adv loss: 0.604918, aux loss: 1.465404, tar loss: 0.730042\n",
      "=====================\n",
      "Epoch 43/200, Batch 666/938\n",
      "D loss: 1.061491, acc: 99% // tar acc: 98% // adv loss: 0.538511, aux loss: 1.461870, tar loss: 0.722282\n",
      "=====================\n",
      "Epoch 44/200, Batch 228/938\n",
      "D loss: 1.079621, acc: 96% // tar acc: 98% // adv loss: 0.632799, aux loss: 1.464740, tar loss: 0.788069\n",
      "=====================\n",
      "Epoch 44/200, Batch 728/938\n",
      "D loss: 1.006669, acc: 96% // tar acc: 93% // adv loss: 0.764324, aux loss: 1.475001, tar loss: 0.690545\n",
      "=====================\n",
      "Epoch 45/200, Batch 290/938\n",
      "D loss: 1.094594, acc: 96% // tar acc: 95% // adv loss: 0.516104, aux loss: 1.495422, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 45/200, Batch 790/938\n",
      "D loss: 1.049734, acc: 98% // tar acc: 93% // adv loss: 0.596501, aux loss: 1.465103, tar loss: 0.732165\n",
      "=====================\n",
      "Epoch 46/200, Batch 352/938\n",
      "D loss: 1.061190, acc: 98% // tar acc: 93% // adv loss: 0.496740, aux loss: 1.463091, tar loss: 0.736976\n",
      "=====================\n",
      "Epoch 46/200, Batch 852/938\n",
      "D loss: 1.047239, acc: 97% // tar acc: 96% // adv loss: 0.536659, aux loss: 1.473631, tar loss: 0.701932\n",
      "=====================\n",
      "Epoch 47/200, Batch 414/938\n",
      "D loss: 1.061819, acc: 98% // tar acc: 98% // adv loss: 0.470392, aux loss: 1.472648, tar loss: 0.760568\n",
      "=====================\n",
      "Epoch 47/200, Batch 914/938\n",
      "D loss: 1.043879, acc: 98% // tar acc: 96% // adv loss: 0.733797, aux loss: 1.462017, tar loss: 0.696450\n",
      "=====================\n",
      "Epoch 48/200, Batch 476/938\n",
      "D loss: 1.039093, acc: 99% // tar acc: 98% // adv loss: 0.468535, aux loss: 1.461480, tar loss: 0.706680\n",
      "=====================\n",
      "Epoch 49/200, Batch 38/938\n",
      "D loss: 1.036323, acc: 99% // tar acc: 96% // adv loss: 0.617166, aux loss: 1.463121, tar loss: 0.762257\n",
      "=====================\n",
      "Epoch 49/200, Batch 538/938\n",
      "D loss: 1.081297, acc: 96% // tar acc: 98% // adv loss: 0.710754, aux loss: 1.461627, tar loss: 0.772043\n",
      "=====================\n",
      "Epoch 50/200, Batch 100/938\n",
      "D loss: 1.042269, acc: 98% // tar acc: 100% // adv loss: 0.478870, aux loss: 1.464742, tar loss: 0.728843\n",
      "=====================\n",
      "Epoch 50/200, Batch 600/938\n",
      "D loss: 1.101960, acc: 98% // tar acc: 93% // adv loss: 0.511603, aux loss: 1.468165, tar loss: 0.715506\n",
      "=====================\n",
      "Epoch 51/200, Batch 162/938\n",
      "D loss: 1.127373, acc: 96% // tar acc: 100% // adv loss: 0.470360, aux loss: 1.471349, tar loss: 0.744458\n",
      "=====================\n",
      "Epoch 51/200, Batch 662/938\n",
      "D loss: 1.027504, acc: 96% // tar acc: 96% // adv loss: 0.898053, aux loss: 1.461359, tar loss: 0.745590\n",
      "=====================\n",
      "Epoch 52/200, Batch 224/938\n",
      "D loss: 1.087776, acc: 98% // tar acc: 98% // adv loss: 0.681912, aux loss: 1.470662, tar loss: 0.729745\n",
      "=====================\n",
      "Epoch 52/200, Batch 724/938\n",
      "D loss: 1.063921, acc: 97% // tar acc: 98% // adv loss: 0.736638, aux loss: 1.463562, tar loss: 0.683056\n",
      "=====================\n",
      "Epoch 53/200, Batch 286/938\n",
      "D loss: 1.080345, acc: 98% // tar acc: 96% // adv loss: 0.510422, aux loss: 1.483782, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 53/200, Batch 786/938\n",
      "D loss: 1.020655, acc: 97% // tar acc: 100% // adv loss: 0.851904, aux loss: 1.463653, tar loss: 0.724717\n",
      "=====================\n",
      "Epoch 54/200, Batch 348/938\n",
      "D loss: 1.061663, acc: 100% // tar acc: 98% // adv loss: 0.590375, aux loss: 1.462284, tar loss: 0.721721\n",
      "=====================\n",
      "Epoch 54/200, Batch 848/938\n",
      "D loss: 1.095840, acc: 96% // tar acc: 96% // adv loss: 0.397875, aux loss: 1.464347, tar loss: 0.724487\n",
      "=====================\n",
      "Epoch 55/200, Batch 410/938\n",
      "D loss: 1.154916, acc: 96% // tar acc: 93% // adv loss: 0.479367, aux loss: 1.479377, tar loss: 0.707174\n",
      "=====================\n",
      "Epoch 55/200, Batch 910/938\n",
      "D loss: 1.087436, acc: 99% // tar acc: 95% // adv loss: 0.754907, aux loss: 1.480938, tar loss: 1.110000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 56/200, Batch 472/938\n",
      "D loss: 1.097376, acc: 97% // tar acc: 98% // adv loss: 0.586164, aux loss: 1.470191, tar loss: 0.727943\n",
      "=====================\n",
      "Epoch 57/200, Batch 34/938\n",
      "D loss: 1.072906, acc: 100% // tar acc: 98% // adv loss: 0.343300, aux loss: 1.470239, tar loss: 0.761386\n",
      "=====================\n",
      "Epoch 57/200, Batch 534/938\n",
      "D loss: 1.116461, acc: 99% // tar acc: 98% // adv loss: 0.456984, aux loss: 1.475827, tar loss: 0.732722\n",
      "=====================\n",
      "Epoch 58/200, Batch 96/938\n",
      "D loss: 1.039398, acc: 99% // tar acc: 98% // adv loss: 0.580406, aux loss: 1.461217, tar loss: 0.764191\n",
      "=====================\n",
      "Epoch 58/200, Batch 596/938\n",
      "D loss: 1.137089, acc: 98% // tar acc: 96% // adv loss: 0.393357, aux loss: 1.508871, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 59/200, Batch 158/938\n",
      "D loss: 1.034417, acc: 97% // tar acc: 96% // adv loss: 0.445418, aux loss: 1.477736, tar loss: 0.726244\n",
      "=====================\n",
      "Epoch 59/200, Batch 658/938\n",
      "D loss: 1.070597, acc: 98% // tar acc: 96% // adv loss: 0.538131, aux loss: 1.477825, tar loss: 0.732716\n",
      "=====================\n",
      "Epoch 60/200, Batch 220/938\n",
      "D loss: 1.083262, acc: 98% // tar acc: 100% // adv loss: 0.454038, aux loss: 1.473166, tar loss: 0.770441\n",
      "=====================\n",
      "Epoch 60/200, Batch 720/938\n",
      "D loss: 1.076534, acc: 95% // tar acc: 96% // adv loss: 0.595754, aux loss: 1.478717, tar loss: 0.708544\n",
      "=====================\n",
      "Epoch 61/200, Batch 282/938\n",
      "D loss: 1.009477, acc: 96% // tar acc: 98% // adv loss: 0.586684, aux loss: 1.462600, tar loss: 0.686252\n",
      "=====================\n",
      "Epoch 61/200, Batch 782/938\n",
      "D loss: 1.055063, acc: 97% // tar acc: 98% // adv loss: 0.520634, aux loss: 1.462269, tar loss: 0.760357\n",
      "=====================\n",
      "Epoch 62/200, Batch 344/938\n",
      "D loss: 1.054212, acc: 96% // tar acc: 98% // adv loss: 0.591908, aux loss: 1.463979, tar loss: 0.713062\n",
      "=====================\n",
      "Epoch 62/200, Batch 844/938\n",
      "D loss: 1.135780, acc: 96% // tar acc: 95% // adv loss: 0.510417, aux loss: 1.479119, tar loss: 0.738646\n",
      "=====================\n",
      "Epoch 63/200, Batch 406/938\n",
      "D loss: 1.027536, acc: 99% // tar acc: 100% // adv loss: 0.714017, aux loss: 1.469676, tar loss: 0.719592\n",
      "=====================\n",
      "Epoch 63/200, Batch 906/938\n",
      "D loss: 1.040154, acc: 99% // tar acc: 100% // adv loss: 0.382717, aux loss: 1.461303, tar loss: 0.729240\n",
      "=====================\n",
      "Epoch 64/200, Batch 468/938\n",
      "D loss: 1.060076, acc: 98% // tar acc: 98% // adv loss: 0.474562, aux loss: 1.488164, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 65/200, Batch 30/938\n",
      "D loss: 1.061099, acc: 99% // tar acc: 96% // adv loss: 0.506515, aux loss: 1.476833, tar loss: 0.722782\n",
      "=====================\n",
      "Epoch 65/200, Batch 530/938\n",
      "D loss: 1.041896, acc: 98% // tar acc: 98% // adv loss: 0.721130, aux loss: 1.462500, tar loss: 0.698070\n",
      "=====================\n",
      "Epoch 66/200, Batch 92/938\n",
      "D loss: 1.071443, acc: 100% // tar acc: 98% // adv loss: 0.653701, aux loss: 1.463455, tar loss: 0.718632\n",
      "=====================\n",
      "Epoch 66/200, Batch 592/938\n",
      "D loss: 1.113958, acc: 98% // tar acc: 98% // adv loss: 0.345972, aux loss: 1.461370, tar loss: 0.715889\n",
      "=====================\n",
      "Epoch 67/200, Batch 154/938\n",
      "D loss: 1.042124, acc: 98% // tar acc: 96% // adv loss: 0.519411, aux loss: 1.478228, tar loss: 0.698378\n",
      "=====================\n",
      "Epoch 67/200, Batch 654/938\n",
      "D loss: 1.092624, acc: 98% // tar acc: 100% // adv loss: 0.609265, aux loss: 1.461338, tar loss: 0.746960\n",
      "=====================\n",
      "Epoch 68/200, Batch 216/938\n",
      "D loss: 1.030169, acc: 99% // tar acc: 96% // adv loss: 0.428459, aux loss: 1.474106, tar loss: 0.708555\n",
      "=====================\n",
      "Epoch 68/200, Batch 716/938\n",
      "D loss: 1.056630, acc: 96% // tar acc: 98% // adv loss: 0.380755, aux loss: 1.485728, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 69/200, Batch 278/938\n",
      "D loss: 1.050871, acc: 96% // tar acc: 95% // adv loss: 0.635461, aux loss: 1.478469, tar loss: 0.673574\n",
      "=====================\n",
      "Epoch 69/200, Batch 778/938\n",
      "D loss: 1.067751, acc: 95% // tar acc: 95% // adv loss: 0.597406, aux loss: 1.472727, tar loss: 0.725271\n",
      "=====================\n",
      "Epoch 70/200, Batch 340/938\n",
      "D loss: 1.081542, acc: 97% // tar acc: 95% // adv loss: 0.610637, aux loss: 1.476696, tar loss: 0.729423\n",
      "=====================\n",
      "Epoch 70/200, Batch 840/938\n",
      "D loss: 1.098281, acc: 96% // tar acc: 96% // adv loss: 0.542700, aux loss: 1.470917, tar loss: 0.725538\n",
      "=====================\n",
      "Epoch 71/200, Batch 402/938\n",
      "D loss: 1.090563, acc: 97% // tar acc: 98% // adv loss: 0.551092, aux loss: 1.461366, tar loss: 0.738867\n",
      "=====================\n",
      "Epoch 71/200, Batch 902/938\n",
      "D loss: 1.150677, acc: 97% // tar acc: 100% // adv loss: 0.485749, aux loss: 1.463368, tar loss: 0.750552\n",
      "=====================\n",
      "Epoch 72/200, Batch 464/938\n",
      "D loss: 1.056907, acc: 99% // tar acc: 96% // adv loss: 0.425737, aux loss: 1.470156, tar loss: 0.752607\n",
      "=====================\n",
      "Epoch 73/200, Batch 26/938\n",
      "D loss: 1.011435, acc: 100% // tar acc: 95% // adv loss: 0.548382, aux loss: 1.461286, tar loss: 0.707342\n",
      "=====================\n",
      "Epoch 73/200, Batch 526/938\n",
      "D loss: 1.084314, acc: 99% // tar acc: 98% // adv loss: 0.481348, aux loss: 1.479783, tar loss: 0.719408\n",
      "=====================\n",
      "Epoch 74/200, Batch 88/938\n",
      "D loss: 1.088856, acc: 98% // tar acc: 96% // adv loss: 0.587474, aux loss: 1.481487, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 74/200, Batch 588/938\n",
      "D loss: 1.019980, acc: 96% // tar acc: 95% // adv loss: 0.541361, aux loss: 1.461862, tar loss: 0.740979\n",
      "=====================\n",
      "Epoch 75/200, Batch 150/938\n",
      "D loss: 0.999880, acc: 99% // tar acc: 96% // adv loss: 0.351773, aux loss: 1.479156, tar loss: 0.698121\n",
      "=====================\n",
      "Epoch 75/200, Batch 650/938\n",
      "D loss: 1.085378, acc: 97% // tar acc: 96% // adv loss: 0.637084, aux loss: 1.463412, tar loss: 0.705019\n",
      "=====================\n",
      "Epoch 76/200, Batch 212/938\n",
      "D loss: 1.027254, acc: 96% // tar acc: 96% // adv loss: 0.673973, aux loss: 1.492581, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 76/200, Batch 712/938\n",
      "D loss: 1.044570, acc: 97% // tar acc: 100% // adv loss: 0.610272, aux loss: 1.465786, tar loss: 0.724014\n",
      "=====================\n",
      "Epoch 77/200, Batch 274/938\n",
      "D loss: 1.114075, acc: 99% // tar acc: 96% // adv loss: 0.494791, aux loss: 1.461673, tar loss: 0.725321\n",
      "=====================\n",
      "Epoch 77/200, Batch 774/938\n",
      "D loss: 1.072156, acc: 97% // tar acc: 96% // adv loss: 0.720262, aux loss: 1.477839, tar loss: 0.740046\n",
      "=====================\n",
      "Epoch 78/200, Batch 336/938\n",
      "D loss: 1.092465, acc: 97% // tar acc: 100% // adv loss: 0.678137, aux loss: 1.464505, tar loss: 0.713282\n",
      "=====================\n",
      "Epoch 78/200, Batch 836/938\n",
      "D loss: 1.021209, acc: 97% // tar acc: 100% // adv loss: 0.545433, aux loss: 1.496902, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 79/200, Batch 398/938\n",
      "D loss: 1.076433, acc: 98% // tar acc: 98% // adv loss: 0.678719, aux loss: 1.469288, tar loss: 0.717937\n",
      "=====================\n",
      "Epoch 79/200, Batch 898/938\n",
      "D loss: 1.056882, acc: 96% // tar acc: 90% // adv loss: 0.616233, aux loss: 1.466034, tar loss: 0.666907\n",
      "=====================\n",
      "Epoch 80/200, Batch 460/938\n",
      "D loss: 1.035565, acc: 99% // tar acc: 98% // adv loss: 0.442492, aux loss: 1.461510, tar loss: 0.745144\n",
      "=====================\n",
      "Epoch 81/200, Batch 22/938\n",
      "D loss: 1.114095, acc: 98% // tar acc: 100% // adv loss: 0.425663, aux loss: 1.476450, tar loss: 0.725714\n",
      "=====================\n",
      "Epoch 81/200, Batch 522/938\n",
      "D loss: 1.088995, acc: 98% // tar acc: 96% // adv loss: 0.559348, aux loss: 1.461228, tar loss: 0.732273\n",
      "=====================\n",
      "Epoch 82/200, Batch 84/938\n",
      "D loss: 1.056357, acc: 97% // tar acc: 93% // adv loss: 0.551734, aux loss: 1.467690, tar loss: 0.719936\n",
      "=====================\n",
      "Epoch 82/200, Batch 584/938\n",
      "D loss: 1.140354, acc: 99% // tar acc: 98% // adv loss: 0.630986, aux loss: 1.471804, tar loss: 0.748068\n",
      "=====================\n",
      "Epoch 83/200, Batch 146/938\n",
      "D loss: 1.057671, acc: 98% // tar acc: 98% // adv loss: 0.592283, aux loss: 1.462303, tar loss: 0.718400\n",
      "=====================\n",
      "Epoch 83/200, Batch 646/938\n",
      "D loss: 1.121902, acc: 97% // tar acc: 98% // adv loss: 0.741289, aux loss: 1.461461, tar loss: 0.691486\n",
      "=====================\n",
      "Epoch 84/200, Batch 208/938\n",
      "D loss: 1.074562, acc: 97% // tar acc: 96% // adv loss: 0.467029, aux loss: 1.465186, tar loss: 0.733303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 84/200, Batch 708/938\n",
      "D loss: 1.036918, acc: 99% // tar acc: 100% // adv loss: 0.490782, aux loss: 1.461796, tar loss: 0.725036\n",
      "=====================\n",
      "Epoch 85/200, Batch 270/938\n",
      "D loss: 1.087543, acc: 100% // tar acc: 98% // adv loss: 0.574082, aux loss: 1.465254, tar loss: 0.750062\n",
      "=====================\n",
      "Epoch 85/200, Batch 770/938\n",
      "D loss: 1.069425, acc: 98% // tar acc: 96% // adv loss: 0.541370, aux loss: 1.472052, tar loss: 0.733162\n",
      "=====================\n",
      "Epoch 86/200, Batch 332/938\n",
      "D loss: 1.028328, acc: 94% // tar acc: 100% // adv loss: 0.552051, aux loss: 1.461738, tar loss: 0.767940\n",
      "=====================\n",
      "Epoch 86/200, Batch 832/938\n",
      "D loss: 1.102319, acc: 98% // tar acc: 98% // adv loss: 0.574909, aux loss: 1.467092, tar loss: 0.748562\n",
      "=====================\n",
      "Epoch 87/200, Batch 394/938\n",
      "D loss: 1.122140, acc: 97% // tar acc: 95% // adv loss: 0.492706, aux loss: 1.489296, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 87/200, Batch 894/938\n",
      "D loss: 1.027699, acc: 99% // tar acc: 100% // adv loss: 0.380148, aux loss: 1.462241, tar loss: 0.716818\n",
      "=====================\n",
      "Epoch 88/200, Batch 456/938\n",
      "D loss: 1.075253, acc: 98% // tar acc: 98% // adv loss: 0.621135, aux loss: 1.472823, tar loss: 0.747359\n",
      "=====================\n",
      "Epoch 89/200, Batch 18/938\n",
      "D loss: 1.012205, acc: 96% // tar acc: 95% // adv loss: 0.609847, aux loss: 1.473853, tar loss: 0.687293\n",
      "=====================\n",
      "Epoch 89/200, Batch 518/938\n",
      "D loss: 1.139053, acc: 96% // tar acc: 100% // adv loss: 0.521059, aux loss: 1.461223, tar loss: 0.738969\n",
      "=====================\n",
      "Epoch 90/200, Batch 80/938\n",
      "D loss: 1.021828, acc: 96% // tar acc: 98% // adv loss: 0.453386, aux loss: 1.474669, tar loss: 0.701506\n",
      "=====================\n",
      "Epoch 90/200, Batch 580/938\n",
      "D loss: 1.095450, acc: 98% // tar acc: 100% // adv loss: 0.565578, aux loss: 1.476751, tar loss: 0.730589\n",
      "=====================\n",
      "Epoch 91/200, Batch 142/938\n",
      "D loss: 1.092420, acc: 99% // tar acc: 100% // adv loss: 0.605352, aux loss: 1.461292, tar loss: 0.734761\n",
      "=====================\n",
      "Epoch 91/200, Batch 642/938\n",
      "D loss: 1.136419, acc: 94% // tar acc: 95% // adv loss: 0.324548, aux loss: 1.461987, tar loss: 0.726298\n",
      "=====================\n",
      "Epoch 92/200, Batch 204/938\n",
      "D loss: 1.066942, acc: 97% // tar acc: 98% // adv loss: 0.670146, aux loss: 1.477991, tar loss: 0.712918\n",
      "=====================\n",
      "Epoch 92/200, Batch 704/938\n",
      "D loss: 1.041158, acc: 98% // tar acc: 100% // adv loss: 0.561980, aux loss: 1.462373, tar loss: 0.737471\n",
      "=====================\n",
      "Epoch 93/200, Batch 266/938\n",
      "D loss: 1.139011, acc: 97% // tar acc: 98% // adv loss: 0.737673, aux loss: 1.462618, tar loss: 0.706867\n",
      "=====================\n",
      "Epoch 93/200, Batch 766/938\n",
      "D loss: 1.081338, acc: 100% // tar acc: 96% // adv loss: 0.539619, aux loss: 1.476550, tar loss: 0.705722\n",
      "=====================\n",
      "Epoch 94/200, Batch 328/938\n",
      "D loss: 1.138518, acc: 99% // tar acc: 96% // adv loss: 0.490497, aux loss: 1.477186, tar loss: 0.700895\n",
      "=====================\n",
      "Epoch 94/200, Batch 828/938\n",
      "D loss: 1.053082, acc: 99% // tar acc: 98% // adv loss: 0.427521, aux loss: 1.472204, tar loss: 0.714848\n",
      "=====================\n",
      "Epoch 95/200, Batch 390/938\n",
      "D loss: 1.131792, acc: 99% // tar acc: 100% // adv loss: 0.512221, aux loss: 1.461781, tar loss: 0.765439\n",
      "=====================\n",
      "Epoch 95/200, Batch 890/938\n",
      "D loss: 1.107575, acc: 99% // tar acc: 96% // adv loss: 0.515271, aux loss: 1.465984, tar loss: 0.681578\n",
      "=====================\n",
      "Epoch 96/200, Batch 452/938\n",
      "D loss: 1.070654, acc: 100% // tar acc: 93% // adv loss: 0.501236, aux loss: 1.461371, tar loss: 0.698340\n",
      "=====================\n",
      "Epoch 97/200, Batch 14/938\n",
      "D loss: 1.051911, acc: 96% // tar acc: 98% // adv loss: 0.844305, aux loss: 1.466570, tar loss: 0.698339\n",
      "=====================\n",
      "Epoch 97/200, Batch 514/938\n",
      "D loss: 1.088166, acc: 97% // tar acc: 96% // adv loss: 0.579633, aux loss: 1.488453, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 98/200, Batch 76/938\n",
      "D loss: 1.046945, acc: 99% // tar acc: 90% // adv loss: 0.712741, aux loss: 1.462407, tar loss: 0.678643\n",
      "=====================\n",
      "Epoch 98/200, Batch 576/938\n",
      "D loss: 1.053534, acc: 100% // tar acc: 98% // adv loss: 0.629779, aux loss: 1.474181, tar loss: 0.782488\n",
      "=====================\n",
      "Epoch 99/200, Batch 138/938\n",
      "D loss: 1.084488, acc: 96% // tar acc: 98% // adv loss: 0.491002, aux loss: 1.461743, tar loss: 0.732731\n",
      "=====================\n",
      "Epoch 99/200, Batch 638/938\n",
      "D loss: 1.053114, acc: 97% // tar acc: 98% // adv loss: 0.700217, aux loss: 1.461177, tar loss: 0.721820\n",
      "=====================\n",
      "Epoch 100/200, Batch 200/938\n",
      "D loss: 1.112145, acc: 97% // tar acc: 96% // adv loss: 0.539285, aux loss: 1.473662, tar loss: 0.694920\n",
      "=====================\n",
      "Epoch 100/200, Batch 700/938\n",
      "D loss: 1.012594, acc: 96% // tar acc: 96% // adv loss: 0.446854, aux loss: 1.463726, tar loss: 0.719080\n",
      "=====================\n",
      "Epoch 101/200, Batch 262/938\n",
      "D loss: 1.117314, acc: 97% // tar acc: 98% // adv loss: 0.388728, aux loss: 1.461152, tar loss: 0.742605\n",
      "=====================\n",
      "Epoch 101/200, Batch 762/938\n",
      "D loss: 1.070942, acc: 99% // tar acc: 95% // adv loss: 0.492229, aux loss: 1.463399, tar loss: 0.742699\n",
      "=====================\n",
      "Epoch 102/200, Batch 324/938\n",
      "D loss: 1.027265, acc: 99% // tar acc: 93% // adv loss: 0.633024, aux loss: 1.478052, tar loss: 0.718525\n",
      "=====================\n",
      "Epoch 102/200, Batch 824/938\n",
      "D loss: 1.093740, acc: 100% // tar acc: 98% // adv loss: 0.653080, aux loss: 1.462489, tar loss: 0.737409\n",
      "=====================\n",
      "Epoch 103/200, Batch 386/938\n",
      "D loss: 1.110854, acc: 99% // tar acc: 98% // adv loss: 0.532715, aux loss: 1.461617, tar loss: 0.749705\n",
      "=====================\n",
      "Epoch 103/200, Batch 886/938\n",
      "D loss: 0.985728, acc: 100% // tar acc: 96% // adv loss: 0.509944, aux loss: 1.462638, tar loss: 0.690342\n",
      "=====================\n",
      "Epoch 104/200, Batch 448/938\n",
      "D loss: 1.071023, acc: 97% // tar acc: 98% // adv loss: 0.666550, aux loss: 1.462831, tar loss: 0.730144\n",
      "=====================\n",
      "Epoch 105/200, Batch 10/938\n",
      "D loss: 1.101282, acc: 97% // tar acc: 96% // adv loss: 0.497668, aux loss: 1.461189, tar loss: 0.704852\n",
      "=====================\n",
      "Epoch 105/200, Batch 510/938\n",
      "D loss: 1.056466, acc: 98% // tar acc: 100% // adv loss: 0.653982, aux loss: 1.472865, tar loss: 0.718022\n",
      "=====================\n",
      "Epoch 106/200, Batch 72/938\n",
      "D loss: 1.065399, acc: 97% // tar acc: 100% // adv loss: 0.549999, aux loss: 1.475179, tar loss: 0.752698\n",
      "=====================\n",
      "Epoch 106/200, Batch 572/938\n",
      "D loss: 1.039524, acc: 99% // tar acc: 100% // adv loss: 0.555636, aux loss: 1.473291, tar loss: 0.737333\n",
      "=====================\n",
      "Epoch 107/200, Batch 134/938\n",
      "D loss: 1.107394, acc: 97% // tar acc: 100% // adv loss: 0.467574, aux loss: 1.462440, tar loss: 0.769001\n",
      "=====================\n",
      "Epoch 107/200, Batch 634/938\n",
      "D loss: 1.093647, acc: 98% // tar acc: 93% // adv loss: 0.710331, aux loss: 1.470112, tar loss: 0.720018\n",
      "=====================\n",
      "Epoch 108/200, Batch 196/938\n",
      "D loss: 1.009730, acc: 99% // tar acc: 96% // adv loss: 0.615587, aux loss: 1.462232, tar loss: 0.706877\n",
      "=====================\n",
      "Epoch 108/200, Batch 696/938\n",
      "D loss: 1.047771, acc: 100% // tar acc: 98% // adv loss: 0.537286, aux loss: 1.461173, tar loss: 0.710621\n",
      "=====================\n",
      "Epoch 109/200, Batch 258/938\n",
      "D loss: 1.107980, acc: 97% // tar acc: 98% // adv loss: 0.575231, aux loss: 1.462064, tar loss: 0.716525\n",
      "=====================\n",
      "Epoch 109/200, Batch 758/938\n",
      "D loss: 1.002998, acc: 99% // tar acc: 100% // adv loss: 0.821627, aux loss: 1.472787, tar loss: 0.732944\n",
      "=====================\n",
      "Epoch 110/200, Batch 320/938\n",
      "D loss: 1.015500, acc: 99% // tar acc: 96% // adv loss: 0.633123, aux loss: 1.461400, tar loss: 0.717762\n",
      "=====================\n",
      "Epoch 110/200, Batch 820/938\n",
      "D loss: 1.060492, acc: 95% // tar acc: 95% // adv loss: 0.664207, aux loss: 1.461283, tar loss: 0.695845\n",
      "=====================\n",
      "Epoch 111/200, Batch 382/938\n",
      "D loss: 1.115873, acc: 99% // tar acc: 96% // adv loss: 0.578683, aux loss: 1.484938, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 111/200, Batch 882/938\n",
      "D loss: 1.085198, acc: 97% // tar acc: 98% // adv loss: 0.762107, aux loss: 1.472704, tar loss: 0.745585\n",
      "=====================\n",
      "Epoch 112/200, Batch 444/938\n",
      "D loss: 1.080525, acc: 97% // tar acc: 98% // adv loss: 0.637169, aux loss: 1.473818, tar loss: 0.731400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 113/200, Batch 6/938\n",
      "D loss: 1.103758, acc: 96% // tar acc: 98% // adv loss: 0.652833, aux loss: 1.479757, tar loss: 0.724463\n",
      "=====================\n",
      "Epoch 113/200, Batch 506/938\n",
      "D loss: 1.054078, acc: 99% // tar acc: 93% // adv loss: 0.568651, aux loss: 1.475664, tar loss: 0.701824\n",
      "=====================\n",
      "Epoch 114/200, Batch 68/938\n",
      "D loss: 1.028264, acc: 100% // tar acc: 95% // adv loss: 0.541973, aux loss: 1.468399, tar loss: 0.735935\n",
      "=====================\n",
      "Epoch 114/200, Batch 568/938\n",
      "D loss: 1.077362, acc: 98% // tar acc: 92% // adv loss: 0.530133, aux loss: 1.461492, tar loss: 0.731546\n",
      "=====================\n",
      "Epoch 115/200, Batch 130/938\n",
      "D loss: 1.098559, acc: 98% // tar acc: 93% // adv loss: 0.604282, aux loss: 1.461169, tar loss: 0.768441\n",
      "=====================\n",
      "Epoch 115/200, Batch 630/938\n",
      "D loss: 1.020148, acc: 97% // tar acc: 93% // adv loss: 0.599339, aux loss: 1.478329, tar loss: 0.706356\n",
      "=====================\n",
      "Epoch 116/200, Batch 192/938\n",
      "D loss: 1.064544, acc: 99% // tar acc: 98% // adv loss: 0.403904, aux loss: 1.463603, tar loss: 0.710789\n",
      "=====================\n",
      "Epoch 116/200, Batch 692/938\n",
      "D loss: 1.068958, acc: 96% // tar acc: 92% // adv loss: 0.579947, aux loss: 1.462708, tar loss: 0.729539\n",
      "=====================\n",
      "Epoch 117/200, Batch 254/938\n",
      "D loss: 1.088189, acc: 99% // tar acc: 98% // adv loss: 0.503668, aux loss: 1.477412, tar loss: 0.732384\n",
      "=====================\n",
      "Epoch 117/200, Batch 754/938\n",
      "D loss: 1.071795, acc: 99% // tar acc: 98% // adv loss: 0.437814, aux loss: 1.466118, tar loss: 0.727534\n",
      "=====================\n",
      "Epoch 118/200, Batch 316/938\n",
      "D loss: 1.081076, acc: 99% // tar acc: 95% // adv loss: 0.737020, aux loss: 1.477133, tar loss: 0.693722\n",
      "=====================\n",
      "Epoch 118/200, Batch 816/938\n",
      "D loss: 1.057035, acc: 97% // tar acc: 100% // adv loss: 0.503208, aux loss: 1.467749, tar loss: 0.749461\n",
      "=====================\n",
      "Epoch 119/200, Batch 378/938\n",
      "D loss: 1.033956, acc: 100% // tar acc: 95% // adv loss: 0.643175, aux loss: 1.470715, tar loss: 0.701062\n",
      "=====================\n",
      "Epoch 119/200, Batch 878/938\n",
      "D loss: 1.077797, acc: 98% // tar acc: 100% // adv loss: 0.503999, aux loss: 1.464334, tar loss: 0.682404\n",
      "=====================\n",
      "Epoch 120/200, Batch 440/938\n",
      "D loss: 1.012164, acc: 96% // tar acc: 92% // adv loss: 0.734163, aux loss: 1.462625, tar loss: 0.680307\n",
      "=====================\n",
      "Epoch 121/200, Batch 2/938\n",
      "D loss: 1.054077, acc: 99% // tar acc: 98% // adv loss: 0.577767, aux loss: 1.462739, tar loss: 0.670477\n",
      "=====================\n",
      "Epoch 121/200, Batch 502/938\n",
      "D loss: 1.027604, acc: 100% // tar acc: 96% // adv loss: 0.568214, aux loss: 1.492426, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 122/200, Batch 64/938\n",
      "D loss: 1.056075, acc: 100% // tar acc: 98% // adv loss: 0.473295, aux loss: 1.475751, tar loss: 0.730613\n",
      "=====================\n",
      "Epoch 122/200, Batch 564/938\n",
      "D loss: 1.070533, acc: 99% // tar acc: 98% // adv loss: 0.583201, aux loss: 1.461278, tar loss: 0.756128\n",
      "=====================\n",
      "Epoch 123/200, Batch 126/938\n",
      "D loss: 1.062234, acc: 98% // tar acc: 98% // adv loss: 0.547594, aux loss: 1.462862, tar loss: 0.750093\n",
      "=====================\n",
      "Epoch 123/200, Batch 626/938\n",
      "D loss: 1.047092, acc: 98% // tar acc: 96% // adv loss: 0.558465, aux loss: 1.463919, tar loss: 0.730402\n",
      "=====================\n",
      "Epoch 124/200, Batch 188/938\n",
      "D loss: 1.074601, acc: 98% // tar acc: 96% // adv loss: 0.550557, aux loss: 1.461152, tar loss: 0.779090\n",
      "=====================\n",
      "Epoch 124/200, Batch 688/938\n",
      "D loss: 1.084082, acc: 97% // tar acc: 92% // adv loss: 0.709246, aux loss: 1.481696, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 125/200, Batch 250/938\n",
      "D loss: 1.038622, acc: 98% // tar acc: 100% // adv loss: 0.530526, aux loss: 1.463227, tar loss: 0.765809\n",
      "=====================\n",
      "Epoch 125/200, Batch 750/938\n",
      "D loss: 1.058397, acc: 98% // tar acc: 96% // adv loss: 0.424617, aux loss: 1.476018, tar loss: 0.704457\n",
      "=====================\n",
      "Epoch 126/200, Batch 312/938\n",
      "D loss: 1.010612, acc: 97% // tar acc: 98% // adv loss: 0.529584, aux loss: 1.464753, tar loss: 0.707783\n",
      "=====================\n",
      "Epoch 126/200, Batch 812/938\n",
      "D loss: 1.068326, acc: 99% // tar acc: 100% // adv loss: 0.520882, aux loss: 1.461376, tar loss: 0.746096\n",
      "=====================\n",
      "Epoch 127/200, Batch 374/938\n",
      "D loss: 1.109249, acc: 98% // tar acc: 98% // adv loss: 0.620223, aux loss: 1.461154, tar loss: 0.735329\n",
      "=====================\n",
      "Epoch 127/200, Batch 874/938\n",
      "D loss: 1.067629, acc: 96% // tar acc: 95% // adv loss: 0.495103, aux loss: 1.461823, tar loss: 0.738162\n",
      "=====================\n",
      "Epoch 128/200, Batch 436/938\n",
      "D loss: 1.055443, acc: 98% // tar acc: 95% // adv loss: 0.467079, aux loss: 1.461233, tar loss: 0.738238\n",
      "=====================\n",
      "Epoch 128/200, Batch 936/938\n",
      "D loss: 1.048229, acc: 96% // tar acc: 96% // adv loss: 0.427097, aux loss: 1.461797, tar loss: 0.720652\n",
      "=====================\n",
      "Epoch 129/200, Batch 498/938\n",
      "D loss: 1.068603, acc: 100% // tar acc: 96% // adv loss: 0.513986, aux loss: 1.462993, tar loss: 0.758603\n",
      "=====================\n",
      "Epoch 130/200, Batch 60/938\n",
      "D loss: 1.086369, acc: 99% // tar acc: 100% // adv loss: 0.568468, aux loss: 1.462901, tar loss: 0.713957\n",
      "=====================\n",
      "Epoch 130/200, Batch 560/938\n",
      "D loss: 1.119179, acc: 97% // tar acc: 96% // adv loss: 0.484446, aux loss: 1.486238, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 131/200, Batch 122/938\n",
      "D loss: 1.071908, acc: 97% // tar acc: 98% // adv loss: 0.599099, aux loss: 1.461347, tar loss: 0.699963\n",
      "=====================\n",
      "Epoch 131/200, Batch 622/938\n",
      "D loss: 1.065014, acc: 99% // tar acc: 98% // adv loss: 0.712201, aux loss: 1.461567, tar loss: 0.696537\n",
      "=====================\n",
      "Epoch 132/200, Batch 184/938\n",
      "D loss: 1.041276, acc: 97% // tar acc: 100% // adv loss: 0.528575, aux loss: 1.461910, tar loss: 0.725178\n",
      "=====================\n",
      "Epoch 132/200, Batch 684/938\n",
      "D loss: 1.083020, acc: 98% // tar acc: 98% // adv loss: 0.508321, aux loss: 1.482621, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 133/200, Batch 246/938\n",
      "D loss: 1.126070, acc: 99% // tar acc: 98% // adv loss: 0.612910, aux loss: 1.479502, tar loss: 0.727118\n",
      "=====================\n",
      "Epoch 133/200, Batch 746/938\n",
      "D loss: 1.077715, acc: 96% // tar acc: 98% // adv loss: 0.591637, aux loss: 1.476777, tar loss: 0.753856\n",
      "=====================\n",
      "Epoch 134/200, Batch 308/938\n",
      "D loss: 1.089678, acc: 98% // tar acc: 100% // adv loss: 0.437324, aux loss: 1.477452, tar loss: 0.721617\n",
      "=====================\n",
      "Epoch 134/200, Batch 808/938\n",
      "D loss: 1.071873, acc: 99% // tar acc: 98% // adv loss: 0.501563, aux loss: 1.461322, tar loss: 0.709076\n",
      "=====================\n",
      "Epoch 135/200, Batch 370/938\n",
      "D loss: 1.087519, acc: 96% // tar acc: 96% // adv loss: 0.606152, aux loss: 1.477745, tar loss: 0.702374\n",
      "=====================\n",
      "Epoch 135/200, Batch 870/938\n",
      "D loss: 1.078004, acc: 96% // tar acc: 95% // adv loss: 0.472786, aux loss: 1.461213, tar loss: 0.730058\n",
      "=====================\n",
      "Epoch 136/200, Batch 432/938\n",
      "D loss: 1.093017, acc: 99% // tar acc: 100% // adv loss: 0.562183, aux loss: 1.461173, tar loss: 0.723781\n",
      "=====================\n",
      "Epoch 136/200, Batch 932/938\n",
      "D loss: 1.048115, acc: 97% // tar acc: 95% // adv loss: 0.728738, aux loss: 1.461815, tar loss: 0.732935\n",
      "=====================\n",
      "Epoch 137/200, Batch 494/938\n",
      "D loss: 1.079461, acc: 100% // tar acc: 95% // adv loss: 0.461230, aux loss: 1.461156, tar loss: 0.730507\n",
      "=====================\n",
      "Epoch 138/200, Batch 56/938\n",
      "D loss: 1.043906, acc: 99% // tar acc: 100% // adv loss: 0.644712, aux loss: 1.461211, tar loss: 0.726777\n",
      "=====================\n",
      "Epoch 138/200, Batch 556/938\n",
      "D loss: 1.104459, acc: 98% // tar acc: 98% // adv loss: 0.480885, aux loss: 1.461522, tar loss: 0.748450\n",
      "=====================\n",
      "Epoch 139/200, Batch 118/938\n",
      "D loss: 1.084892, acc: 97% // tar acc: 98% // adv loss: 0.581921, aux loss: 1.490280, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 139/200, Batch 618/938\n",
      "D loss: 1.017149, acc: 97% // tar acc: 100% // adv loss: 0.514794, aux loss: 1.472464, tar loss: 0.728100\n",
      "=====================\n",
      "Epoch 140/200, Batch 180/938\n",
      "D loss: 1.223346, acc: 96% // tar acc: 100% // adv loss: 0.608131, aux loss: 1.461180, tar loss: 0.750654\n",
      "=====================\n",
      "Epoch 140/200, Batch 680/938\n",
      "D loss: 1.090308, acc: 98% // tar acc: 96% // adv loss: 0.540526, aux loss: 1.461745, tar loss: 0.732094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 141/200, Batch 242/938\n",
      "D loss: 1.041348, acc: 100% // tar acc: 96% // adv loss: 0.601877, aux loss: 1.461156, tar loss: 0.728066\n",
      "=====================\n",
      "Epoch 141/200, Batch 742/938\n",
      "D loss: 0.981212, acc: 98% // tar acc: 98% // adv loss: 0.453854, aux loss: 1.476413, tar loss: 0.740815\n",
      "=====================\n",
      "Epoch 142/200, Batch 304/938\n",
      "D loss: 1.031346, acc: 99% // tar acc: 96% // adv loss: 0.597584, aux loss: 1.461367, tar loss: 0.737435\n",
      "=====================\n",
      "Epoch 142/200, Batch 804/938\n",
      "D loss: 1.046610, acc: 96% // tar acc: 96% // adv loss: 0.464298, aux loss: 1.466286, tar loss: 0.727960\n",
      "=====================\n",
      "Epoch 143/200, Batch 366/938\n",
      "D loss: 1.082028, acc: 99% // tar acc: 96% // adv loss: 0.458199, aux loss: 1.461507, tar loss: 0.752208\n",
      "=====================\n",
      "Epoch 143/200, Batch 866/938\n",
      "D loss: 1.009441, acc: 100% // tar acc: 98% // adv loss: 0.515554, aux loss: 1.462531, tar loss: 0.728422\n",
      "=====================\n",
      "Epoch 144/200, Batch 428/938\n",
      "D loss: 1.071695, acc: 100% // tar acc: 93% // adv loss: 0.352840, aux loss: 1.462008, tar loss: 0.745982\n",
      "=====================\n",
      "Epoch 144/200, Batch 928/938\n",
      "D loss: 1.089451, acc: 98% // tar acc: 100% // adv loss: 0.574898, aux loss: 1.463191, tar loss: 0.706876\n",
      "=====================\n",
      "Epoch 145/200, Batch 490/938\n",
      "D loss: 1.075166, acc: 98% // tar acc: 98% // adv loss: 0.569247, aux loss: 1.461713, tar loss: 0.741454\n",
      "=====================\n",
      "Epoch 146/200, Batch 52/938\n",
      "D loss: 1.123827, acc: 99% // tar acc: 96% // adv loss: 0.347674, aux loss: 1.476444, tar loss: 0.734738\n",
      "=====================\n",
      "Epoch 146/200, Batch 552/938\n",
      "D loss: 1.060051, acc: 97% // tar acc: 96% // adv loss: 0.582986, aux loss: 1.461159, tar loss: 0.708578\n",
      "=====================\n",
      "Epoch 147/200, Batch 114/938\n",
      "D loss: 1.033841, acc: 99% // tar acc: 95% // adv loss: 0.506320, aux loss: 1.462499, tar loss: 0.694930\n",
      "=====================\n",
      "Epoch 147/200, Batch 614/938\n",
      "D loss: 1.088505, acc: 98% // tar acc: 95% // adv loss: 0.531524, aux loss: 1.461199, tar loss: 0.676738\n",
      "=====================\n",
      "Epoch 148/200, Batch 176/938\n",
      "D loss: 1.071406, acc: 98% // tar acc: 98% // adv loss: 0.556715, aux loss: 1.461609, tar loss: 0.752558\n",
      "=====================\n",
      "Epoch 148/200, Batch 676/938\n",
      "D loss: 1.112900, acc: 97% // tar acc: 92% // adv loss: 0.491888, aux loss: 1.462648, tar loss: 0.705688\n",
      "=====================\n",
      "Epoch 149/200, Batch 238/938\n",
      "D loss: 1.081188, acc: 97% // tar acc: 98% // adv loss: 0.410044, aux loss: 1.478405, tar loss: 0.726248\n",
      "=====================\n",
      "Epoch 149/200, Batch 738/938\n",
      "D loss: 1.064528, acc: 97% // tar acc: 95% // adv loss: 0.669234, aux loss: 1.461732, tar loss: 0.736040\n",
      "=====================\n",
      "Epoch 150/200, Batch 300/938\n",
      "D loss: 1.153935, acc: 97% // tar acc: 93% // adv loss: 0.373661, aux loss: 1.474512, tar loss: 0.692771\n",
      "=====================\n",
      "Epoch 150/200, Batch 800/938\n",
      "D loss: 1.109648, acc: 98% // tar acc: 95% // adv loss: 0.584404, aux loss: 1.482259, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 151/200, Batch 362/938\n",
      "D loss: 1.021407, acc: 99% // tar acc: 95% // adv loss: 0.626005, aux loss: 1.461267, tar loss: 0.714831\n",
      "=====================\n",
      "Epoch 151/200, Batch 862/938\n",
      "D loss: 1.061623, acc: 96% // tar acc: 95% // adv loss: 0.626294, aux loss: 1.477489, tar loss: 0.710966\n",
      "=====================\n",
      "Epoch 152/200, Batch 424/938\n",
      "D loss: 1.117101, acc: 97% // tar acc: 100% // adv loss: 0.424369, aux loss: 1.480133, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 152/200, Batch 924/938\n",
      "D loss: 1.056849, acc: 99% // tar acc: 98% // adv loss: 0.762476, aux loss: 1.461173, tar loss: 0.755183\n",
      "=====================\n",
      "Epoch 153/200, Batch 486/938\n",
      "D loss: 1.093654, acc: 98% // tar acc: 98% // adv loss: 0.575381, aux loss: 1.464041, tar loss: 0.730675\n",
      "=====================\n",
      "Epoch 154/200, Batch 48/938\n",
      "D loss: 1.070709, acc: 100% // tar acc: 98% // adv loss: 0.465203, aux loss: 1.476742, tar loss: 0.721666\n",
      "=====================\n",
      "Epoch 154/200, Batch 548/938\n",
      "D loss: 1.112900, acc: 99% // tar acc: 100% // adv loss: 0.366914, aux loss: 1.461181, tar loss: 0.740002\n",
      "=====================\n",
      "Epoch 155/200, Batch 110/938\n",
      "D loss: 1.096444, acc: 97% // tar acc: 95% // adv loss: 0.503019, aux loss: 1.475415, tar loss: 0.705870\n",
      "=====================\n",
      "Epoch 155/200, Batch 610/938\n",
      "D loss: 1.013457, acc: 96% // tar acc: 93% // adv loss: 0.516516, aux loss: 1.475469, tar loss: 0.713988\n",
      "=====================\n",
      "Epoch 156/200, Batch 172/938\n",
      "D loss: 1.063692, acc: 98% // tar acc: 100% // adv loss: 0.538238, aux loss: 1.467735, tar loss: 0.713816\n",
      "=====================\n",
      "Epoch 156/200, Batch 672/938\n",
      "D loss: 1.104073, acc: 98% // tar acc: 93% // adv loss: 0.475762, aux loss: 1.461571, tar loss: 0.707071\n",
      "=====================\n",
      "Epoch 157/200, Batch 234/938\n",
      "D loss: 1.047342, acc: 98% // tar acc: 98% // adv loss: 0.661972, aux loss: 1.461417, tar loss: 0.733254\n",
      "=====================\n",
      "Epoch 157/200, Batch 734/938\n",
      "D loss: 1.036624, acc: 100% // tar acc: 98% // adv loss: 0.718632, aux loss: 1.461214, tar loss: 0.723441\n",
      "=====================\n",
      "Epoch 158/200, Batch 296/938\n",
      "D loss: 1.008487, acc: 97% // tar acc: 96% // adv loss: 0.600635, aux loss: 1.462492, tar loss: 0.717126\n",
      "=====================\n",
      "Epoch 158/200, Batch 796/938\n",
      "D loss: 1.024726, acc: 97% // tar acc: 98% // adv loss: 0.594725, aux loss: 1.476733, tar loss: 0.700450\n",
      "=====================\n",
      "Epoch 159/200, Batch 358/938\n",
      "D loss: 1.062797, acc: 97% // tar acc: 95% // adv loss: 0.617474, aux loss: 1.462350, tar loss: 0.731882\n",
      "=====================\n",
      "Epoch 159/200, Batch 858/938\n",
      "D loss: 1.043717, acc: 98% // tar acc: 100% // adv loss: 0.479636, aux loss: 1.461889, tar loss: 0.713667\n",
      "=====================\n",
      "Epoch 160/200, Batch 420/938\n",
      "D loss: 1.140245, acc: 99% // tar acc: 100% // adv loss: 0.369711, aux loss: 1.461541, tar loss: 0.754538\n",
      "=====================\n",
      "Epoch 160/200, Batch 920/938\n",
      "D loss: 1.061592, acc: 96% // tar acc: 95% // adv loss: 0.495587, aux loss: 1.480073, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 161/200, Batch 482/938\n",
      "D loss: 1.037772, acc: 99% // tar acc: 93% // adv loss: 0.509046, aux loss: 1.461192, tar loss: 0.717405\n",
      "=====================\n",
      "Epoch 162/200, Batch 44/938\n",
      "D loss: 1.114591, acc: 100% // tar acc: 98% // adv loss: 0.497018, aux loss: 1.461156, tar loss: 0.737286\n",
      "=====================\n",
      "Epoch 162/200, Batch 544/938\n",
      "D loss: 1.042597, acc: 98% // tar acc: 98% // adv loss: 0.391196, aux loss: 1.462122, tar loss: 0.735324\n",
      "=====================\n",
      "Epoch 163/200, Batch 106/938\n",
      "D loss: 1.056287, acc: 96% // tar acc: 93% // adv loss: 0.634153, aux loss: 1.462409, tar loss: 0.730884\n",
      "=====================\n",
      "Epoch 163/200, Batch 606/938\n",
      "D loss: 1.096351, acc: 98% // tar acc: 96% // adv loss: 0.681243, aux loss: 1.461549, tar loss: 0.716051\n",
      "=====================\n",
      "Epoch 164/200, Batch 168/938\n",
      "D loss: 1.103051, acc: 97% // tar acc: 100% // adv loss: 0.685803, aux loss: 1.463467, tar loss: 0.729987\n",
      "=====================\n",
      "Epoch 164/200, Batch 668/938\n",
      "D loss: 1.111960, acc: 96% // tar acc: 98% // adv loss: 0.389385, aux loss: 1.461160, tar loss: 0.720100\n",
      "=====================\n",
      "Epoch 165/200, Batch 230/938\n",
      "D loss: 1.067764, acc: 96% // tar acc: 93% // adv loss: 0.541101, aux loss: 1.461291, tar loss: 0.730160\n",
      "=====================\n",
      "Epoch 165/200, Batch 730/938\n",
      "D loss: 1.093924, acc: 98% // tar acc: 98% // adv loss: 0.510439, aux loss: 1.461228, tar loss: 0.720491\n",
      "=====================\n",
      "Epoch 166/200, Batch 292/938\n",
      "D loss: 1.043264, acc: 97% // tar acc: 98% // adv loss: 0.488525, aux loss: 1.461275, tar loss: 0.742485\n",
      "=====================\n",
      "Epoch 166/200, Batch 792/938\n",
      "D loss: 1.065591, acc: 97% // tar acc: 96% // adv loss: 0.628244, aux loss: 1.476585, tar loss: 0.709908\n",
      "=====================\n",
      "Epoch 167/200, Batch 354/938\n",
      "D loss: 1.103997, acc: 99% // tar acc: 95% // adv loss: 0.508816, aux loss: 1.461975, tar loss: 0.720080\n",
      "=====================\n",
      "Epoch 167/200, Batch 854/938\n",
      "D loss: 1.093888, acc: 96% // tar acc: 93% // adv loss: 0.429545, aux loss: 1.472597, tar loss: 0.734078\n",
      "=====================\n",
      "Epoch 168/200, Batch 416/938\n",
      "D loss: 1.059063, acc: 100% // tar acc: 100% // adv loss: 0.652955, aux loss: 1.461190, tar loss: 0.695198\n",
      "=====================\n",
      "Epoch 168/200, Batch 916/938\n",
      "D loss: 1.094236, acc: 98% // tar acc: 96% // adv loss: 0.593958, aux loss: 1.464178, tar loss: 0.715942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 169/200, Batch 478/938\n",
      "D loss: 1.097578, acc: 96% // tar acc: 95% // adv loss: 0.630628, aux loss: 1.490201, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 170/200, Batch 40/938\n",
      "D loss: 1.085714, acc: 100% // tar acc: 96% // adv loss: 0.467572, aux loss: 1.462662, tar loss: 0.741092\n",
      "=====================\n",
      "Epoch 170/200, Batch 540/938\n",
      "D loss: 1.081771, acc: 98% // tar acc: 96% // adv loss: 0.778774, aux loss: 1.476852, tar loss: 0.722795\n",
      "=====================\n",
      "Epoch 171/200, Batch 102/938\n",
      "D loss: 1.049416, acc: 98% // tar acc: 90% // adv loss: 0.572177, aux loss: 1.476108, tar loss: 0.721377\n",
      "=====================\n",
      "Epoch 171/200, Batch 602/938\n",
      "D loss: 1.066619, acc: 96% // tar acc: 98% // adv loss: 0.512080, aux loss: 1.471441, tar loss: 0.736313\n",
      "=====================\n",
      "Epoch 172/200, Batch 164/938\n",
      "D loss: 1.006714, acc: 98% // tar acc: 100% // adv loss: 0.579929, aux loss: 1.461933, tar loss: 0.765095\n",
      "=====================\n",
      "Epoch 172/200, Batch 664/938\n",
      "D loss: 1.055970, acc: 99% // tar acc: 95% // adv loss: 0.352413, aux loss: 1.461166, tar loss: 0.732516\n",
      "=====================\n",
      "Epoch 173/200, Batch 226/938\n",
      "D loss: 1.085546, acc: 97% // tar acc: 98% // adv loss: 0.455581, aux loss: 1.463374, tar loss: 0.709247\n",
      "=====================\n",
      "Epoch 173/200, Batch 726/938\n",
      "D loss: 1.042917, acc: 99% // tar acc: 95% // adv loss: 0.480332, aux loss: 1.499921, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 174/200, Batch 288/938\n",
      "D loss: 1.111910, acc: 99% // tar acc: 100% // adv loss: 0.680715, aux loss: 1.461223, tar loss: 0.773177\n",
      "=====================\n",
      "Epoch 174/200, Batch 788/938\n",
      "D loss: 1.066078, acc: 98% // tar acc: 95% // adv loss: 0.500114, aux loss: 1.481006, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 175/200, Batch 350/938\n",
      "D loss: 1.021943, acc: 100% // tar acc: 95% // adv loss: 0.554046, aux loss: 1.461168, tar loss: 0.679625\n",
      "=====================\n",
      "Epoch 175/200, Batch 850/938\n",
      "D loss: 1.091917, acc: 97% // tar acc: 100% // adv loss: 0.459354, aux loss: 1.470302, tar loss: 0.747784\n",
      "=====================\n",
      "Epoch 176/200, Batch 412/938\n",
      "D loss: 1.124583, acc: 96% // tar acc: 98% // adv loss: 0.540080, aux loss: 1.461235, tar loss: 0.733605\n",
      "=====================\n",
      "Epoch 176/200, Batch 912/938\n",
      "D loss: 1.070360, acc: 98% // tar acc: 96% // adv loss: 0.458175, aux loss: 1.476222, tar loss: 0.729247\n",
      "=====================\n",
      "Epoch 177/200, Batch 474/938\n",
      "D loss: 1.007844, acc: 100% // tar acc: 90% // adv loss: 0.555540, aux loss: 1.475886, tar loss: 0.693719\n",
      "=====================\n",
      "Epoch 178/200, Batch 36/938\n",
      "D loss: 1.117716, acc: 100% // tar acc: 98% // adv loss: 0.479826, aux loss: 1.461163, tar loss: 0.704947\n",
      "=====================\n",
      "Epoch 178/200, Batch 536/938\n",
      "D loss: 1.070714, acc: 99% // tar acc: 98% // adv loss: 0.625394, aux loss: 1.473717, tar loss: 0.721846\n",
      "=====================\n",
      "Epoch 179/200, Batch 98/938\n",
      "D loss: 1.075622, acc: 98% // tar acc: 100% // adv loss: 0.530398, aux loss: 1.470612, tar loss: 0.727946\n",
      "=====================\n",
      "Epoch 179/200, Batch 598/938\n",
      "D loss: 1.125446, acc: 96% // tar acc: 95% // adv loss: 0.431534, aux loss: 1.461255, tar loss: 0.720729\n",
      "=====================\n",
      "Epoch 180/200, Batch 160/938\n",
      "D loss: 1.039481, acc: 96% // tar acc: 95% // adv loss: 0.468186, aux loss: 1.462595, tar loss: 0.730058\n",
      "=====================\n",
      "Epoch 180/200, Batch 660/938\n",
      "D loss: 1.023955, acc: 98% // tar acc: 96% // adv loss: 0.631413, aux loss: 1.461189, tar loss: 0.697790\n",
      "=====================\n",
      "Epoch 181/200, Batch 222/938\n",
      "D loss: 1.081970, acc: 98% // tar acc: 100% // adv loss: 0.580306, aux loss: 1.462651, tar loss: 0.710961\n",
      "=====================\n",
      "Epoch 181/200, Batch 722/938\n",
      "D loss: 1.044387, acc: 99% // tar acc: 96% // adv loss: 0.661269, aux loss: 1.462059, tar loss: 0.714670\n",
      "=====================\n",
      "Epoch 182/200, Batch 284/938\n",
      "D loss: 1.052520, acc: 97% // tar acc: 100% // adv loss: 0.512901, aux loss: 1.461475, tar loss: 0.733433\n",
      "=====================\n",
      "Epoch 182/200, Batch 784/938\n",
      "D loss: 1.098046, acc: 98% // tar acc: 98% // adv loss: 0.501779, aux loss: 1.463985, tar loss: 0.732559\n",
      "=====================\n",
      "Epoch 183/200, Batch 346/938\n",
      "D loss: 1.106218, acc: 100% // tar acc: 98% // adv loss: 0.604960, aux loss: 1.461203, tar loss: 0.731958\n",
      "=====================\n",
      "Epoch 183/200, Batch 846/938\n",
      "D loss: 1.042602, acc: 99% // tar acc: 98% // adv loss: 0.392923, aux loss: 1.463717, tar loss: 0.724076\n",
      "=====================\n",
      "Epoch 184/200, Batch 408/938\n",
      "D loss: 1.073920, acc: 98% // tar acc: 92% // adv loss: 0.632643, aux loss: 1.461264, tar loss: 0.714635\n",
      "=====================\n",
      "Epoch 184/200, Batch 908/938\n",
      "D loss: 1.142921, acc: 96% // tar acc: 96% // adv loss: 0.586076, aux loss: 1.491865, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 185/200, Batch 470/938\n",
      "D loss: 1.054886, acc: 98% // tar acc: 95% // adv loss: 0.450455, aux loss: 1.461250, tar loss: 0.707982\n",
      "=====================\n",
      "Epoch 186/200, Batch 32/938\n",
      "D loss: 1.051166, acc: 98% // tar acc: 98% // adv loss: 0.517008, aux loss: 1.466855, tar loss: 0.733800\n",
      "=====================\n",
      "Epoch 186/200, Batch 532/938\n",
      "D loss: 1.132021, acc: 99% // tar acc: 95% // adv loss: 0.522301, aux loss: 1.476726, tar loss: 0.698570\n",
      "=====================\n",
      "Epoch 187/200, Batch 94/938\n",
      "D loss: 1.022754, acc: 99% // tar acc: 100% // adv loss: 0.827767, aux loss: 1.461153, tar loss: 0.693518\n",
      "=====================\n",
      "Epoch 187/200, Batch 594/938\n",
      "D loss: 1.104858, acc: 99% // tar acc: 98% // adv loss: 0.460006, aux loss: 1.476898, tar loss: 0.749963\n",
      "=====================\n",
      "Epoch 188/200, Batch 156/938\n",
      "D loss: 1.053865, acc: 97% // tar acc: 98% // adv loss: 0.534951, aux loss: 1.476911, tar loss: 0.746308\n",
      "=====================\n",
      "Epoch 188/200, Batch 656/938\n",
      "D loss: 1.024903, acc: 99% // tar acc: 92% // adv loss: 0.732211, aux loss: 1.461195, tar loss: 0.736720\n",
      "=====================\n",
      "Epoch 189/200, Batch 218/938\n",
      "D loss: 1.066721, acc: 100% // tar acc: 96% // adv loss: 0.702010, aux loss: 1.462000, tar loss: 0.743487\n",
      "=====================\n",
      "Epoch 189/200, Batch 718/938\n",
      "D loss: 1.073018, acc: 100% // tar acc: 98% // adv loss: 0.471777, aux loss: 1.466929, tar loss: 0.747931\n",
      "=====================\n",
      "Epoch 190/200, Batch 280/938\n",
      "D loss: 1.122339, acc: 97% // tar acc: 100% // adv loss: 0.497702, aux loss: 1.461163, tar loss: 0.756455\n",
      "=====================\n",
      "Epoch 190/200, Batch 780/938\n",
      "D loss: 1.036879, acc: 96% // tar acc: 96% // adv loss: 0.575264, aux loss: 1.461193, tar loss: 0.719702\n",
      "=====================\n",
      "Epoch 191/200, Batch 342/938\n",
      "D loss: 1.063896, acc: 99% // tar acc: 100% // adv loss: 0.539095, aux loss: 1.461174, tar loss: 0.754995\n",
      "=====================\n",
      "Epoch 191/200, Batch 842/938\n",
      "D loss: 1.066643, acc: 98% // tar acc: 98% // adv loss: 0.519588, aux loss: 1.461191, tar loss: 0.735953\n",
      "=====================\n",
      "Epoch 192/200, Batch 404/938\n",
      "D loss: 1.082036, acc: 99% // tar acc: 98% // adv loss: 0.473494, aux loss: 1.461194, tar loss: 0.696273\n",
      "=====================\n",
      "Epoch 192/200, Batch 904/938\n",
      "D loss: 1.101370, acc: 97% // tar acc: 98% // adv loss: 0.652353, aux loss: 1.461206, tar loss: 0.714455\n",
      "=====================\n",
      "Epoch 193/200, Batch 466/938\n",
      "D loss: 1.081990, acc: 98% // tar acc: 95% // adv loss: 0.427848, aux loss: 1.461152, tar loss: 0.763811\n",
      "=====================\n",
      "Epoch 194/200, Batch 28/938\n",
      "D loss: 1.066514, acc: 96% // tar acc: 98% // adv loss: 0.522527, aux loss: 1.461422, tar loss: 0.716923\n",
      "=====================\n",
      "Epoch 194/200, Batch 528/938\n",
      "D loss: 1.097660, acc: 97% // tar acc: 98% // adv loss: 0.460128, aux loss: 1.462683, tar loss: 0.700370\n",
      "=====================\n",
      "Epoch 195/200, Batch 90/938\n",
      "D loss: 1.144025, acc: 99% // tar acc: 96% // adv loss: 0.479096, aux loss: 1.461911, tar loss: 0.732187\n",
      "=====================\n",
      "Epoch 195/200, Batch 590/938\n",
      "D loss: 1.093062, acc: 99% // tar acc: 95% // adv loss: 0.347398, aux loss: 1.462563, tar loss: 0.714863\n",
      "=====================\n",
      "Epoch 196/200, Batch 152/938\n",
      "D loss: 1.038452, acc: 99% // tar acc: 93% // adv loss: 0.544614, aux loss: 1.461296, tar loss: 0.684699\n",
      "=====================\n",
      "Epoch 196/200, Batch 652/938\n",
      "D loss: 1.095401, acc: 95% // tar acc: 100% // adv loss: 0.463991, aux loss: 1.465091, tar loss: 0.759075\n",
      "=====================\n",
      "Epoch 197/200, Batch 214/938\n",
      "D loss: 1.098987, acc: 96% // tar acc: 96% // adv loss: 0.513795, aux loss: 1.476811, tar loss: 0.684788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 197/200, Batch 714/938\n",
      "D loss: 1.090556, acc: 100% // tar acc: 96% // adv loss: 0.521840, aux loss: 1.462583, tar loss: 0.715464\n",
      "=====================\n",
      "Epoch 198/200, Batch 276/938\n",
      "D loss: 1.110210, acc: 99% // tar acc: 95% // adv loss: 0.413220, aux loss: 1.475931, tar loss: 0.735903\n",
      "=====================\n",
      "Epoch 198/200, Batch 776/938\n",
      "D loss: 1.025543, acc: 97% // tar acc: 95% // adv loss: 0.633249, aux loss: 1.491300, tar loss: 1.110000\n",
      "=====================\n",
      "Epoch 199/200, Batch 338/938\n",
      "D loss: 1.039352, acc: 99% // tar acc: 100% // adv loss: 0.474189, aux loss: 1.461372, tar loss: 0.739939\n",
      "=====================\n",
      "Epoch 199/200, Batch 838/938\n",
      "D loss: 1.096465, acc: 96% // tar acc: 96% // adv loss: 0.363089, aux loss: 1.461183, tar loss: 0.750407\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import csv\n",
    "f = open('../models/Run2_log.csv', 'a')\n",
    "log_writer = csv.writer(f, delimiter=',')\n",
    "log_writer.writerow(['Epoch', 'Batch', 'DLoss', 'DAcc', 'TarAcc', 'AdvLoss', 'AuxLoss', 'TarLoss', 'GLoss'])\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        \n",
    "        batch_size = imgs.shape[0]\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity, pred_label = discriminator(gen_imgs)\n",
    "        target_classifier_pred_label = target_classifier(gen_imgs)\n",
    "        \n",
    "        t_acc = np.mean(np.argmax(target_classifier_pred_label.data.cpu().numpy(), axis=1) == gen_labels.data.cpu().numpy())\n",
    "        \n",
    "        adv_loss = adv_loss_coeff * adversarial_loss(validity, valid)\n",
    "        aux_loss = aux_loss_coeff * auxiliary_loss(pred_label, gen_labels)\n",
    "        tar_loss = tar_loss_coeff * get_target_loss(adv_loss, aux_loss, target_classifier_pred_label, gen_labels)\n",
    "        g_loss = adv_loss + aux_loss + tar_loss\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_pred, real_aux = discriminator(real_imgs)\n",
    "        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "        # Loss for fake images\n",
    "        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = d_real_loss_coeff * d_real_loss + d_fake_loss_coeff * d_fake_loss\n",
    "\n",
    "        # Calculate discriminator accuracy\n",
    "        pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "        gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "        d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        log_writer.writerow([epoch, i, d_loss.item(), 100*d_acc, 100*t_acc, adv_loss.item(), aux_loss.item(), tar_loss.item(), g_loss.item()])\n",
    "        \n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_image(n_row=10, batches_done=batches_done)\n",
    "            # Saves weights\n",
    "            torch.save(generator.state_dict(), \"../models/Success2_G\")\n",
    "            torch.save(discriminator.state_dict(), \"../models/Success2_D\")\n",
    "            print(\n",
    "                \"=====================\\nEpoch %d/%d, Batch %d/%d\\nD loss: %f, acc: %d%% // tar acc: %d%% // adv loss: %f, aux loss: %f, tar loss: %f\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), 100 * d_acc, 100 * t_acc, adv_loss.item(), aux_loss.item(), tar_loss.item())\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves weights\n",
    "torch.save(generator.state_dict(), \"../models/Success2_G\")\n",
    "torch.save(discriminator.state_dict(), \"../models/Success2_D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "batch_size = 1000\n",
    "z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
    "\n",
    "# Generate a batch of images\n",
    "gen_imgs = generator(z, gen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Discriminator Valid: [0.565994] Discriminator Class: 9')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots a sample\n",
    "import matplotlib.pyplot as plt\n",
    "sample_idx = 28\n",
    "print(gen_labels[sample_idx])\n",
    "plt.imshow(gen_imgs[sample_idx][0].cpu().detach().numpy(), cmap='gray', interpolation='none')\n",
    "plt.title(\"Gen Label: \" + str(gen_labels.cpu().detach().numpy()[sample_idx]) + \"; LeNet Label: \" + str(np.argmax(pred_labels.data.cpu().numpy()[sample_idx])))\n",
    "plt.xlabel(\"Discriminator Valid: \" + str(validity.data.cpu().numpy()[sample_idx]) +\" Discriminator Class: \" + str(np.argmax(dpred_labels.data.cpu().numpy()[sample_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds target classification for sample\n",
    "pred_labels = target_classifier(gen_imgs)\n",
    "np.argmax(pred_labels.data.cpu().numpy()[sample_idx])\n",
    "#pred_labels.data.cpu().numpy()[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 28, 120, 163, 301, 308, 338, 357, 381, 411, 473, 481, 630, 671,\n",
       "        733, 763, 859, 939, 950, 952, 954, 961, 970]),)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds indexes that have adversarial examples\n",
    "t_acc = np.argmax(pred_labels.data.cpu().numpy(), axis=1) == gen_labels.data.cpu().numpy()\n",
    "np.where(t_acc == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen = gen_imgs.cpu().detach().numpy()\n",
    "preds = pred_labels.data.cpu().numpy()\n",
    "true = gen_labels.data.cpu().numpy()\n",
    "v = validity.data.cpu().numpy()\n",
    "dpred = dpred_labels.data.cpu().numpy()\n",
    "for i in range(len(true)):\n",
    "    if np.argmax(preds[i]) != true[i]:\n",
    "        plt.title(\"Gen Label: \" + str(true[i]) + \"; LeNet Label: \" + str(np.argmax(preds[i])))\n",
    "        plt.xlabel(\"Discriminator Valid: \" + str(v[i]) +\" Discriminator Class: \" + str(np.argmax(dpred[i])))\n",
    "        plt.imshow(gen[i][0], cmap='gray')\n",
    "        plt.savefig(\"../images/SecondRun/AdversarialExamples/\" + str(1000+i) + \".png\")\n",
    "        \n",
    "#for i in range(len(true)):\n",
    "#    plt.title(\"Gen Label: \" + str(true[i]) + \"; LeNet Label: \" + str(np.argmax(preds[i])))\n",
    "#    plt.imshow(gen[i][0], cmap='gray')\n",
    "#    plt.savefig(\"../images/SecondRun/AllSamples/\" + str(i) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra saving\n",
    "torch.save(gen_imgs, \"../models/Success2_GenSample\")\n",
    "torch.save(gen_labels, \"../models/Success2_GenLabels\")\n",
    "torch.save(pred_labels, \"../models/Success2_LeNetLabels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achyut/Projects/AdversarialRobustness/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "validity, dpred_labels = discriminator(gen_imgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
